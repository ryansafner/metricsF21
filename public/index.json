[{"authors":["admin"],"categories":null,"content":"I am Assistant Professor of Economics at Hood College. Previously I taught at Wake Forest University.\nI have a Ph.D and M.A. in Economics from George Mason University, and a B.A. in Economics from the University of Connecticut. Go Huskies!\nMy research broadly explores the political economy of innovation, technological growth, and intellectual property using the tools of new institutional economics, public choice, and market process economics.\nI am a passionate user of R for data analysis, and R Markdown and GitHub for writing reproducible research, version control, and managing my workflow. I occasionally blog here about using these tools in my research and teaching.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1630381769,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"https://metricsf21.classes.ryansafner.com/authors/admin/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/admin/","section":"authors","summary":"I am Assistant Professor of Economics at Hood College. Previously I taught at Wake Forest University.\nI have a Ph.D and M.A. in Economics from George Mason University, and a B.A. in Economics from the University of Connecticut. Go Huskies!\nMy research broadly explores the political economy of innovation, technological growth, and intellectual property using the tools of new institutional economics, public choice, and market process economics.\nI am a passionate user of R for data analysis, and R Markdown and GitHub for writing reproducible research, version control, and managing my workflow.","tags":null,"title":"Ryan Safner","type":"authors"},{"authors":null,"categories":null,"content":"  On this page, you can find more information about each individual assignment, as well as the assignments themselves.\n  Grade Calculator  Use this tool to help calculate your overall course grade using existing assignment grades, as well as forecast “what if” scenarios.\nPlease refer to the syllabus for more information about grades.\nHomeworks There will be several problem sets (one at the end of each lesson). Problem sets will be a combination of math/statistical theory \u0026amp; application problems and problems that require use of R with real data. You may collaborate with other students to work on problem sets, but each person must turn in an individual problem set. Problem sets are due one week from the class period where we finish a lesson, and must emailed to me by the start of class (so please type or, if you must, hand write and scan them).\n Exams This class will have two (2) exams.\n Midterm Exam After we have finished linear regression, there will be a midterm constituting a combination of multiple choice, problem, and short answer questions. This will cover the content we discuss in class, my lectures, and the readings. The midterm provides feedback both to you and to me that ensures everyone is progressing on schedule and comprehending the material. This is critical, as the rest of the course will build off of this foundation.\n Final Exam On the college-determined date, we will have a comprehensive, closed-book, in-class final exam.\n ","date":-62135596800,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1627070356,"objectID":"b9d7929e6b0943738455b33dd1664142","permalink":"https://metricsf21.classes.ryansafner.com/assignments/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/assignments/","section":"assignments","summary":"On this page, you can find more information about each individual assignment, as well as the assignments themselves.\n  Grade Calculator  Use this tool to help calculate your overall course grade using existing assignment grades, as well as forecast “what if” scenarios.\nPlease refer to the syllabus for more information about grades.\nHomeworks There will be several problem sets (one at the end of each lesson). Problem sets will be a combination of math/statistical theory \u0026amp; application problems and problems that require use of R with real data.","tags":null,"title":"Assignment Details","type":"docs"},{"authors":null,"categories":null,"content":"  These content pages provide the readings, lecture slide (in html and printable PDF), appendices, and other useful materials for each class. Each lesson covers approximately one class meeting.\nPlease note that the lesson numbers, topics, and titles (e.g. 1.1) are my design, and do not match up with the textbook!\n","date":-62135596800,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1629657987,"objectID":"1413f960ec974b9863bc45d887efa8bd","permalink":"https://metricsf21.classes.ryansafner.com/content/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/content/","section":"content","summary":"These content pages provide the readings, lecture slide (in html and printable PDF), appendices, and other useful materials for each class. Each lesson covers approximately one class meeting.\nPlease note that the lesson numbers, topics, and titles (e.g. 1.1) are my design, and do not match up with the textbook!","tags":null,"title":"Overview","type":"docs"},{"authors":null,"categories":null,"content":"  Here are all practice problems we do in class. Answers are posted after class on the original page.\n","date":-62135596800,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1629979894,"objectID":"9a68f85b489f821baa9881145ecca9ed","permalink":"https://metricsf21.classes.ryansafner.com/r/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/r/","section":"r","summary":"Here are all practice problems we do in class. Answers are posted after class on the original page.","tags":null,"title":"Overview","type":"docs"},{"authors":null,"categories":null,"content":"Look here over the course of the semester for resources, links, and tips on how to succeed in the course, how to write well, and other things of interest related to econometrics, data analysis, managing your workflow, and using R.\n","date":-62135596800,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1626990847,"objectID":"b19066f515b030ede8b37d7a9ead8692","permalink":"https://metricsf21.classes.ryansafner.com/resources/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/resources/","section":"resources","summary":"Look here over the course of the semester for resources, links, and tips on how to succeed in the course, how to write well, and other things of interest related to econometrics, data analysis, managing your workflow, and using R.","tags":null,"title":"Resources","type":"docs"},{"authors":null,"categories":null,"content":" Take notes. On paper. Yes. Studies show that using pen and paper trump taking notes on a device. The main reason is because you cannot possibly write down everything I am saying by hand. This forces you to selectively filter my lecture to its most essential and important components \u0026ndash; an integral part of the learning and retention process. If you are taking notes on a laptop, you become a court stenographer, thoughtlessly transcribing everything I am saying. At the other extreme, do not assume everything is written in my lecture slides. My lecture slides are visuals and cues to organize the content both for me and for you. I try hard to make sure that I have few words on slides, and even fewer answers to problems.  My suggestion: print out my lecture slides in advance (or follow along on your device) and take additional notes by hand.   At least skim all of the readings. I give out readings for a reason, and that reason is not to bore you or waste your time. The truth is, as a beginner, you can\u0026rsquo;t rely on heuristics or memory to \u0026ldquo;fill in the blanks.\u0026rdquo; You need the readings to provide context to what I am saying in class. I cannot help you if you are not going to try. Any professor also will lose patience at short notice when it\u0026rsquo;s discovered you haven\u0026rsquo;t done the reading. Do the homeworks (if there are any). You might be surprised that I need to say this, but I do. Students that do not do the homeworks do poorly on exams, which often are similar to homework questions. Homeworks are the best practice for exams, they give you a sense of the relevant content areas that might come up, the type and style of questions that I ask, and you often get answer keys to help you \u0026ldquo;get inside my head\u0026rdquo; and study from. There is no good reason why you should have a low homework grade. Work on assignments together and study together. Recognize that you are not in this alone, and other students are just as anxious or uncertain as you are. More surprisingly, other students probably have some answers you are looking for, and you may have answers for some of their problems! You will learn better when you collaborate with others similar to you. More importantly, you don\u0026rsquo;t truly understand something unless you can explain it to others.^[Yes, that means I am doing a ton of learning every time I teach!] As a wise woman once told me \u0026ldquo;whomever is doing the talking is doing the learning.\u0026rdquo; Learn how to learn. The most important skill you learn in college is how to learn. Taking a course on a subject will not make you an expert on that subject. It a) helps you recognize that you do not know everything on that subject, and this prevents you from actively saying stupid things; and b) gives you enough context and skills to figure out how to actually fill those gaps. This is the actual skill that\u0026rsquo;s relevant in the real world.^[Yes, Google is your best friend. But you do not yet know how to ask the right questions, or understand what constitutes good answers.]  ","date":1557010800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1626808675,"objectID":"a8f2b974e9ce460bb770962ed237d1c6","permalink":"https://metricsf21.classes.ryansafner.com/resources/tips/","publishdate":"2019-05-05T00:00:00+01:00","relpermalink":"/resources/tips/","section":"resources","summary":"Take notes. On paper. Yes. Studies show that using pen and paper trump taking notes on a device. The main reason is because you cannot possibly write down everything I am saying by hand. This forces you to selectively filter my lecture to its most essential and important components \u0026ndash; an integral part of the learning and retention process. If you are taking notes on a laptop, you become a court stenographer, thoughtlessly transcribing everything I am saying.","tags":null,"title":"Tips for Success in This Course","type":"docs"},{"authors":null,"categories":null,"content":"   List of Public Datasets, Data Sources, and R APIs Built-in Datasets  A near-comprehensive list of all existing data sets built-in to R or R packages1   General Databases of Datasets  Google Database Search Kaggle Harvard Law School: Find a Database   Good R Packages for Getting Data in R Format2 Below are packages written by and for R users that link up with the API of key data sets for easy use in R. Each link goes to the documentation and description of each package.\nDon’t forget to install3 first and then load it with library().\n owidR for importing data from Our World in Data wbstats provides access to all the data available on the World Bank API, which is basically everything on their website. The World Bank keeps track of many country-level indicators over time. tidycensus gives you access to data from the US Census and the American Community Survey. These are the largest high-quality data sets you’ll find of cross-sectional data on individual people in the US. You’ll need to get a (free) API key from the website (or ask me for mine). fredr gets data from the Federal Reserve’s Economic Database (FRED). You’ll need to get a (free) API key from the website (or ask me for mine). tidyquant gets data from a number of financial sources (including fredr). icpsrdata downloads data from the Inter-university Consortium for Political and Social Research (you’ll need an account and a keycode). ICPSR is a database of datasets from published social science papers for the purposes of reproducibility. NHANES uses data from the US National Health and Nutrition Examination Survey. ipumsr has census data from all around the world, in addition to the US census, American Community Survey, and Current Population Survey. If you’re doing international micro work, look at IPUMS. It’s also the easiest way to get the Current Population Survey (CPS), which is very popular for labor economics. Unfortunately ipumsr won’t get the data from within R; you’ll have to make your own data extract on the IPUMS website and download it. But ipumsr will read that file into R and preserve things like names and labels. education-data-package-r4 is the Urban Institute’s data data on educational institutions in the US, including colleges (in IPEDS) and K-12 schools (in CCD). This package also has data on county-level poverty rates from SAIPE. psidR is the Panel Study of Income Dynamics. This study doesn’t just follow people over their lifetimes, it follows their children too, generationally! A great source for studying how things follow families through generations. atus is th e American Time Use Survey, which is a large cross-sectional data set with information on how people spend their time. Rilostat uses data from the International Labor Organization. This contains lots of different statistics on labor, like employment, wage gaps, etc., generally aggregated to the national level and changing over time. democracyData5 is a great “package for accessing and manipulating existing measures of democracy.” politicaldata provides useful functions for obtaining commonly-used data in political analysis and political science, including from sources such as the Comparative Agendas Project (which provides data on politics and policy from 20+ countries), the MIT Election and Data Science Lab, and FiveThirtyEight.  Below is a list of good data sources depending on the types of topics you might be interested in writing on:6\n  Key Data Sources  Coronavirus Data: John Hopkins CSSE Covid-19 data (definitive), Our World in Data, New York Times Covid data, covdata r package, Tidy Covid data\n Our World in Data\n American Economic Association Data\n IPUMS (Integrated Public Use Microdata Series)\n EconData from UMD\n ICPSR (Inter-university Consortium for Political and Social Research)\n NBER’s Public Use Data Archive\n Historical Macroeconomic Statistics\n UMD’s Interindustry Forecasting\n DB-nomics\n Internet UPC Database\n International Trade Data\n OurWorldinData.org (download datasets)\n SciencesPo International Trade Gravity Dataset\n Center for International Data\n Atlas of Economic Complexity\n U.N. World Development Reports\n Observatory of Economic Complexity\n Reddit /r/datasets\n Google Cloud Public Datasets\n  By Topic\n Quality of Government Data has an extremely wide range of data sources pertaining to measures of institutions. The data itself can be found here. National and State Accounts Data: Bureau of Economic Analysis Labor Market and Price Data: Bureau of Labor Statistics Macroeconomic Data: Federal Reserve Economic Data (FRED), World Development Indicators (World Bank), Penn World Table International Data: NationMaster.com, Doing Business, CIESIN Census Data: U.S. Census Bureau Sports Data: Spotrac, Rodney Fort’s Sports Data Data Clearing House: Stat USA, Fedstats, Statistical Abstract of the United States, Resources for Economists Political and Social Data: ICPSR, Federal Election Commission, Poole and Rosenthal Roll Call Data (Voting ideology), Archigos Data on Political Leaders, Library of Congress: Thomas (Legislation), Iowa Electronic Markets (Prediction Markets) War and Violence Data: Correlates of War State Level Data: Correlates of State Policy Health Data: Centers for Disease Control, CDC Wonder System Crime Data: Bureau of Justice Statistics Education Data: National Center for Education Statistics Environmental Data: EPA Religion Data: American Religion Data Archiva (ARDA) Financial Data: Financial Data Finder{Financial Data Finder} Philanthropy Data: The Urban Institute    Note: You should use these more for playing around with in R to boost your data wrangling skills. These should not be used for your projects in most circumstances.↩︎\n Some of these come from Nick Huntington-Klein’s excellent list.↩︎\n install.packages(\"name_of_package\")↩︎\n Note you will need to install devtools package first, and then install the package directly from Github with the command devtools::install_github('UrbanInstitute/education-data-package-r')↩︎\n Note you will need to install devtools package first, and then install the package directly from Github with the command devtools::install_github('xmarquez/democracyData')↩︎\n Some of these come from various sources, including https://github.com/awesomedata/awesome-public-datasets#economics↩︎\n   ","date":1557010800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1634399402,"objectID":"82283dbface403be8b1fd6c65e78ad40","permalink":"https://metricsf21.classes.ryansafner.com/resources/data/","publishdate":"2019-05-05T00:00:00+01:00","relpermalink":"/resources/data/","section":"resources","summary":"List of Public Datasets, Data Sources, and R APIs Built-in Datasets  A near-comprehensive list of all existing data sets built-in to R or R packages1   General Databases of Datasets  Google Database Search Kaggle Harvard Law School: Find a Database   Good R Packages for Getting Data in R Format2 Below are packages written by and for R users that link up with the API of key data sets for easy use in R.","tags":null,"title":"Data Resources","type":"docs"},{"authors":null,"categories":null,"content":"Using an App on Your Phone There are many good apps out there that will allow you to take photos and convert them to PDFs. This is actually a better method than using your computer (described below), since theses apps optimize your photos for PDFs (using your computer to convert will often result in very large PDF file sizes!). Here are a few apps you can use:\n Scannable ^[If you use Evernote for notes (I do, it\u0026rsquo;s amazing), this can sync up and store your PDFs in Evernote] Turboscan  Image to PDF Converter Free  PDF Converter Pro  Simple Scan   Personally, I use Scannable — primarily because of its association with Evernote, if you wanted a recommendation. But note it does not exist on Android. I also have successfully used Turboscan in the past.\nAdditionally, as Hood students, you all have Onedrive, you can use the app on your phone to scan documents with photos and convert them to PDFs.\nUsing Images Sent to Your Computer Most modern versions of operating system have a built-in tool in the File Viewer (or Finder) menus, after clicking on one or multiple files, to create a PDF from the files.\nSo first take photos on your smartphone of your written work (one photo per page). Please try to frame your photos properly! Put your paper flat on a solid surface (table, desk, the floor, etc). Get the whole page within the borders of the photo, and not too much background. I don\u0026rsquo;t need to see half of your desk or bed as you are taking the photo! Take a look at it and make sure it is legible.\nNext, get the photos onto your computer (whether by Airdrop, email to yourself, Dropbox, etc.). Finally, depending on your OS, convert the files to a PDF:\n1. On a Windows PC\nOpen the folder where your photos are currently, in the File Explorer. Select all of the photos, and right click, and select Print. In the dialog box that pops up, select Microsoft Print to PDF in the Printer box, and then click Print. This will save it as a .pdf file in that folder. See more information.\n2. On a Mac\nAs I use a Mac, I will show you how Mac OS has a neat feature built into Finder, which allows converting multiple files into a single PDF file as a Quick Action. I have written two pages in a notebook and taken two separate pictures of them, and airdropped them onto my computer.\nHere is the  example PDF.\n3. On Linux\nIf you use Linux, I assume you know your way around a computer well enough to make a PDF! 🤖\n","date":1557010800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1632766478,"objectID":"aecca33a7bfb6fad35a4cc43f876accd","permalink":"https://metricsf21.classes.ryansafner.com/resources/pdfs/","publishdate":"2019-05-05T00:00:00+01:00","relpermalink":"/resources/pdfs/","section":"resources","summary":"Using an App on Your Phone There are many good apps out there that will allow you to take photos and convert them to PDFs. This is actually a better method than using your computer (described below), since theses apps optimize your photos for PDFs (using your computer to convert will often result in very large PDF file sizes!). Here are a few apps you can use:\n Scannable ^[If you use Evernote for notes (I do, it\u0026rsquo;s amazing), this can sync up and store your PDFs in Evernote] Turboscan  Image to PDF Converter Free  PDF Converter Pro  Simple Scan   Personally, I use Scannable — primarily because of its association with Evernote, if you wanted a recommendation.","tags":null,"title":"How to Make a PDF","type":"docs"},{"authors":null,"categories":null,"content":"   Installing R Install R Studio R Studio Cloud   We will do all of our work in this course with the free \u0026amp; open source programming language R. While you can run everything you need directly in the command line using R, it is a lot more convenient to use an integrated development environment (IDE) like R Studio. Think of R as the engine of a car, and R Studio as the dashboard.\nYou will need to install both, but we will ever only open R Studio.\nInstalling R First you will need to download and install R on your computer.\nGo to the Comprehensive R Archive Network (CRAN) that maintains R and its official packages at: https://cran.r-project.org Click on “Download R for …” your operating system (Mac or Windows)   If you use a Mac, scroll to the first .pkg file listed on the left and download. If you use Windows, click on base (“This what you want to install R for the first time”)  Install the downloaded package like you would any software application on your computer.   Typically, open the file from your Downloads folder (or whever you save downloaded files) and follow the prompts to install on your computer.  If you use a Mac, also download and install XQuartz (https://www.xquartz.org/). You do not need to do this on Windows.   Install R Studio Go to RStudio.com and download the free desktop version. The website should automatically detect your operating system and give you a large button to click to download the application. Install the downloaded package like you would any software application on your computer.   R Studio Cloud R is free, but sometimes can be difficult to install and configure on your computer. To make things easier, and to ensure everyone has a consistent experience for class, you can (and should) use the free Rstudio.Cloud service initially. This allows you to run R in your browser (i.e. Chrome, Firefox, Safari, etc), meaning you don’t need to worry about installing things on your computer.\nGo to https://rstudio.cloud and create an account (please use your first and last name). I will send you a link via email to join our class workspace.\nR Studio Cloud is convenient, but is not designed to be as fully customizable and extensive as the main desktop version. I would start with the Cloud version if you have trouble with your own computer or computers on campus running R or R Studio. But ultimately, you will want to eventually do everything on your own computer and not the cloud version.\n ","date":1557010800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1632766478,"objectID":"2774a692091b996921c7271a5f3e682c","permalink":"https://metricsf21.classes.ryansafner.com/resources/installing-r/","publishdate":"2019-05-05T00:00:00+01:00","relpermalink":"/resources/installing-r/","section":"resources","summary":"Installing R Install R Studio R Studio Cloud   We will do all of our work in this course with the free \u0026amp; open source programming language R. While you can run everything you need directly in the command line using R, it is a lot more convenient to use an integrated development environment (IDE) like R Studio. Think of R as the engine of a car, and R Studio as the dashboard.","tags":null,"title":"Installing R and R Studio","type":"docs"},{"authors":null,"categories":null,"content":"     Appendix Type Class    Budget Constraint for n Goods Math/Economics 1.2 class    List of Math and R Appendices The following list contains links to all appendices spread across the Class Notes pages:\n R: Installing R and R Studio (Class 1.3) R: R Packages (Class 1.3) R: Getting Help for R (Class 1.3) R: Other Useful Commands to Know (Class 1.3) R: Style Guide for Coding (Class 1.3) Math: The Summation Operator (Class 2.1) R: Creating Mathematical Functions (Class 2.2) R: Graphing Mathematical Functions (Class 2.2) R: Bultin Statistical Functions (Class 2.2) R: Graphing Statistical Functions (Class 2.2) Math: Variance (Class 2.3) Math: Covariance (Class 2.3) Math: Correlation (Class 2.3) R: Calculating Correlation Example (Class 2.3) Math: Deriving the OLS Estimators (Class 2.4) Math: Algebraic Properties of OLS Estimators (Class 2.4) Math: Bias in \\(\\hat{\\beta_1}\\) (Class 2.4) Math: Proof of the Unbiasedness of \\(\\hat{\\beta_1}\\) (Class 2.4) –\u0026gt;   ","date":1557010800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1634399402,"objectID":"8387287e0edf7fb7175ee9c8965291f0","permalink":"https://metricsf21.classes.ryansafner.com/resources/appendices/","publishdate":"2019-05-05T00:00:00+01:00","relpermalink":"/resources/appendices/","section":"resources","summary":"Appendix Type Class    Budget Constraint for n Goods Math/Economics 1.2 class    List of Math and R Appendices The following list contains links to all appendices spread across the Class Notes pages:\n R: Installing R and R Studio (Class 1.3) R: R Packages (Class 1.3) R: Getting Help for R (Class 1.3) R: Other Useful Commands to Know (Class 1.","tags":null,"title":"List of Math Appendices","type":"docs"},{"authors":null,"categories":null,"content":"   A General Symbol Guide There are a lot of symbols (often greek letters or ligatures on English letters) used in statistics and econometrics. Luckliy, most of them follow some standard patterns, and are consistent across textbooks and research (note there are exceptions!).\n    Style Examples Meaning    Greek letters \\(\\beta_0, \\beta_1, \\sigma, u\\) True parameters of population  Hats \\(\\hat{\\beta_0}, \\hat{\\beta_1}, \\hat{\\sigma}, \\hat{u}\\) Our statistical estimates of population parameters, from sample data  English capital letters \\(X_1, X_2, Y\\) (Random) variables in our sample data  English lowercase letters \\(x_{1i}, x_{2i}, y_i\\) Individual observations of variables in our sample data  Modified capital letters \\(\\bar{X}, \\bar{Y}\\) Statistics calculated from our sample data (e.g. sample mean)  Bold capital letters \\(X= \\begin{bmatrix} x_1, x_2, \\cdots , x_n \\\\ \\end{bmatrix}\\) \\(\\mathbf{\\beta} = \\begin{bmatrix} \\beta_1, \\beta_2, \\cdots , \\beta_k \\\\ \\end{bmatrix}\\) Vector or matrix     Sample Statistics vs Population Parameters Formulae      Sample Population    Population \\(n\\) \\(N\\)  Mean \\(\\bar{x} = \\frac{1}{n} \\displaystyle\\sum^n_{i=1} x_i\\) \\(\\mu = \\frac{1}{N} \\displaystyle\\sum^N_{i=1} x_i\\)  Variance \\(s^2=\\frac{1}{n-1} \\displaystyle\\sum^n_{i=1} (x_i-\\bar{x})^2\\) \\(\\sigma^2=\\frac{1}{N} \\displaystyle\\sum^N_{i=1} (x_i-\\mu)^2\\)  Standard Deviation \\(s = \\sqrt{s^2}\\) \\(\\sigma = \\sqrt{\\sigma^2}\\)     ","date":1557010800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1626990847,"objectID":"610bf95ca3367e111999404814b0bc57","permalink":"https://metricsf21.classes.ryansafner.com/resources/statistics/","publishdate":"2019-05-05T00:00:00+01:00","relpermalink":"/resources/statistics/","section":"resources","summary":"A General Symbol Guide There are a lot of symbols (often greek letters or ligatures on English letters) used in statistics and econometrics. Luckliy, most of them follow some standard patterns, and are consistent across textbooks and research (note there are exceptions!).\n    Style Examples Meaning    Greek letters \\(\\beta_0, \\beta_1, \\sigma, u\\) True parameters of population  Hats \\(\\hat{\\beta_0}, \\hat{\\beta_1}, \\hat{\\sigma}, \\hat{u}\\) Our statistical estimates of population parameters, from sample data  English capital letters \\(X_1, X_2, Y\\) (Random) variables in our sample data  English lowercase letters \\(x_{1i}, x_{2i}, y_i\\) Individual observations of variables in our sample data  Modified capital letters \\(\\bar{X}, \\bar{Y}\\) Statistics calculated from our sample data (e.","tags":null,"title":"Statistics Resources","type":"docs"},{"authors":null,"categories":null,"content":"   Visualization Content Class (See Description for Refernece)     Visualizing the Consumer\u0026rsquo;s Problem Consumer Theory 1.6 class   Visualizing Changes in the Consumer\u0026rsquo;s Problem Consumer Theory 1.6 class   Visualizing Demand Shifters Consumer Theory 1.6 class    ","date":1557010800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1626808675,"objectID":"bacc98eb1f2dc1dc9d50c3f8b16e0142","permalink":"https://metricsf21.classes.ryansafner.com/resources/visualizations/","publishdate":"2019-05-05T00:00:00+01:00","relpermalink":"/resources/visualizations/","section":"resources","summary":"   Visualization Content Class (See Description for Refernece)     Visualizing the Consumer\u0026rsquo;s Problem Consumer Theory 1.6 class   Visualizing Changes in the Consumer\u0026rsquo;s Problem Consumer Theory 1.6 class   Visualizing Demand Shifters Consumer Theory 1.6 class    ","tags":null,"title":"List of Interactive Visualizations","type":"docs"},{"authors":null,"categories":null,"content":"   library(tidyverse) ## ── Attaching packages ─────────────────────────────────────── tidyverse 1.3.1 ── ## ✓ ggplot2 3.3.5 ✓ purrr 0.3.4 ## ✓ tibble 3.1.5 ✓ dplyr 1.0.7 ## ✓ tidyr 1.1.3 ✓ stringr 1.4.0 ## ✓ readr 2.0.0 ✓ forcats 0.5.1 ## ── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ── ## x dplyr::filter() masks stats::filter() ## x dplyr::lag() masks stats::lag() Importing Data The janitor package\ncleannames()\n generally makes lowercase names   Dealing with Missing Data When calculating statistics (e.g. with summarize()), many calculations will give errors if your data contains NAs.\nExample: Calculating Mean data_missing \u0026lt;- tribble( ~x, ~y, 2, 3, 1, 4, NA, 2, 3, NA, 7, 8 ) Now if we were to get the mean of x:\ndata_missing %\u0026gt;% summarize(mean_x = mean(x)) ## # A tibble: 1 × 1 ## mean_x ## \u0026lt;dbl\u0026gt; ## 1 NA It gives us NA.\nOne way to combat this is to ignore all observations that contain NA values. Most statistics functions (like mean()) have an optional argument na.rm, which if set to TRUE, will ignore NAs when performing the calculation:\ndata_missing %\u0026gt;% summarize(mean_x = mean(x, na.rm = TRUE)) ## # A tibble: 1 × 1 ## mean_x ## \u0026lt;dbl\u0026gt; ## 1 3.25   ","date":1557010800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1634645372,"objectID":"4666a086ba0bf4f47a972c6fc74c766b","permalink":"https://metricsf21.classes.ryansafner.com/resources/wrangling/","publishdate":"2019-05-05T00:00:00+01:00","relpermalink":"/resources/wrangling/","section":"resources","summary":"library(tidyverse) ## ── Attaching packages ─────────────────────────────────────── tidyverse 1.3.1 ── ## ✓ ggplot2 3.3.5 ✓ purrr 0.3.4 ## ✓ tibble 3.1.5 ✓ dplyr 1.0.7 ## ✓ tidyr 1.1.3 ✓ stringr 1.4.0 ## ✓ readr 2.0.0 ✓ forcats 0.5.1 ## ── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ── ## x dplyr::filter() masks stats::filter() ## x dplyr::lag() masks stats::lag() Importing Data The janitor package\ncleannames()\n generally makes lowercase names   Dealing with Missing Data When calculating statistics (e.","tags":null,"title":"Data Wrangling Tips \u0026 Tricks","type":"docs"},{"authors":null,"categories":null,"content":"   The following is a compendium of all R packages used in this course (in order of appearance), their main uses, and what we use them for1. Each package name below is a link to the package’s website and/or documentation.\n† Indicates package is part of the tidyverse\n  Name Type Description/Reason(s) for Use Classes Used    ggplot2† Plotting For nice plots [1.3]  gganimate Plotting For animating plots [1.3]  haven† Data Wrangling For importing nonstandard data files [1.4]  dplyr† Data Wrangling For manipulating data (part of tidyverse) [1.4]  readr† Data Wrangling For importing most data files [1.4]  tidyr† Data Wrangling For reshaping data (wide and long) [1.4]  magrittr† Data Wrangling For the pipe [1.4]  tibble† Data Wrangling For a friendlier data.frame [1.4]  ggrepel Plotting For annotating text that doesn’t cover observations [1.4]  broom Models For tidying regression output [2.3]  car Models For testing for outliers [2.5]  estimatr Models For calculating robust standard errors [2.5]  lmtest Models For testing for heteroskedasticity [2.5]  huxtable Models For making nice regression tables [2.5]  infer Models For simulation and statistical inference [2.6]     Note, many of these packages have multiple uses beyond our purposes!↩︎\n   ","date":1557010800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1634399402,"objectID":"6c7ad62cd04ae78a95959a992c59410d","permalink":"https://metricsf21.classes.ryansafner.com/resources/r-packages/","publishdate":"2019-05-05T00:00:00+01:00","relpermalink":"/resources/r-packages/","section":"resources","summary":"The following is a compendium of all R packages used in this course (in order of appearance), their main uses, and what we use them for1. Each package name below is a link to the package’s website and/or documentation.\n† Indicates package is part of the tidyverse\n  Name Type Description/Reason(s) for Use Classes Used    ggplot2† Plotting For nice plots [1.3]  gganimate Plotting For animating plots [1.","tags":null,"title":"R Packages We Use","type":"docs"},{"authors":null,"categories":null,"content":"   Unzipping files on macOS Unzipping files on Windows   Since R projects typically consist of multiple files (R scripts, datasets, images, etc.) the easiest way to distribute and send them is to combine all the different files in to a single compressed .zip file. When you unzip a .zip file, your computer extracts all the files contained inside to a new folder on your computer.\nUnzipping files on macOS is simple, but unzipping files on Windows can cause problems if you don’t pay careful attention.\nUnzipping files on macOS Double click on the downloaded .zip file. macOS will automatically create a new folder with the same name as the .zip file, and all the file’s contents will be inside. Double click on the RStudio Project file (.Rproj) to get started.\n Unzipping files on Windows A long story short: right click on the .zip file, select “Extract All…”, and work with the resulting unzipped folder.\nUnlike macOS, Windows does not automatically unzip things for you. If you double click on the .zip file, Windows will show you what’s inside, but it will do so without actually extracting anything. This is quite annoying. Here’s what it looks like—the only clues that this folder is really a .zip file are that there’s a “Compressed Folder Tools” tab at the top with “Extract” in red.\nIt is tempting to just open files from this view, but this causes problems. If you open the R Project file, for instance, RStudio will point to a bizarre working directory buried deep in some temporary folder:\nInstead, you need to right click on the .zip file and select “Extract All…”:\nThen choose where you want to unzip all the files and click on “Extract”\nYou should then finally have a real folder with all the contents of the zipped file. Open the R Project file and RStudio will point to the correct working directory and everything will work.\n ","date":1557010800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1632766478,"objectID":"f5e677f07c1b5f95f70e28c5d0f09b46","permalink":"https://metricsf21.classes.ryansafner.com/resources/unzipping_files/","publishdate":"2019-05-05T00:00:00+01:00","relpermalink":"/resources/unzipping_files/","section":"resources","summary":"Unzipping files on macOS Unzipping files on Windows   Since R projects typically consist of multiple files (R scripts, datasets, images, etc.) the easiest way to distribute and send them is to combine all the different files in to a single compressed .zip file. When you unzip a .zip file, your computer extracts all the files contained inside to a new folder on your computer.\nUnzipping files on macOS is simple, but unzipping files on Windows can cause problems if you don’t pay careful attention.","tags":null,"title":"Unzipping Files","type":"docs"},{"authors":null,"categories":null,"content":"         Extensions to ggplot2  Working with Scales  Subsetting Data ggrepel ggflag plotly Better barplots   Extensions to ggplot2 ggplot2, being one of the most popular packages, has a lot of user-made extensions that allow you to do lots of neat things with your plots, from plotting networks, Parliaments, dendrograms, and other types of graphs, to formatting and visual tools that help improve your figures.\nFor the following demonstrations, we will use the gapminder data. Let’s start just by making a basic graph and saving it as an object called p. I have decided to map (aes()) each geom_point’s color to continent and size to pop:\nlibrary(gapminder) library(ggplot2) p\u0026lt;-ggplot(gapminder) + aes(x = gdpPercap, y = lifeExp) + geom_point(aes(size = pop, color = continent)) p Working with Scales I don’t like the default choices ggplot2 has made for my point sizes for population, or the way it depicts them (in scientific notation) on the legend.\nI will set my own scale by setting the breaks1 manually, according to a vector I define as: c(100000, 1000000, 100000000, 1000000000). So, I will use one point size for populations of 100 thousand, a bigger one for a million, a bigger one for 100 millions, and the biggest for 1 billion.\nI am going to label these (on my legend) as the following vector: c(\"\u0026lt;1 million\",\"1 million\",\"100 million\", \"1 billion\").\nLastly, I don’t think the size of the billion circle is big enough, a billion is a lot of people! So I will set the range of sizes from size 1 point to size 10 point.\nTo do this, I include all of this inside the scale_size command (because I am scaling the size of points):\n# let\u0026#39;s save this as p2 p2\u0026lt;-p+scale_size(breaks = c(100000, 1000000, 100000000, 1000000000), # cut offs labels=c(\u0026quot;\u0026lt;1 million\u0026quot;,\u0026quot;1 million\u0026quot;,\u0026quot;100 million\u0026quot;, \u0026quot;1 billion\u0026quot;), # labels on legend range=c(1,10)) # min \u0026amp; max point size # let\u0026#39;s see what we did p2 This is also very hard to see the relationship (because it is nonlinear!). So I will rescale the x_axis logarithmically with base 10:\np2+scale_x_log10() Doing this already gives me a much clearer view of the relationship! But I don’t like the labels, or the breaks it has chosen, so I can customize them again:\np2+scale_x_log10( breaks = c(10^3, 10^4, 10^5), # 1,000, 10,000, and 100,000 labels = scales::dollar) The scales package has a nice command to format labels, in this case I am calling the scales::dollar function to print dollar signs in front of my axes numbers. I could have done it manually instead by setting labels = c(\"$1,000\", \"$10,000\", \"$100,000\").\n  Subsetting Data We learn more about this in class 1.4 using tidyverse, but let’s only look at one year of data (there’s too much going on in this plot, especially with the large points, some points are covering other points). So let’s only look at the year 2007. I can do this in two ways:\nSubset the data, save the subsetted data as an object (I’ll call gap2007), plot with that object as my data:  library(tidyverse) gap2007 \u0026lt;- gapminder %\u0026gt;% filter(year == 2007) p2007 \u0026lt;- ggplot(data = gap2007)+ aes(x = gdpPercap, y = lifeExp) + geom_point(aes(size = pop, color = continent)) p2007 Subset data and pipe it directly into ggplot2’s data argument:  library(tidyverse) p2007 \u0026lt;- gapminder %\u0026gt;% filter(year == 2007) %\u0026gt;% ggplot(data = .)+ # . is placeholder aes(x = gdpPercap, y = lifeExp) + geom_point(aes(size = pop, color = continent)) p2007 Now let’s clean up the graph with the same things I did before, hide the legends (set the color and size, my two aesthetic mappings, guides equal to FALSE), add some labels, and change the theme:\np3\u0026lt;-p2007+scale_size(breaks = c(100000, 1000000, 100000000, 1000000000), labels=c(\u0026quot;\u0026lt;1 million\u0026quot;,\u0026quot;1 million\u0026quot;,\u0026quot;100 million\u0026quot;, \u0026quot;1 billion\u0026quot;), range=c(1,10))+ scale_x_log10( breaks = c(10^3, 10^4, 10^5), labels = scales::dollar)+ labs(x = \u0026quot;GDP per Capita (USD)\u0026quot;, y = \u0026quot;Life Expectancy (years)\u0026quot;)+ guides(color = FALSE, size = FALSE)+ theme_classic() ## Warning: `guides(\u0026lt;scale\u0026gt; = FALSE)` is deprecated. Please use `guides(\u0026lt;scale\u0026gt; = ## \u0026quot;none\u0026quot;)` instead. p3  ggrepel If I were to try to label some countries, with either geom_text (just word) or geom_label() (text in a box), setting the label aesthetic to country, see what would happen:\np3+geom_label(aes(label = country, color = continent)) The labels, which are plotted right on top of each point, cover the points!\nSomeone figured out a clever way to let us do both, and it is a package called ggrepel, which allows you to plot labels that are “repelled” away from the point they are labelling in an intelligent way. This is a separate package, which you must first install and then load with library to use it!\n# install.packages(\u0026quot;ggrepel\u0026quot;) # do this only once library(ggrepel) p3+geom_text_repel(aes(label = country, color = continent, size = pop), size = 3) ## Warning: ggrepel: 69 unlabeled data points (too many overlaps). Consider ## increasing max.overlaps This is much better, but for this particular chart, since a lot of observations are close together, it would be unwise to label everything, perhaps only label a subset of important points.\n ggflag One alternative is instead of points, to use some other marking. Someone created the ggflags package to let you plot flags of countries. This creates a new type of geom, called geom_flag, that requires you to map the country aesthetic to a variable in your data with the country name (incidentally, in gapminder that variable is also called country). Let’s try that out instead (and add my same customizations as above):\n# install.packages(\u0026quot;ggflags\u0026quot;) # do this only once library(ggflags) pflag\u0026lt;-p3+geom_flag(aes(country = country, size = pop)) pflag  plotly We can also make our plot a bit more interactive (on web only of course!) using the ggplotly package, which allows ggplot2 to interface with a javascript library called plotly.2\n# install.packages(\u0026quot;plotly\u0026quot;) # do this only once library(plotly) ggplotly(p3)  {\"x\":{\"data\":[{\"x\":[3.79402544509666,3.68099065562682,3.1587498285201,4.09933015629768,3.08530235219712,2.63353984732021,3.31007599301138,2.848814873635,3.23148583131337,2.99394204506796,2.44334414004637,3.56021253312911,3.18885823537455,3.31858116595608,3.74672610693845,4.08472243900371,2.80710831941093,2.83935583435696,4.12078722664154,2.87665060639063,3.1230701584329,2.9743524119827,2.76285235400399,3.16531831968284,3.19571467604724,2.61753222691212,4.08125724466051,3.01902074595738,2.88044194586588,3.0181100381011,3.25603221652431,4.03969130967891,3.58208328429303,2.91576148434958,3.68224081186337,2.79216530171644,3.30405457229069,3.88480230343241,2.93605531183875,3.20369500466057,3.23362351372365,2.93577962517273,2.96667714274902,3.96706370233863,3.41537321502711,3.65451158423759,3.04433674769752,2.94594592049266,3.85082524675123,3.02382022004224,3.10421784482941,2.67182915731664],\"y\":[72.301,42.731,56.728,50.728,52.295,49.58,50.43,44.741,50.651,65.152,46.462,55.322,48.328,54.791,71.338,51.579,58.04,52.947,56.735,59.448,60.022,56.007,46.388,54.11,42.592,45.678,73.952,59.443,48.303,54.467,64.164,72.801,71.164,42.082,52.906,56.867,46.859,76.442,46.242,65.528,63.062,42.568,48.159,49.339,58.556,39.613,52.517,58.42,73.923,51.542,42.384,43.487],\"text\":[\"gdpPercap: 6223.3675\nlifeExp: 72.301\npop: 33333216\ncontinent: Africa\",\"gdpPercap: 4797.2313\nlifeExp: 42.731\npop: 12420476\ncontinent: Africa\",\"gdpPercap: 1441.2849\nlifeExp: 56.728\npop: 8078314\ncontinent: Africa\",\"gdpPercap: 12569.8518\nlifeExp: 50.728\npop: 1639131\ncontinent: Africa\",\"gdpPercap: 1217.0330\nlifeExp: 52.295\npop: 14326203\ncontinent: Africa\",\"gdpPercap: 430.0707\nlifeExp: 49.580\npop: 8390505\ncontinent: Africa\",\"gdpPercap: 2042.0952\nlifeExp: 50.430\npop: 17696293\ncontinent: Africa\",\"gdpPercap: 706.0165\nlifeExp: 44.741\npop: 4369038\ncontinent: Africa\",\"gdpPercap: 1704.0637\nlifeExp: 50.651\npop: 10238807\ncontinent: Africa\",\"gdpPercap: 986.1479\nlifeExp: 65.152\npop: 710960\ncontinent: Africa\",\"gdpPercap: 277.5519\nlifeExp: 46.462\npop: 64606759\ncontinent: Africa\",\"gdpPercap: 3632.5578\nlifeExp: 55.322\npop: 3800610\ncontinent: Africa\",\"gdpPercap: 1544.7501\nlifeExp: 48.328\npop: 18013409\ncontinent: Africa\",\"gdpPercap: 2082.4816\nlifeExp: 54.791\npop: 496374\ncontinent: Africa\",\"gdpPercap: 5581.1810\nlifeExp: 71.338\npop: 80264543\ncontinent: Africa\",\"gdpPercap: 12154.0897\nlifeExp: 51.579\npop: 551201\ncontinent: Africa\",\"gdpPercap: 641.3695\nlifeExp: 58.040\npop: 4906585\ncontinent: Africa\",\"gdpPercap: 690.8056\nlifeExp: 52.947\npop: 76511887\ncontinent: Africa\",\"gdpPercap: 13206.4845\nlifeExp: 56.735\npop: 1454867\ncontinent: Africa\",\"gdpPercap: 752.7497\nlifeExp: 59.448\npop: 1688359\ncontinent: Africa\",\"gdpPercap: 1327.6089\nlifeExp: 60.022\npop: 22873338\ncontinent: Africa\",\"gdpPercap: 942.6542\nlifeExp: 56.007\npop: 9947814\ncontinent: Africa\",\"gdpPercap: 579.2317\nlifeExp: 46.388\npop: 1472041\ncontinent: Africa\",\"gdpPercap: 1463.2493\nlifeExp: 54.110\npop: 35610177\ncontinent: Africa\",\"gdpPercap: 1569.3314\nlifeExp: 42.592\npop: 2012649\ncontinent: Africa\",\"gdpPercap: 414.5073\nlifeExp: 45.678\npop: 3193942\ncontinent: Africa\",\"gdpPercap: 12057.4993\nlifeExp: 73.952\npop: 6036914\ncontinent: Africa\",\"gdpPercap: 1044.7701\nlifeExp: 59.443\npop: 19167654\ncontinent: Africa\",\"gdpPercap: 759.3499\nlifeExp: 48.303\npop: 13327079\ncontinent: Africa\",\"gdpPercap: 1042.5816\nlifeExp: 54.467\npop: 12031795\ncontinent: Africa\",\"gdpPercap: 1803.1515\nlifeExp: 64.164\npop: 3270065\ncontinent: Africa\",\"gdpPercap: 10956.9911\nlifeExp: 72.801\npop: 1250882\ncontinent: Africa\",\"gdpPercap: 3820.1752\nlifeExp: 71.164\npop: 33757175\ncontinent: Africa\",\"gdpPercap: 823.6856\nlifeExp: 42.082\npop: 19951656\ncontinent: Africa\",\"gdpPercap: 4811.0604\nlifeExp: 52.906\npop: 2055080\ncontinent: Africa\",\"gdpPercap: 619.6769\nlifeExp: 56.867\npop: 12894865\ncontinent: Africa\",\"gdpPercap: 2013.9773\nlifeExp: 46.859\npop: 135031164\ncontinent: Africa\",\"gdpPercap: 7670.1226\nlifeExp: 76.442\npop: 798094\ncontinent: Africa\",\"gdpPercap: 863.0885\nlifeExp: 46.242\npop: 8860588\ncontinent: Africa\",\"gdpPercap: 1598.4351\nlifeExp: 65.528\npop: 199579\ncontinent: Africa\",\"gdpPercap: 1712.4721\nlifeExp: 63.062\npop: 12267493\ncontinent: Africa\",\"gdpPercap: 862.5408\nlifeExp: 42.568\npop: 6144562\ncontinent: Africa\",\"gdpPercap: 926.1411\nlifeExp: 48.159\npop: 9118773\ncontinent: Africa\",\"gdpPercap: 9269.6578\nlifeExp: 49.339\npop: 43997828\ncontinent: Africa\",\"gdpPercap: 2602.3950\nlifeExp: 58.556\npop: 42292929\ncontinent: Africa\",\"gdpPercap: 4513.4806\nlifeExp: 39.613\npop: 1133066\ncontinent: Africa\",\"gdpPercap: 1107.4822\nlifeExp: 52.517\npop: 38139640\ncontinent: Africa\",\"gdpPercap: 882.9699\nlifeExp: 58.420\npop: 5701579\ncontinent: Africa\",\"gdpPercap: 7092.9230\nlifeExp: 73.923\npop: 10276158\ncontinent: Africa\",\"gdpPercap: 1056.3801\nlifeExp: 51.542\npop: 29170398\ncontinent: Africa\",\"gdpPercap: 1271.2116\nlifeExp: 42.384\npop: 11746035\ncontinent: Africa\",\"gdpPercap: 469.7093\nlifeExp: 43.487\npop: 12311143\ncontinent: Africa\"],\"type\":\"scatter\",\"mode\":\"markers\",\"marker\":{\"autocolorscale\":false,\"color\":\"rgba(248,118,109,1)\",\"opacity\":1,\"size\":[9.17186707134452,7.05439869410931,6.40901390549693,4.9035017839512,7.30049384335335,6.46060386522178,7.69803720253323,5.69238465892928,6.7477253137447,4.44943525570712,11.2976572925347,5.55721786228597,7.7333879014621,4.28988090166577,12.1618416439493,4.33502324865599,5.81195499746525,11.963044159878,4.82910469643636,4.92255836798588,8.24024214783767,6.70439150114293,4.83626010737606,9.35407099736227,5.04091923115731,5.40057191086803,6.04286851580263,7.85947250211585,7.17369831222697,7.00189987699689,5.42104776974703,4.7400480303731,9.20625605614534,7.94293638389671,5.05559394837149,7.11735528623198,14.6572630326479,4.50426415706232,6.53646504420609,3.77952755905512,7.03383648011053,6.06364264377998,6.57725548979649,9.97923288155683,9.85736973717032,4.68462807101931,9.54973844613408,5.97689642333099,6.75324178508698,8.82176229393711,6.96275027917212,7.03971663329602],\"symbol\":\"circle\",\"line\":{\"width\":1.88976377952756,\"color\":\"rgba(248,118,109,1)\"}},\"hoveron\":\"points\",\"name\":\"Africa\",\"legendgroup\":\"Africa\",\"showlegend\":true,\"xaxis\":\"x\",\"yaxis\":\"y\",\"hoverinfo\":\"text\",\"frame\":null},{\"x\":[4.10650977201769,3.58230625931688,3.95740617340857,4.56013669242519,4.11963981438709,3.84550611104731,3.98430499758718,3.95173097080165,3.77998406330737,3.83716291957619,3.75802981175127,3.71483669984604,3.0797733480287,3.55002410662006,3.86456330372846,4.07836889751077,3.43922544380543,3.99163295350919,3.6204315729333,3.86975405900188,4.28620284789525,4.25547776303394,4.632979883279,4.02577526370184,4.05750656798335],\"y\":[75.32,65.554,72.39,80.653,78.553,72.889,78.782,78.273,72.235,74.994,71.878,70.259,60.916,70.198,72.567,76.195,72.899,75.537,71.752,71.421,78.746,69.819,78.242,76.384,73.747],\"text\":[\"gdpPercap: 12779.3796\nlifeExp: 75.320\npop: 40301927\ncontinent: Americas\",\"gdpPercap: 3822.1371\nlifeExp: 65.554\npop: 9119152\ncontinent: Americas\",\"gdpPercap: 9065.8008\nlifeExp: 72.390\npop: 190010647\ncontinent: Americas\",\"gdpPercap: 36319.2350\nlifeExp: 80.653\npop: 33390141\ncontinent: Americas\",\"gdpPercap: 13171.6388\nlifeExp: 78.553\npop: 16284741\ncontinent: Americas\",\"gdpPercap: 7006.5804\nlifeExp: 72.889\npop: 44227550\ncontinent: Americas\",\"gdpPercap: 9645.0614\nlifeExp: 78.782\npop: 4133884\ncontinent: Americas\",\"gdpPercap: 8948.1029\nlifeExp: 78.273\npop: 11416987\ncontinent: Americas\",\"gdpPercap: 6025.3748\nlifeExp: 72.235\npop: 9319622\ncontinent: Americas\",\"gdpPercap: 6873.2623\nlifeExp: 74.994\npop: 13755680\ncontinent: Americas\",\"gdpPercap: 5728.3535\nlifeExp: 71.878\npop: 6939688\ncontinent: Americas\",\"gdpPercap: 5186.0500\nlifeExp: 70.259\npop: 12572928\ncontinent: Americas\",\"gdpPercap: 1201.6372\nlifeExp: 60.916\npop: 8502814\ncontinent: Americas\",\"gdpPercap: 3548.3308\nlifeExp: 70.198\npop: 7483763\ncontinent: Americas\",\"gdpPercap: 7320.8803\nlifeExp: 72.567\npop: 2780132\ncontinent: Americas\",\"gdpPercap: 11977.5750\nlifeExp: 76.195\npop: 108700891\ncontinent: Americas\",\"gdpPercap: 2749.3210\nlifeExp: 72.899\npop: 5675356\ncontinent: Americas\",\"gdpPercap: 9809.1856\nlifeExp: 75.537\npop: 3242173\ncontinent: Americas\",\"gdpPercap: 4172.8385\nlifeExp: 71.752\npop: 6667147\ncontinent: Americas\",\"gdpPercap: 7408.9056\nlifeExp: 71.421\npop: 28674757\ncontinent: Americas\",\"gdpPercap: 19328.7090\nlifeExp: 78.746\npop: 3942491\ncontinent: Americas\",\"gdpPercap: 18008.5092\nlifeExp: 69.819\npop: 1056608\ncontinent: Americas\",\"gdpPercap: 42951.6531\nlifeExp: 78.242\npop: 301139947\ncontinent: Americas\",\"gdpPercap: 10611.4630\nlifeExp: 76.384\npop: 3447496\ncontinent: Americas\",\"gdpPercap: 11415.8057\nlifeExp: 73.747\npop: 26084662\ncontinent: Americas\"],\"type\":\"scatter\",\"mode\":\"markers\",\"marker\":{\"autocolorscale\":false,\"color\":\"rgba(163,165,0,1)\",\"opacity\":1,\"size\":[9.71188859996772,6.5773149305723,16.6858776044272,9.17649721801001,7.53665038513786,9.99547035822454,5.63766008617189,6.91706510514433,6.60858077236626,7.22866159247969,6.21159791037081,7.07476189901512,6.47892193200982,6.30785388802791,5.28439783802595,13.5375130560296,5.97165374565047,5.41357505468652,6.16191930215675,8.77844422821804,5.59190013370175,4.6467698186458,20.0306325948774,5.46781012065641,8.54567479416711],\"symbol\":\"circle\",\"line\":{\"width\":1.88976377952756,\"color\":\"rgba(163,165,0,1)\"}},\"hoveron\":\"points\",\"name\":\"Americas\",\"legendgroup\":\"Americas\",\"showlegend\":true,\"xaxis\":\"x\",\"yaxis\":\"y\",\"hoverinfo\":\"text\",\"frame\":null},{\"x\":[2.98881764549129,4.47415867018597,3.143406361057,3.23395473727517,3.6954041667466,4.59906367268864,3.38955773121288,3.54908318988216,4.06467188236304,3.65041068311967,4.40693643553008,4.50045697098462,3.65508665949928,3.20223362703668,4.3682522837381,4.67492531406996,4.01957563822304,4.09522710701315,3.49076900592376,2.97497199429807,3.03796794384465,4.34862010618943,3.41596567540354,3.50385616482709,4.33555481758265,4.67341887079622,3.59880094359832,3.62164856315343,4.45815837772449,3.87264545733796,3.38767031916547,3.4807755960326,3.35808147396803],\"y\":[43.828,75.635,64.062,59.723,72.961,82.208,64.698,70.65,70.964,59.545,80.745,82.603,72.535,67.297,78.623,77.588,71.993,74.241,66.803,62.069,63.785,75.64,65.483,71.688,72.777,79.972,72.396,74.143,78.4,70.616,74.249,73.422,62.698],\"text\":[\"gdpPercap: 974.5803\nlifeExp: 43.828\npop: 31889923\ncontinent: Asia\",\"gdpPercap: 29796.0483\nlifeExp: 75.635\npop: 708573\ncontinent: Asia\",\"gdpPercap: 1391.2538\nlifeExp: 64.062\npop: 150448339\ncontinent: Asia\",\"gdpPercap: 1713.7787\nlifeExp: 59.723\npop: 14131858\ncontinent: Asia\",\"gdpPercap: 4959.1149\nlifeExp: 72.961\npop: 1318683096\ncontinent: Asia\",\"gdpPercap: 39724.9787\nlifeExp: 82.208\npop: 6980412\ncontinent: Asia\",\"gdpPercap: 2452.2104\nlifeExp: 64.698\npop: 1110396331\ncontinent: Asia\",\"gdpPercap: 3540.6516\nlifeExp: 70.650\npop: 223547000\ncontinent: Asia\",\"gdpPercap: 11605.7145\nlifeExp: 70.964\npop: 69453570\ncontinent: Asia\",\"gdpPercap: 4471.0619\nlifeExp: 59.545\npop: 27499638\ncontinent: Asia\",\"gdpPercap: 25523.2771\nlifeExp: 80.745\npop: 6426679\ncontinent: Asia\",\"gdpPercap: 31656.0681\nlifeExp: 82.603\npop: 127467972\ncontinent: Asia\",\"gdpPercap: 4519.4612\nlifeExp: 72.535\npop: 6053193\ncontinent: Asia\",\"gdpPercap: 1593.0655\nlifeExp: 67.297\npop: 23301725\ncontinent: Asia\",\"gdpPercap: 23348.1397\nlifeExp: 78.623\npop: 49044790\ncontinent: Asia\",\"gdpPercap: 47306.9898\nlifeExp: 77.588\npop: 2505559\ncontinent: Asia\",\"gdpPercap: 10461.0587\nlifeExp: 71.993\npop: 3921278\ncontinent: Asia\",\"gdpPercap: 12451.6558\nlifeExp: 74.241\npop: 24821286\ncontinent: Asia\",\"gdpPercap: 3095.7723\nlifeExp: 66.803\npop: 2874127\ncontinent: Asia\",\"gdpPercap: 944.0000\nlifeExp: 62.069\npop: 47761980\ncontinent: Asia\",\"gdpPercap: 1091.3598\nlifeExp: 63.785\npop: 28901790\ncontinent: Asia\",\"gdpPercap: 22316.1929\nlifeExp: 75.640\npop: 3204897\ncontinent: Asia\",\"gdpPercap: 2605.9476\nlifeExp: 65.483\npop: 169270617\ncontinent: Asia\",\"gdpPercap: 3190.4810\nlifeExp: 71.688\npop: 91077287\ncontinent: Asia\",\"gdpPercap: 21654.8319\nlifeExp: 72.777\npop: 27601038\ncontinent: Asia\",\"gdpPercap: 47143.1796\nlifeExp: 79.972\npop: 4553009\ncontinent: Asia\",\"gdpPercap: 3970.0954\nlifeExp: 72.396\npop: 20378239\ncontinent: Asia\",\"gdpPercap: 4184.5481\nlifeExp: 74.143\npop: 19314747\ncontinent: Asia\",\"gdpPercap: 28718.2768\nlifeExp: 78.400\npop: 23174294\ncontinent: Asia\",\"gdpPercap: 7458.3963\nlifeExp: 70.616\npop: 65068149\ncontinent: Asia\",\"gdpPercap: 2441.5764\nlifeExp: 74.249\npop: 85262356\ncontinent: Asia\",\"gdpPercap: 3025.3498\nlifeExp: 73.422\npop: 4018332\ncontinent: Asia\",\"gdpPercap: 2280.7699\nlifeExp: 62.698\npop: 22211743\ncontinent: Asia\"],\"type\":\"scatter\",\"mode\":\"markers\",\"marker\":{\"autocolorscale\":false,\"color\":\"rgba(0,191,125,1)\",\"opacity\":1,\"size\":[9.05311498519542,4.44786994524866,15.2623360474798,7.27619037210708,37.7952755905512,6.21893417725337,34.9930405243088,17.7796922474324,11.5754063207234,8.67420938599383,6.1172104237703,14.3477745253016,6.0460222901843,8.28218425481713,10.3266983238546,5.20208706341552,5.58675701419557,8.42790893914936,5.31155968285997,10.2401529293716,8.79833288071788,5.40353453228879,15.9603688022812,12.7099366962826,8.68329106458849,5.73413008778765,7.9876545260984,7.87526146228488,8.26974879812008,11.3245378339389,12.4195018826322,5.61016970980921,8.17468101875895],\"symbol\":\"circle\",\"line\":{\"width\":1.88976377952756,\"color\":\"rgba(0,191,125,1)\"}},\"hoveron\":\"points\",\"name\":\"Asia\",\"legendgroup\":\"Asia\",\"showlegend\":true,\"xaxis\":\"x\",\"yaxis\":\"y\",\"hoverinfo\":\"text\",\"frame\":null},{\"x\":[3.77356920876217,4.55782580062218,4.52753459151331,3.87194045947979,4.02860349094832,4.16492428251317,4.35856884462783,4.54750911081685,4.52123074598147,4.48387269225076,4.50745611553652,4.43993889119905,4.25548825822109,4.55847803563392,4.60933819969144,4.45590597950278,3.96632461957789,4.56582342810493,4.69335042800253,4.18723649435127,4.31195820191596,4.03376444688129,3.990628940935,4.27133768016358,4.41108505321789,4.4597100052953,4.52968372605252,4.57410560181062,3.92728187211911,4.52118074293136],\"y\":[76.423,79.829,79.441,74.852,73.005,75.748,76.486,78.332,79.313,80.657,79.406,79.483,73.338,81.757,78.885,80.546,74.543,79.762,80.196,75.563,78.098,72.476,74.002,74.663,77.926,80.941,80.884,81.701,71.777,79.425],\"text\":[\"gdpPercap: 5937.0295\nlifeExp: 76.423\npop: 3600523\ncontinent: Europe\",\"gdpPercap: 36126.4927\nlifeExp: 79.829\npop: 8199783\ncontinent: Europe\",\"gdpPercap: 33692.6051\nlifeExp: 79.441\npop: 10392226\ncontinent: Europe\",\"gdpPercap: 7446.2988\nlifeExp: 74.852\npop: 4552198\ncontinent: Europe\",\"gdpPercap: 10680.7928\nlifeExp: 73.005\npop: 7322858\ncontinent: Europe\",\"gdpPercap: 14619.2227\nlifeExp: 75.748\npop: 4493312\ncontinent: Europe\",\"gdpPercap: 22833.3085\nlifeExp: 76.486\npop: 10228744\ncontinent: Europe\",\"gdpPercap: 35278.4187\nlifeExp: 78.332\npop: 5468120\ncontinent: Europe\",\"gdpPercap: 33207.0844\nlifeExp: 79.313\npop: 5238460\ncontinent: Europe\",\"gdpPercap: 30470.0167\nlifeExp: 80.657\npop: 61083916\ncontinent: Europe\",\"gdpPercap: 32170.3744\nlifeExp: 79.406\npop: 82400996\ncontinent: Europe\",\"gdpPercap: 27538.4119\nlifeExp: 79.483\npop: 10706290\ncontinent: Europe\",\"gdpPercap: 18008.9444\nlifeExp: 73.338\npop: 9956108\ncontinent: Europe\",\"gdpPercap: 36180.7892\nlifeExp: 81.757\npop: 301931\ncontinent: Europe\",\"gdpPercap: 40675.9964\nlifeExp: 78.885\npop: 4109086\ncontinent: Europe\",\"gdpPercap: 28569.7197\nlifeExp: 80.546\npop: 58147733\ncontinent: Europe\",\"gdpPercap: 9253.8961\nlifeExp: 74.543\npop: 684736\ncontinent: Europe\",\"gdpPercap: 36797.9333\nlifeExp: 79.762\npop: 16570613\ncontinent: Europe\",\"gdpPercap: 49357.1902\nlifeExp: 80.196\npop: 4627926\ncontinent: Europe\",\"gdpPercap: 15389.9247\nlifeExp: 75.563\npop: 38518241\ncontinent: Europe\",\"gdpPercap: 20509.6478\nlifeExp: 78.098\npop: 10642836\ncontinent: Europe\",\"gdpPercap: 10808.4756\nlifeExp: 72.476\npop: 22276056\ncontinent: Europe\",\"gdpPercap: 9786.5347\nlifeExp: 74.002\npop: 10150265\ncontinent: Europe\",\"gdpPercap: 18678.3144\nlifeExp: 74.663\npop: 5447502\ncontinent: Europe\",\"gdpPercap: 25768.2576\nlifeExp: 77.926\npop: 2009245\ncontinent: Europe\",\"gdpPercap: 28821.0637\nlifeExp: 80.941\npop: 40448191\ncontinent: Europe\",\"gdpPercap: 33859.7484\nlifeExp: 80.884\npop: 9031088\ncontinent: Europe\",\"gdpPercap: 37506.4191\nlifeExp: 81.701\npop: 7554661\ncontinent: Europe\",\"gdpPercap: 8458.2764\nlifeExp: 71.777\npop: 71158647\ncontinent: Europe\",\"gdpPercap: 33203.2613\nlifeExp: 79.425\npop: 60776238\ncontinent: Europe\"],\"type\":\"scatter\",\"mode\":\"markers\",\"marker\":{\"autocolorscale\":false,\"color\":\"rgba(0,176,246,1)\",\"opacity\":1,\"size\":[5.50712445033372,6.42920619519428,6.77031924892042,5.73394801796184,6.27977303928336,5.72068245173131,6.74623732768221,5.929772108609,5.88238448143105,11.0891587319513,12.27294218016,6.81604700534319,6.70563550396768,4.07923038647061,5.63179490697003,10.9107252520474,4.43203252970621,7.56988990169337,5.75087646240843,9.57845722742051,6.80686377461846,8.18109700174399,6.73460716335436,5.92556058199178,5.03973455695427,9.72269720797748,6.56346922779363,6.32012837309692,11.6707925032296,11.0706657881971],\"symbol\":\"circle\",\"line\":{\"width\":1.88976377952756,\"color\":\"rgba(0,176,246,1)\"}},\"hoveron\":\"points\",\"name\":\"Europe\",\"legendgroup\":\"Europe\",\"showlegend\":true,\"xaxis\":\"x\",\"yaxis\":\"y\",\"hoverinfo\":\"text\",\"frame\":null},{\"x\":[4.53700472145269,4.40114211228811],\"y\":[81.235,80.204],\"text\":[\"gdpPercap: 34435.3674\nlifeExp: 81.235\npop: 20434176\ncontinent: Oceania\",\"gdpPercap: 25185.0091\nlifeExp: 80.204\npop: 4115771\ncontinent: Oceania\"],\"type\":\"scatter\",\"mode\":\"markers\",\"marker\":{\"autocolorscale\":false,\"color\":\"rgba(231,107,243,1)\",\"opacity\":1,\"size\":[7.99348313645917,5.63337785828153],\"symbol\":\"circle\",\"line\":{\"width\":1.88976377952756,\"color\":\"rgba(231,107,243,1)\"}},\"hoveron\":\"points\",\"name\":\"Oceania\",\"legendgroup\":\"Oceania\",\"showlegend\":true,\"xaxis\":\"x\",\"yaxis\":\"y\",\"hoverinfo\":\"text\",\"frame\":null}],\"layout\":{\"margin\":{\"t\":26.2283105022831,\"r\":7.30593607305936,\"b\":40.1826484018265,\"l\":37.2602739726027},\"plot_bgcolor\":\"rgba(255,255,255,1)\",\"paper_bgcolor\":\"rgba(255,255,255,1)\",\"font\":{\"color\":\"rgba(0,0,0,1)\",\"family\":\"\",\"size\":14.6118721461187},\"xaxis\":{\"domain\":[0,1],\"automargin\":true,\"type\":\"linear\",\"autorange\":false,\"range\":[2.33084382564856,4.80585074240034],\"tickmode\":\"array\",\"ticktext\":[\"$1,000\",\"$10,000\"],\"tickvals\":[3,4],\"categoryorder\":\"array\",\"categoryarray\":[\"$1,000\",\"$10,000\"],\"nticks\":null,\"ticks\":\"outside\",\"tickcolor\":\"rgba(51,51,51,1)\",\"ticklen\":3.65296803652968,\"tickwidth\":0.66417600664176,\"showticklabels\":true,\"tickfont\":{\"color\":\"rgba(77,77,77,1)\",\"family\":\"\",\"size\":11.689497716895},\"tickangle\":-0,\"showline\":true,\"linecolor\":\"rgba(0,0,0,1)\",\"linewidth\":0.66417600664176,\"showgrid\":false,\"gridcolor\":null,\"gridwidth\":0,\"zeroline\":false,\"anchor\":\"y\",\"title\":{\"text\":\"GDP per Capita (USD)\",\"font\":{\"color\":\"rgba(0,0,0,1)\",\"family\":\"\",\"size\":14.6118721461187}},\"hoverformat\":\".2f\"},\"yaxis\":{\"domain\":[0,1],\"automargin\":true,\"type\":\"linear\",\"autorange\":false,\"range\":[37.4635,84.7525],\"tickmode\":\"array\",\"ticktext\":[\"40\",\"50\",\"60\",\"70\",\"80\"],\"tickvals\":[40,50,60,70,80],\"categoryorder\":\"array\",\"categoryarray\":[\"40\",\"50\",\"60\",\"70\",\"80\"],\"nticks\":null,\"ticks\":\"outside\",\"tickcolor\":\"rgba(51,51,51,1)\",\"ticklen\":3.65296803652968,\"tickwidth\":0.66417600664176,\"showticklabels\":true,\"tickfont\":{\"color\":\"rgba(77,77,77,1)\",\"family\":\"\",\"size\":11.689497716895},\"tickangle\":-0,\"showline\":true,\"linecolor\":\"rgba(0,0,0,1)\",\"linewidth\":0.66417600664176,\"showgrid\":false,\"gridcolor\":null,\"gridwidth\":0,\"zeroline\":false,\"anchor\":\"x\",\"title\":{\"text\":\"Life Expectancy (years)\",\"font\":{\"color\":\"rgba(0,0,0,1)\",\"family\":\"\",\"size\":14.6118721461187}},\"hoverformat\":\".2f\"},\"shapes\":[{\"type\":\"rect\",\"fillcolor\":null,\"line\":{\"color\":null,\"width\":0,\"linetype\":[]},\"yref\":\"paper\",\"xref\":\"paper\",\"x0\":0,\"x1\":1,\"y0\":0,\"y1\":1}],\"showlegend\":true,\"legend\":{\"bgcolor\":\"rgba(255,255,255,1)\",\"bordercolor\":\"transparent\",\"borderwidth\":1.88976377952756,\"font\":{\"color\":\"rgba(0,0,0,1)\",\"family\":\"\",\"size\":11.689497716895},\"y\":1},\"hovermode\":\"closest\",\"barmode\":\"relative\"},\"config\":{\"doubleClick\":\"reset\",\"showSendToCloud\":false},\"source\":\"A\",\"attrs\":{\"63834da0363d\":{\"x\":{},\"y\":{},\"size\":{},\"colour\":{},\"type\":\"scatter\"}},\"cur_data\":\"63834da0363d\",\"visdat\":{\"63834da0363d\":[\"function (y) \",\"x\"]},\"highlight\":{\"on\":\"plotly_click\",\"persistent\":false,\"dynamic\":false,\"selectize\":false,\"opacityDim\":0.2,\"selected\":{\"opacity\":1},\"debounce\":0},\"shinyEvents\":[\"plotly_hover\",\"plotly_click\",\"plotly_selected\",\"plotly_relayout\",\"plotly_brushed\",\"plotly_brushing\",\"plotly_clickannotation\",\"plotly_doubleclick\",\"plotly_deselect\",\"plotly_afterplot\",\"plotly_sunburstclick\"],\"base_url\":\"https://plot.ly\"},\"evals\":[],\"jsHooks\":[]}  Better barplots Another major type of plot that we may use often is a barplot. Suppose we want to show the GDP per Capita of the top 20 countries in 2007. If I were to plot country on the x axis and gdpPercap on the y axis with geom_col,3 I get the following mess:\nggplot(gap2007)+ aes(x = country, y = gdpPercap, fill = continent)+ geom_col() So let’s filter4 arrange our data in descending order by gdpPercap:\ngap2007 %\u0026gt;% arrange(desc(gdpPercap)) ## # A tibble: 142 × 6 ## country continent year lifeExp pop gdpPercap ## \u0026lt;fct\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; ## 1 Norway Europe 2007 80.2 4627926 49357. ## 2 Kuwait Asia 2007 77.6 2505559 47307. ## 3 Singapore Asia 2007 80.0 4553009 47143. ## 4 United States Americas 2007 78.2 301139947 42952. ## 5 Ireland Europe 2007 78.9 4109086 40676. ## 6 Hong Kong, China Asia 2007 82.2 6980412 39725. ## 7 Switzerland Europe 2007 81.7 7554661 37506. ## 8 Netherlands Europe 2007 79.8 16570613 36798. ## 9 Canada Americas 2007 80.7 33390141 36319. ## 10 Iceland Europe 2007 81.8 301931 36181. ## # … with 132 more rows We only want the top 20 observations, so lets slice to extract just rows 1:20. Then we’ll pipe it into our plot:\nbar\u0026lt;-gap2007 %\u0026gt;% arrange(desc(gdpPercap)) %\u0026gt;% slice(1:20) %\u0026gt;% ggplot(data = .)+ aes(x = country, y = gdpPercap, fill = continent)+ geom_col() bar Now that’s closer to what we wanted! But here are a few more tips and tricks to make it better. First, let’s flip the axes to be able to read the countries better, using coord_flip()\nbar+coord_flip() One other useful thing to know would be how to display the bars in numerical order (from largest gdpPercap to smalleset gdpPercap) so we can get a clear ranking of countries. To do this, we are going to make use of another tidyverse package called forcats (dealing with factors), specifically the function fct_reorder(), which reorders a factor variable (our country) by the values of some other variable (our gdpPercap). We need to do this to our x variable aesthetic:\nbar2\u0026lt;-gap2007 %\u0026gt;% arrange(desc(gdpPercap)) %\u0026gt;% slice(1:20) %\u0026gt;% ggplot(data = .)+ aes(x = forcats::fct_reorder(country, gdpPercap), #\u0026lt;\u0026lt; y = gdpPercap, fill = continent)+ geom_col()+ coord_flip() bar2 This is already looking good. Here’s another creative way to depict the same thing in a more visually-striking way. Instead of using geom_bar(), let’s combine geom_flag (to serve as end points) and geom_segment()5 (line segments) to make the following version:\nbar3\u0026lt;-gap2007 %\u0026gt;% arrange(desc(gdpPercap)) %\u0026gt;% slice(1:20) %\u0026gt;% ggplot(data = .)+ aes(x = forcats::fct_reorder(country, gdpPercap), #\u0026lt;\u0026lt; y = gdpPercap, fill = continent)+ geom_segment(aes(x = forcats::fct_reorder(country, gdpPercap), #\u0026lt;\u0026lt; y = 0, #\u0026lt;\u0026lt; xend = country, #\u0026lt;\u0026lt; yend = gdpPercap, #\u0026lt;\u0026lt; color = continent), #\u0026lt;\u0026lt; size = 1)+ #\u0026lt;\u0026lt; geom_flag(aes(country = country))+ #\u0026lt;\u0026lt; coord_flip() bar3   The cut offs for using one size circle vs. the next/previous size circle, depending on population.↩︎\n I am plotting our graph from above, not the flags, since ggflags has not been configured for plotly yet.↩︎\n Using geom_col() allows you to specify an x and a y variable. geom_bar() only plots the counts of each value on the x axis. If we had done +aes(x = country)+geom_bar(), it would plot the number of observations of each country in the data, not what we want!↩︎\n You learn about this in class 1.4↩︎\n Note because we are defining an x aesthetic again for this, we need to make sure x is also the reordered list of countries, so note I am doing the whole fct_reorder() again.↩︎\n   ","date":1557010800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1632766478,"objectID":"a419ffeb40ed64d43521b8d8437e73a1","permalink":"https://metricsf21.classes.ryansafner.com/resources/ggplot2/","publishdate":"2019-05-05T00:00:00+01:00","relpermalink":"/resources/ggplot2/","section":"resources","summary":"Extensions to ggplot2  Working with Scales  Subsetting Data ggrepel ggflag plotly Better barplots   Extensions to ggplot2 ggplot2, being one of the most popular packages, has a lot of user-made extensions that allow you to do lots of neat things with your plots, from plotting networks, Parliaments, dendrograms, and other types of graphs, to formatting and visual tools that help improve your figures.","tags":null,"title":"ggplot2 Extensions","type":"docs"},{"authors":null,"categories":null,"content":"    Overview  Readings  Slides  Assignments  Preliminary Statistics Survey Due Sunday August 29     Tuesday, August 24, 2021    Please complete the preliminary survey by Sunday August 29.    Overview Welcome to ECON 480 — Econometrics! Today’s lesson will be an overview of the content and the assignments of the course. Please read and familiarize yourself with the syllabus.\nThis is not just a “syllabus day,” as we need to hit the ground running, beginning with learning the software we will be using this semester. Starting next class, we will do a deep dive in to R for about 2 weeks. In preparation, please do the following before next class:\nGo to RStudio.cloud and register a (free) account with your Hood details.\n Try to install R and R Studio on your computer if you are able, before next class. You will still always have RStudio.cloud available all to use in your browser, but it is much better to have a real version of R and R Studio on your own machine to work with all semester.\n    Readings Today is introductory, but please heed this timeless message:\nA message to students from the Doggfather himself, @SnoopDogg pic.twitter.com/wsSANYv8u6 — Ryan Briggs (@ryancbriggs) August 12, 2020   Please note going forward, the lesson numbers and topics (e.g. 1.1) on this website are my design, and will not match up with the textbook!\n  Slides Below, you can find the slides in two formats. Clicking the image will bring you to the html version of the slides in a new tab. Note while in going through the slides, you can type h to see a special list of viewing options, and type o for an outline view of all the slides.\nThe lower button will allow you to download a PDF version of the slides. I suggest printing the slides beforehand and using them to take additional notes in class (not everything is in the slides)!\n \n Download as PDF\n  Assignments Preliminary Statistics Survey Due Sunday August 29 Please take the preliminary survey on your statistics and software background by 11:59 PM Sunday. This will help us all have a productive semester together.\n  ","date":1591574400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1629980070,"objectID":"95253bd6e56863ee98b11b433bbffb93","permalink":"https://metricsf21.classes.ryansafner.com/content/1.1-content/","publishdate":"2020-06-08T00:00:00Z","relpermalink":"/content/1.1-content/","section":"content","summary":"Overview  Readings  Slides  Assignments  Preliminary Statistics Survey Due Sunday August 29     Tuesday, August 24, 2021    Please complete the preliminary survey by Sunday August 29.    Overview Welcome to ECON 480 — Econometrics! Today’s lesson will be an overview of the content and the assignments of the course. Please read and familiarize yourself with the syllabus.","tags":null,"title":"1.1 — Introduction to Econometrics — Class Content","type":"docs"},{"authors":null,"categories":null,"content":"    Overview  Readings  Slides  R Practice  Assignments  Preliminary Statistics Survey Due Sunday August 29  Additional Useful Information  Installing R and R Studio R Packages  Getting Help for R  Other Useful Commands to Know Suggested Style Guide for Coding     Thursday, August 26, 2021    Please complete the preliminary survey by Sunday August 29.    Overview Today we begin the long slog to your mastery of R. We begin with the basics - how R works, how to use it, the different data types, and how to create and manipulate objects.\n  Readings  Ch.1 in Wickham \u0026amp; Grolemund, http://r4ds.had.co.nz/introduction.html  Now that we start working with R, you should consider this book to be your primary reference for R-related questions. We will broadly cover the first few chapters in order over the next 2-3 class periods.\n  Slides Below, you can find the slides in two formats. Clicking the image will bring you to the html version of the slides in a new tab. Note while in going through the slides, you can type h to see a special list of viewing options, and type o for an outline view of all the slides.\nThe lower button will allow you to download a PDF version of the slides. I suggest printing the slides beforehand and using them to take additional notes in class (not everything is in the slides)!\n \n Download as PDF\n  R Practice Today we will be working on practice problems. Answers will be posted later on that page.\n  Assignments Preliminary Statistics Survey Due Sunday August 29 Please take the preliminary survey on your statistics and software background by 11:59 PM Sunday. This will help us all have a productive semester together.\n  Additional Useful Information Installing R and R Studio We will be using Rstudio.cloud, for which you have made a free account. However, since R is free, I strongly recommend you download and install it on your own computers. You may decide also you prefer to use your own computers in class when we work with R.\nInstall R from CRAN1 by clicking “Download R” (or the CRAN link under Downloads on the left menu). This will take you to a mirrors page, where you can select a location in the U.S. and download a copy of R Install R Studio (Desktop Version), choose the “Free” option  (This will also be posted on the Reference page.)\n R Packages Packages come from multiple sources.\nThe polished, publicly released versions are found on CRAN. When installing a package available on CRAN, it is sufficient simply to tell R the following:2\ninstall.packages(\u0026quot;packagename\u0026quot;)  Other packages, which may be in various developmental states (including perfectly functional!) are often hosted on GitHub before they make their way onto CRAN. Simply telling R install.packages(\"packagename\") will fail to find it (as R only looks in CRAN for packages), so you must use another package called devtools3 to install packages directly from Github:4\ndevtools::install_github(\u0026quot;username/packagename\u0026quot;)  For example, to install Hadley Wickham’s package r4ds from its Github page https://github.com/hadley/r4ds, we would type:\ndevtools::install_github(\u0026quot;hadley/r4ds\u0026quot;) To use a package, you need to ensure it is loaded to your workspace (you only need to do this once)^[When we learn how to write R Markdown documents, . with library(\"package_name\").5\n  Getting Help for R For specific functions or commands, you can simply type:\n?functionname() # example ?mean() This will display a help page specific to that function in the Viewer pane. R functions and packages are extremely well-documented; help pages normally6 a short description of the function, arguments and options (as well as their default values), and several examples or vignettes to demonstrate usage of the function.\nAdditionally, you can turn to the community by searching on Google or better yet, StackExchange.\nOther Useful Commands to Know One of the best/worst things about R is that it is a language, meaning there are multiple ways that you can accomplish the same task. Here are a few alternative methods relevant to what we have learned so far that might prove useful.\nCreating Vectors We know vectors can be created with the c() command (and stored with = or \u0026lt;-), but there are other shortcuts to combine objects into a vector, particularly numeric data:\n: creates a series of integers  1:5 # create a vector of 1 through 5 ## [1] 1 2 3 4 5 12:17 # create a vector of 12 through 17 ## [1] 12 13 14 15 16 17  seq(from = , to = , by = ) creates a numeric sequence, and is not restricted to integers  seq(from = 1, to = 10, by = 2) # sequence from 1 to 10, by 2s ## [1] 1 3 5 7 9 # note you do not need to fully write out the name of each argument, just the input! seq(32.5,40,1.5) # sequence from 32.5 to 40, by 1.5 ## [1] 32.5 34.0 35.5 37.0 38.5 40.0 rep(., times =)7 repeats an element a specified number of times  rep(2, times = 4) # repeat \u0026quot;2\u0026quot; four times ## [1] 2 2 2 2 rep(2, 4) # does the same thing ## [1] 2 2 2 2 # the thing repeated could itself be a vector rep(c(1,4,7), 3) # repeat the vector \u0026quot;1, 4, 7\u0026quot; three times ## [1] 1 4 7 1 4 7 1 4 7 We can combine these:  # combine (the sequence of 4 to 8 by 2\u0026#39;s repeated three times) and 1 and 5 c(rep(seq(4,8,2),3),1,5) ## [1] 4 6 8 4 6 8 4 6 8 1 5   Suggested Style Guide for Coding We want to maximize human-readability of code, not just machine-readability. I try to follow Hadley Wickham’s style guide for all of my code, including that in this class.\nYou will not be graded on the style of your code. But now’s the best time to learn best practices (while you don’t know any alternatives!) to save yourself and your potential colleagues (including your future self) from unnecessary frustration.\n comment above for overall idea comment on side for individual elements of long commands name with _ use %\u0026gt;% wherever possible spaces betweeen all operators: \u0026lt;-, =, +, etc.  Exception: : and ::  line breaks between multiple arguments to a function  p\u0026lt;-ggplot(data=data, aes(x=x,y=y,fill=fill))+geom_point() becomes\np \u0026lt;- ggplot(data = data, aes(x = x, y = y, fill = fill))+ geom_point()    The Comprehensive R Archive Network↩︎\n Note the plural s on packages, and the quotes around the “package name”↩︎\n Which you will need to install first if you (probably) don’t already have it!↩︎\n Note the :: allows you to use the function install_github() from the devtools package without having to first load the devtools package with library(devtools).↩︎\n Quotes are not necessary this time.↩︎\n This useful guide comes from Kieran Healy’s excellent (free online!) book on Data Visualization.↩︎\n The . is a placeholder here.↩︎\n   ","date":1591574400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1631197923,"objectID":"eab062b2028198d6033d10ca4130c899","permalink":"https://metricsf21.classes.ryansafner.com/content/1.2-content/","publishdate":"2020-06-08T00:00:00Z","relpermalink":"/content/1.2-content/","section":"content","summary":"Overview  Readings  Slides  R Practice  Assignments  Preliminary Statistics Survey Due Sunday August 29  Additional Useful Information  Installing R and R Studio R Packages  Getting Help for R  Other Useful Commands to Know Suggested Style Guide for Coding     Thursday, August 26, 2021    Please complete the preliminary survey by Sunday August 29.    Overview Today we begin the long slog to your mastery of R.","tags":null,"title":"1.2 — Meet R — Class Content","type":"docs"},{"authors":null,"categories":null,"content":"   Download PDF Answers  Answer Key (html)  Answer Key (PDF)  Answer Key (R Script)  Getting Set Up Before we begin, start a new file with File \\(\\rightarrow\\) New File \\(\\rightarrow\\) R Script. As you work through this sheet in the console in R, also add (copy/paste) your commands that work into this new file. At the end, save it, and run to execute all of your commands at once.\n Creating Objects Question 1 Work on the following parts:\n a. Create a vector called me with two objects, your first name, and your last name. Call the vector to inspect it.  Confirm it is a character class vector.    Question 2 Use R’s help functions to determine what the paste() function does. Then paste together your first name and last name.\n Question 3 Create a vector called my_vector with all the even integers from 2 to 10.\n Question 4 Find the mean of my_vector with mean().\n Question 5 Take all the integers from 18 to 763,1 then get the mean.\n  Playing with Data For the following questions, we will use the diamonds dataset, included as part of ggplot2.\nQuestion 6 Install ggplot2.\n Question 7 Load ggplot2 with the library() command.\n Question 8 Get the structure of the diamonds data frame. What are the different variables and what kind of data does each contain?\n Question 9 Get summary statistics separately for carat, depth, table, and price.\n Question 10 color, cut, and clarity are categorical variables (factors). Use the table() command to generate frequency tables for each.\n Question 11 Now rerun the summary() command on the entire data frame.\n Question 12 Now look only at (subset) the first 4 diamonds in the dataset.\n Question 13 Now look only at (subset) the third and seventh diamond in the dataset.\n Question 14 Now look only at (subset) the second column of the dataset.\n Question 15 Do this again, but look using the $ to pull up the second column by name.\n Question 16 Now look only at diamonds that have a carat greater than or equal to 1.\n Question 17 Now look only at diamonds that have a VVS1 clarity.\n Question 18 Now look only at dimaonds that have a color of E, F, I, and J.\n Question 19 Now look only at diamonds that have a carat greater than or equal to 1 and a VVS1 clarity.\n Question 20 Get the average price of diamonds in question 18.2\n Question 21 What is the highest price for a diamond with a 1.0 carat, D color, and VVS1 clarity?\n  Execute your R Script Save the R Script you created at the beginning and (hopefully) have been pasting all of your valid commands to. This creates a .R file wherever you choose to save it to. Now looking at the file in the upper left pane of R Studio look for the button in the upper right corner that says Run. Sit back and watch R redo everything you’ve carefully worked on, all at once.\n  Hint: use the : operator to create a sequence from a starting number to an ending number↩︎\n Hints: use your subset command as an argument to the mean function. You will not need a comma here because you are looking for a single row.↩︎\n   ","date":1591574400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1630449216,"objectID":"c6c92f78e62b4b158ada53d1ff056d07","permalink":"https://metricsf21.classes.ryansafner.com/r/1.2-r/","publishdate":"2020-06-08T00:00:00Z","relpermalink":"/r/1.2-r/","section":"r","summary":"Download PDF Answers  Answer Key (html)  Answer Key (PDF)  Answer Key (R Script)  Getting Set Up Before we begin, start a new file with File \\(\\rightarrow\\) New File \\(\\rightarrow\\) R Script. As you work through this sheet in the console in R, also add (copy/paste) your commands that work into this new file. At the end, save it, and run to execute all of your commands at once.","tags":null,"title":"1.2 — Meet R — Practice","type":"docs"},{"authors":null,"categories":null,"content":"    Overview  Readings  On ggplot2 On data visualization   Slides  R Practice  Assignments  Preliminary Statistics Survey  Additional Useful Information  New Packages Mentioned ggplot2 Extensions     Tuesday, August 30, 2021    Overview Today we start the fun stuff - data visualization. We will cover how to build plots with the package ggplot2 (part of the tidyverse): I will lecture for the first half (again, it will be a lot of information that you can refer back to as needed over the semester) and you will practice making plots.\n  Readings   Ch.3 in Wickham \u0026amp; Grolemund, R for Data Science  On ggplot2  R Studio’s ggplot2 Cheat Sheet ggplot2’s website reference section Hadley Wickham’sR for Data Science book chapter on ggplot2 STHDA’s be awesome in ggplot2 r-statistic’s top 50 ggplot2 visualizations   On data visualization  Kieran Healy’s Data Visualization: A Practical Guide Claus Wilke’s Fundamentals of Data Visualization PolicyViz Better Presentations Karl Broman’s How to Display Data Badly I Want Hue     Slides Below, you can find the slides in two formats. Clicking the image will bring you to the html version of the slides in a new tab. Note while in going through the slides, you can type h to see a special list of viewing options, and type o for an outline view of all the slides.\nThe lower button will allow you to download a PDF version of the slides. I suggest printing the slides beforehand and using them to take additional notes in class (not everything is in the slides)!\n \n Download as PDF\n  R Practice Answers from last class’ practice problems on base R are posted on that page. Today you will be working on R practice problems on data visualization. Answers will be posted later on that page.\n  Assignments Preliminary Statistics Survey “Answers” for the statistics survey have been posted, including some summaries of how your classmates feel.\n  Additional Useful Information New Packages Mentioned Learn more about each of these in context in the slides, but below are links to each packages’s online help pages, which include descriptions of how each function works, examples, and cheatsheets to help you\n ggplot2 a grammar of graphics for plotting  You can find a running list of packages we use in this course in the resources pages.\n ggplot2 Extensions I have put up an additional resources page on packages that extend ggplot2 in various ways, as well as some ways to diagnose and improve several data visualization challenges.\n  ","date":1591574400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1631197923,"objectID":"b4cdd589314e12794f9762b9c170c079","permalink":"https://metricsf21.classes.ryansafner.com/content/1.3-content/","publishdate":"2020-06-08T00:00:00Z","relpermalink":"/content/1.3-content/","section":"content","summary":"Overview  Readings  On ggplot2 On data visualization   Slides  R Practice  Assignments  Preliminary Statistics Survey  Additional Useful Information  New Packages Mentioned ggplot2 Extensions     Tuesday, August 30, 2021    Overview Today we start the fun stuff - data visualization. We will cover how to build plots with the package ggplot2 (part of the tidyverse): I will lecture for the first half (again, it will be a lot of information that you can refer back to as needed over the semester) and you will practice making plots.","tags":null,"title":"1.3 — Data Visualization with ggplot2 — Class Content","type":"docs"},{"authors":null,"categories":null,"content":"   Download PDF Answers  Answer Key (html)  Answer Key (PDF)  Answer Key (R Script)  Getting Set Up Before we begin, start a new file with File \\(\\rightarrow\\) New File \\(\\rightarrow\\) R Script. As you work through this sheet in the console in R, also add (copy/paste) your commands that work into this new file. At the end, save it, and run to execute all of your commands at once.\n“Our Plot” from Class # load ggplot2 package library(ggplot2) # make plot ggplot(data = mpg)+ # set data source to mpg (included in ggplot2) aes(x = displ, # x is displacement y = hwy)+ # y is hwy mpg geom_point(aes(color = class))+ # color points by car class geom_smooth()+ # add regression line facet_wrap(~year)+ # separate plots by year labs(x = \u0026quot;Engine Displacement (Liters)\u0026quot;, y = \u0026quot;Highway MPG\u0026quot;, title = \u0026quot;Car Mileage and Displacement\u0026quot;, subtitle = \u0026quot;More Displacement Lowers Highway MPG\u0026quot;, caption = \u0026quot;Source: EPA\u0026quot;, color = \u0026quot;Vehicle Class\u0026quot;)+ scale_color_viridis_d()+ # change color scale theme_minimal()+ # change theme theme(text = element_text(family = \u0026quot;Fira Sans\u0026quot;)) # change font ## `geom_smooth()` using method = \u0026#39;loess\u0026#39; and formula \u0026#39;y ~ x\u0026#39;   Download R Script  Download and run in R Studio on your computer (or open the file in our R Studio cloud project and run it there) to see our plot from class.\n  Exploring the Data Question 1 We will look at GDP per Capita and Life Expectancy using some data from the gapminder project. There is a handy package called gapminder that uses a small snippet of this data for exploratory analysis. Install and load the package gapminder. Type ?gapminder and hit enter to see a description of the data.\n Question 2 Let’s get a quick look at gapminder to see what we’re dealing with.\nGet the structure of the gapminder data. What variables are there? Look at the head of the dataset to get an idea of what the data looks like. Get summary statistics of all variables.    Simple Plots in Base R Question 3 Let’s make sure you can do some basic plots before we get into the gg. Use base R’s hist() function to plot a histogram of gdpPercap.\n Question 4 Use base R’s boxplot() function to plot a boxplot of gdpPercap.\n Question 5 Now make it a boxplot by continent. Hint: use formula notation with ~.\n Question 6 Now make a scatterplot of gdpPercap on the \\(x\\)-axis and LifeExp on the \\(y\\)-axis.\n  Plots with ggplot2 Question 7 Load the package ggplot2 (you should have installed it previously. If not, install first with install.packages(\"ggplot2\")).\n Question 8 Let’s first make a bar graph to see how many countries are in each continent. The only aesthetic you need is to map continent to x. Bar graphs are great for representing categories, but not quantitative data.\n Question 9 For quantitative data, we want a histogram to visualize the distribution of a variable. Make a histogram of gdpPercap. Your only aesthetic here is to map gdpPercap to x.\n Question 10 Now let’s try adding some color, specifically, add an aesthetic that maps continent to fill. (In general, color refers to the outside borders of a geom (except points), fill is the interior of an object.)\n Question 11 Instead of a histogram, change the geom to make it a density graph. To avoid overplotting, add alpha=0.4 to the geom argument (alpha changes the transparency of a fill).\n Question 12 Redo your plot from 11 for lifeExp instead of gdpPercap.\n Question 13 Now let’s try a scatterplot for lifeExp (as y) on gdpPercap (as x). You’ll need both for aesthetics. The geom here is geom_point().\n Question 14 Add some color by mapping continent to color in your aesthetics.\n Question 15 Now let’s try adding a regression line with geom_smooth(). Add this layer on top of your geom_point() layer.\n Question 16 Did you notice that you got multiple regression lines (colored by continent)? That’s because we set a global aesthetic of mapping continent to color. If we want just one regression line, we need to instead move the color = continent inside the aes of geom_point. This will only map continent to color for points, not for anything else.\n Question 17 Now add an aesthetic to your points to map pop to size.\n Question 18 Change the color of the regression line to \"black\". Try first by putting this inside an aes() in your geom_smooth, and try a second time by just putting it inside geom_smooth without an aes(). What’s the difference, and why?\n Question 19 Another way to separate out continents is with faceting. Add +facet_wrap(~continent) to create subplots by continent.\n Question 20 Remove the facet layer. The scale is quite annoying for the x-axis, a lot of points are clustered on the lower level. Let’s try changing the scale by adding a layer: +scale_x_log10().\n Question 21 Now let’s fix the labels by adding +labs(). Inside labs, make proper axes titles for x, y, and a title to the plot. If you want to change the name of the legends (continent color), add one for color and size.\n Question 22 Now let’s try subsetting by looking only at North America. Take the gapminder dataframe and subset it to only look at continent==\"Americas\"). Assign this to a new dataframe object (call it something like america.) Now, use this as your data, and redo the graph from question 17. (You might want to take a look at your new dataframe to make sure it worked first!)\n Question 23 Try this again for the whole world, but just for observations in the year 2002.\n  ","date":1591574400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1630621373,"objectID":"52cfe5fe59880af539a400a51f8dfd0a","permalink":"https://metricsf21.classes.ryansafner.com/r/1.3-r/","publishdate":"2020-06-08T00:00:00Z","relpermalink":"/r/1.3-r/","section":"r","summary":"Download PDF Answers  Answer Key (html)  Answer Key (PDF)  Answer Key (R Script)  Getting Set Up Before we begin, start a new file with File \\(\\rightarrow\\) New File \\(\\rightarrow\\) R Script. As you work through this sheet in the console in R, also add (copy/paste) your commands that work into this new file. At the end, save it, and run to execute all of your commands at once.","tags":null,"title":"1.3 — Data Visualization with ggplot2 — Practice","type":"docs"},{"authors":null,"categories":null,"content":"    Overview  Readings  References by Package/Task   Slides  R Practice Additional Useful Information  New Packages Mentioned     Thursday, September 2, 2021    Overview Today we will cover the heart of tidyverse and use it for “data wrangling”. Today will again be a lot of content thrown at you, so you can look back at this as a reference all semester. Then we will do more practice problems.\n  Readings   Chs. 5, 10-12, 18 in Wickham \u0026amp; Grolemund, R for Data Science  References by Package/Task  tibble  R For Data Science, Chapter 10: Tibbles  readr and importing data  R For Data Science, Chapter 11: Data Import R Studio Cheatsheet: Data Import  dplyr and data wrangling  R For Data Science, Chapter 5: Data Transformation R Studio Cheatsheet: Data Wrangling (New version)  tidyr and tidying or reshaping data  R For Data Science, Chapter 12: Tidy Data R Studio Cheatsheet: Data Wrangling R Studio Cheatsheet: Data Import  joining data  R For Data Science, Chapter 13: Relational Data R Studio Cheatsheet: Data Transformation      Slides Below, you can find the slides in two formats. Clicking the image will bring you to the html version of the slides in a new tab. Note while in going through the slides, you can type h to see a special list of viewing options, and type o for an outline view of all the slides.\nThe lower button will allow you to download a PDF version of the slides. I suggest printing the slides beforehand and using them to take additional notes in class (not everything is in the slides)!\n \n Download as PDF\n  R Practice Answers from last class’ practice problems on base R are posted on that page. Today you will be working on R practice problems on data visualization. Answers will be posted later on that page.\n Additional Useful Information New Packages Mentioned Learn more about each of these in context in the slides, but below are links to each packages’s online help pages, which include descriptions of how each function works, examples, and cheatsheets to help you\n tidyverse collection of packages, including the following: tibble for friendlier, tidier dataframes magrittr for use of the pipe (%\u0026gt;%) readr, readxl, and haven for importing data dplyr for data wrangling and manipulation tidyr for reshaping data  You can find a running list of packages we use in this course in the resources pages.\n  ","date":1591574400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1631197923,"objectID":"4540fc3f8475c42ad4480a1f06fa9590","permalink":"https://metricsf21.classes.ryansafner.com/content/1.4-content/","publishdate":"2020-06-08T00:00:00Z","relpermalink":"/content/1.4-content/","section":"content","summary":"Overview  Readings  References by Package/Task   Slides  R Practice Additional Useful Information  New Packages Mentioned     Thursday, September 2, 2021    Overview Today we will cover the heart of tidyverse and use it for “data wrangling”. Today will again be a lot of content thrown at you, so you can look back at this as a reference all semester.","tags":null,"title":"1.4 — Data Wrangling in the tidyverse — Class Content","type":"docs"},{"authors":null,"categories":null,"content":"  Answers  Answers (html)  Answers (Rmd)  Answers (R Script)  Getting Set Up Before we begin, start a new file with File \\(\\rightarrow\\) New File \\(\\rightarrow\\) R Script. As you work through this sheet in the console in R, also add (copy/paste) your commands that work into this new file. At the end, save it, and run to execute all of your commands at once.\nFirst things first, load tidyverse.\nlibrary(\u0026quot;tidyverse\u0026quot;)  Warm Up to dplyr With gapminder Question 1 Load gapminder. First, select() the variables year, lifeExp, country.\n Question 2 select() all variables except pop.\n Question 3 rename() continent to cont.\n Question 4 arrange() by year.\n Question 5 arrange() by year, but in descending order.\n Question 6 arrange() by year, then by life expectancy.\n Question 7 filter() observations with pop greater than 1 billion.\n Question 8 Of those, look only at India.\n Question 9 Try out the pipe (%\u0026gt;%) if you haven’t already, by chaining commands: select() your data to look only at year, gdpPercap, and country in the year 1997, for countries that have a gdpPercap greater than 20,000, and arrange() them alphabetically.\n Question 10 mutate() a new variable called GDP that is equal to gdpPercap * pop.\n Question 11 mutate() a new population variable that is the pop in millions.\n Question 12 summarize() to get the average GDP per capita.\n Question 13 Get the number of observations, average, minimum, maximum, and standard deviation for GDP per capita.\n Question 14 Get the average GDP per capita over time. Hint, first group_by() year.\n Question 15 Get the average GDP per capita by continent.\n Question 16 Get the average GDP per capita by year and by continent. [Hint: do year first, if you do continent first, there are no years to group by!] Then save this as another tibble called gdp. Create a ggplot of a line graph of average continent GDP over time using the gdp data.\n Question 17 Try it again all in one command with the pipe %\u0026gt;%. Instead of saving the data as gdp, pipe it right into ggplot! [Hint: You can use . as a placeholder.]\n  Example: the Economics of College Majors Now let’s step it up to work with some data “in the wild” to answer some research questions. This will have you combine your dplyr skills and add some new things such as importing with readr.\nLet’s look at fivethirtyeight’s article \" The Economic Guide To Picking A College Major \". fivethirtyeight is great about making the data behind their articles public, we can download all of their data here. Search for college majors and click download (the blue arrow button). [This will download a .zip file that contains many spreadsheets. Unzip it with a program that unzips files (such as WinZip, 7-zip, the Unarchiver, etc).] We will look at the recent-grads.csv file.\nThe description in the readme file for the data is as follows:\n    Variable Description    Rank Rank by median earnings  Major_code Major code, FO1DP in ACS PUMS  Major Major description  Major_category Category of major from Carnevale et al  Total Total number of people with major  Sample_size Sample size (unweighted) of full-time, year-round ONLY (used for earnings)  Men Male graduates  Women Female graduates  ShareWomen Women as share of total  Employed Number employed (ESR == 1 or 2)  Full_time Employed 35 hours or more  Part_time Employed less than 35 hours  Full_time_year_round Employed at least 50 weeks (WKW == 1) and at least 35 hours (WKHP \u0026gt;= 35)  Unemployed Number unemployed (ESR == 3)  Unemployment_rate Unemployed / (Unemployed + Employed)  Median Median earnings of full-time, year-round workers  P25th 25th percentile of earnigns  P75th 75th percentile of earnings  College_jobs Number with job requiring a college degree  Non_college_jobs Number with job not requiring a college degree  Low_wage_jobs Number in low-wage service jobs    Question 18 Import the data with read_csv() and assign it to an object called majors. [One way to avoid error messages is to move (on your computer) recent_grads.csv to the same folder as R’s working directory, which again you can check with getwd().] The first argument of this command is the name of the original file, in quotes. [If the file is in a different folder, the argument is the full path in quotes.]\n Question 19 Look at the data with glimpse(). This is a suped-up version of str() in tidyverse.\n Question 20 What are all of the unique values of Major? How many are there?\n Question 21 Which major has the lowest unemployment rate?\n Question 22 What are the top 3 majors that have the highest percentage of women?\n Question 23 Make a boxplot of Median wage by Major_Category. [You won’t be able to read the labels easily, so add theme(axis.text.x=element_text(angle=45, hjust=1) to angle x-axis labels (and move them down by 1)]\n Question 24 Which major category is the least popular in this sample? [Hint: use group_by first.]\n Question 25 Is there a systematic difference in median earnings between STEM majors and non-STEM majors? First define:\nstem_categories \u0026lt;- c(\u0026quot;Biology \u0026amp; Life Science\u0026quot;, \u0026quot;Computers \u0026amp; Mathematics\u0026quot;, \u0026quot;Engineering\u0026quot;, \u0026quot;Physical Sciences\u0026quot;) Next, make a variable called stem, for whether or not a Major_category is \"stem\" or \"not_stem\". Then summarize() median for stem and not stem groups.\n[Hint: try out the ifelse() function which has three inputs: condition(s) for a variable(s), what to do if TRUE (the if), and what to if FALSE (the else), i.e.\nstem = ifelse(my_conditions, yes = do_this_if_TRUE, no = do_this_if_FALSE) You’ll of course need to change the do_this into something! ]\n  ","date":1591574400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1631063004,"objectID":"b9d47be8396e06fb2d2fe804f29c90c4","permalink":"https://metricsf21.classes.ryansafner.com/r/1.4-r/","publishdate":"2020-06-08T00:00:00Z","relpermalink":"/r/1.4-r/","section":"r","summary":"Answers  Answers (html)  Answers (Rmd)  Answers (R Script)  Getting Set Up Before we begin, start a new file with File \\(\\rightarrow\\) New File \\(\\rightarrow\\) R Script. As you work through this sheet in the console in R, also add (copy/paste) your commands that work into this new file. At the end, save it, and run to execute all of your commands at once.\nFirst things first, load tidyverse.","tags":null,"title":"1.4 — Data Wrangling in the tidyverse — Practice","type":"docs"},{"authors":null,"categories":null,"content":"    Overview  Readings  On Plain-Text Workflow On Markdown Syntax On R Markdown On Git and Github   Slides  R Practice  Assignments  Problem Set 1     Tuesday, September 7, 2021    Problem Set 1 is due by DATE.    Overview Today I will give you about half the class period to finish your 1.4 practice problems and I can answer questions. Then I will briefly show you how you the magic of how you can improve your workflow, efficiency, automation, reproducibility, and safe file backups with R Markdown, R Projects, git and github. All of these tools are slightly advanced, and optional for you to use, but the learning curve pays off very high returns for the rest of your life! A good handful of people every semester tell me that they keep using these tools years in the future.\n  Readings   Chs. 8, 27, 28, 29, 30 (but especially 27) in Wickham \u0026amp; Grolemund, R for Data Science  On Plain-Text Workflow  Healey, Kieran, The Plain Person’s Guide to Plain Text Social Science   On Markdown Syntax  Markdown tutorial   On R Markdown  Xie, Yihui, J.J. Allaire, and Garett Grolemund, R Markdown: The Definitive Guide RStudio, R Markdown Reference Guide RStudio, Overview of R Markdown Rstudio, *R Markdown Cheatsheet   On Git and Github  Bryan, Jenny and Jim Hester, Happy Git and GitHub for the useR     Slides Below, you can find the slides in two formats. Clicking the image will bring you to the html version of the slides in a new tab. Note while in going through the slides, you can type h to see a special list of viewing options, and type o for an outline view of all the slides.\nThe lower button will allow you to download a PDF version of the slides. I suggest printing the slides beforehand and using them to take additional notes in class (not everything is in the slides)!\n \n Download as PDF\n  R Practice Answers from last class’ practice problems on base R are posted on that page. Today’s “practice problems” get you to practice the tools we are working with today. They are again not required, but will help you if you are interested.\n  Assignments Problem Set 1 Problem Set 1 is posted and is due by class Tuesday September 14. Please see the instructions for more information on how to submit your assignment (there are multiple ways!).\n  ","date":1591574400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1631197923,"objectID":"961bc217afee9b48b132347b3bb550e2","permalink":"https://metricsf21.classes.ryansafner.com/content/1.5-content/","publishdate":"2020-06-08T00:00:00Z","relpermalink":"/content/1.5-content/","section":"content","summary":"Overview  Readings  On Plain-Text Workflow On Markdown Syntax On R Markdown On Git and Github   Slides  R Practice  Assignments  Problem Set 1     Tuesday, September 7, 2021    Problem Set 1 is due by DATE.    Overview Today I will give you about half the class period to finish your 1.4 practice problems and I can answer questions.","tags":null,"title":"1.5 — Optimize Your Workflow with Projects, Markdown, and Git — Class Content","type":"docs"},{"authors":null,"categories":null,"content":"  Part 1 We are going to clone my repository on Github into R Studio on your computer. Go to my workflow repository on github. Click the green code button. See there are several options. Feel free to try each of them out:\n On Your Computer (Download): Download ZIP will download a zipped file to your computer containing the entire repository. You will need to unzip this (sometimes it’s automatic, as on a Mac), and then it will create a folder on your computer. Open up the workflow.Rproj file, which will open the project in R Studio. On R Studio Cloud: Highlight the url code (or click the copy button on the right). On Rstudio Cloud, click on the arrow on the right of the New Project button and select New Project from GitHub Repository. Paste the url you copied from Github here. This will open the repository as a new project in your cloud workspace On Your Computer (via Git) ADVANCED: If you have git installed (see the ultimate guide for help doing this), copy the url code on GitHub. Next, go to R Studio on your computer, click File -\u0026gt; New Project -\u0026gt; Version Control -\u0026gt; Git and type paste the url in the first field. The final field is where on your computer you want to save this folder. It will then open up the project in R Studio.   Part 2 Now that you have the workflow project on your computer (or cloud), let’s explore it. Notice in the file viewer pane in the bottom right, there are several folders and files there. Look at the top of the pane is the file path (this is where you can find this folder on your computer). Mine looks like this, for example:\nThe nice thing with projects is that any files you open or save are in this folder on your computer!\n Part 3 Open up Example_paper.Rmd. This is an example of a file I would use to write a paper. Note the parts of the .Rmd file: - The yaml at the top - The text written in markdown - R chunks scattered throughout\n Part 4 I have this set to knit to produce a PDF. For you to do this, you will need to install a distribution of LaTeX. Unless you intend to use LaTeX in the future (for math classes, going to graduate school, or you are a masochist…), we can get around this with a lightweight version we can install inside of R. Run the following code in the console:\ninstall.packages(\u0026quot;tinytex\u0026quot;) tinytex::install_tinytex() This will probably take a few minutes to install. Like any package, you only ever have to do this once! Once this is complete, now try to knit Example_paper.Rmd to PDF by clicking the Knit button at the top. View your PDF that pops up!1\n Part 5 Now let’s learn more about writing with markdown syntax. Complete this brief tutorial to practice!\n Part 6 Just practice working in the Example_paper.Rmd file. Create a new R chunk (anywhere) and write some R code to open data/clean_data.csv (which you can find by clicking through the folders too) and save it as an R object. (You may need to load tidyverse!) Run only this chunk by clicking the green play button at the upper right corner of the chunk. Make sure it works. Using projects to organize your files is much simpler than worrying about your working directory!\n Part 7 Find my slides on the class github page. Note there is a lot going on here, because this is a pretty full website. They are in the static/slides folder. Now if you are ever curious how I did something in my slides, you can see the source .Rmd files. Note my slides are written in Xaringan (a special, html-based slides package). Note the slides are quite advanced - I use a lot of html and css formatting to make them pretty!\n Part 8 Now we are just exposing you to more .Rmd files. Look at the answer key for 1.4 R practice and download and open the  markdown file. Take a look through it and see how it works, then when you are ready, click the knit button. It will make the html webpage (which you can open in any browser) that is identical to the answer key web page I made!\n Part 9 Look at the  markdown file in this week’s homework assignment. If you want, you can complete your homework using that file.\n  On R Studio cloud, it may request you to change your popup settings, since it will pop up the PDF in a new window.↩︎\n   ","date":1591574400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1631062268,"objectID":"82113c3cbec899b17ea6a48cec61a86a","permalink":"https://metricsf21.classes.ryansafner.com/r/1.5-r/","publishdate":"2020-06-08T00:00:00Z","relpermalink":"/r/1.5-r/","section":"r","summary":"Part 1 We are going to clone my repository on Github into R Studio on your computer. Go to my workflow repository on github. Click the green code button. See there are several options. Feel free to try each of them out:\n On Your Computer (Download): Download ZIP will download a zipped file to your computer containing the entire repository. You will need to unzip this (sometimes it’s automatic, as on a Mac), and then it will create a folder on your computer.","tags":null,"title":"1.5 — Optimize Your Workflow with Projects, Markdown, and Git — Practice","type":"docs"},{"authors":null,"categories":null,"content":"    Overview  Readings  R Practice  Assignments  Problem Set 1  Math Appendix  The Summation Operator Useful Properties of Summation Operators Advanced: Useful Properties for Regression     Thursday, September 9, 2021    Problem Set 1 is due by class Tuesday September 14 via Blackboard Assignments.    Overview Today we begin with a review and overview of using data and descriptive statistics. We want to quantify characteristics about samples as statistics, which we will later use to infer things about populations (between which we will later identify causal relationships).\nNext class will be on random variables and distributions. This full week is your crash course/review of basic statistics that we will need to start the “meat and potatoes” of this class: linear regression next Thursday As such, I’ll give you a brief homework next week to review these statistical concepts (with minimal use of R!).\n  Readings   Math and Probability Background Appendix A in Bailey  Now that we return to the statistics, we will do a minimal overview of basic statistics and distributions. Review Bailey’s appendices. Today, only A is really useful, the rest will come next class.\nChapter 2 is optional, but will give you a good overview of using data. ##  Slides\nBelow, you can find the slides in two formats. Clicking the image will bring you to the html version of the slides in a new tab. Note while in going through the slides, you can type h to see a special list of viewing options, and type o for an outline view of all the slides.\nThe lower button will allow you to download a PDF version of the slides. I suggest printing the slides beforehand and using them to take additional notes in class (not everything is in the slides)!\n \n Download as PDF\n  R Practice Answers from last class’ practice problems on base R are posted on that page. Today’s “practice problems” get you to practice the tools we are working with today. They are again not required, but will help you if you are interested.\n  Assignments Problem Set 1 Problem Set 1 is posted and is due by class Tuesday September 14. Please see the instructions for more information on how to submit your assignment (there are multiple ways!).\nProblem set 2 (on classes 2.1-2.2) will be posted shortly, and will be due by Tuesday September 21.\n  Math Appendix The Summation Operator Many elementary propositions in econometrics (and statistics) involve the use of the sums of numbers. Mathematicians often use the summation operator (the greek letter \\(\\Sigma\\) –“sigma”) as a shorthand, rather than writing everything out the long way. It will be worth your time to understand the summation operator, and some of its properties, and how these can provide shortcuts to proving more advanced theorems in econometrics.\nLet \\(X\\) be a random variable from which a sample of \\(n\\) observations is observed, so we have a sequence \\(\\{x_1, x_2,...,x_n\\}\\) i.e. $x_i, $ for \\(i=1,2,...,n\\). Then the total sum of the observations \\((x_1+x_2+...+x_n)\\) can be represented as:\n\\[\\sum_{i=1}^n x_i = x_1+x_2+...+x_n\\]\n The term beneath \\(\\Sigma\\) is known as the “index,” which tells us where to begin our adding (at the 1st individual \\(x\\) term, \\(x_1\\))  Note other letters, such as \\(j\\), or \\(k\\) may be used (especially if \\(i\\) is defined elsewhere)  The term above \\(\\Sigma\\) is the total number of \\(x\\) terms we should add \\((n)\\) Essentially, read \\(\\displaystyle \\sum_{i=1}^{n} x_i\\) as “add up all the individual \\(x\\) observations from the 1st \\((x_1)\\) to the final \\(n\\) \\((x_n)\\).”   Useful Properties of Summation Operators Rule 1: The summation of a constant \\(k\\) times a random variable \\(X_i\\) is equal to the constant times the summation of that random variable:\n\\[\\sum_{i=1}^n kX_i = k \\sum^n_{i=1} X_i\\]\nProof:\n\\[\\begin{align*} \\sum_{i=1}^n kX_i \u0026amp;= k x_1 + kx_2 +...+ kx_n\\\\ \u0026amp;= k(x_1+x_2+...x_n)\\\\ \u0026amp;= k\\sum_{i=1}^n X_i. \\\\ \\end{align*}\\]\nRule 2: The summation of a sum of two random variables is equal to the sum of their summations:\n\\[\\sum_{i=1}^n (X_i+Y_i) = \\sum_{i=1}^n X_i + \\sum_{i=1}^n Y_i\\]\nProof:\n\\[\\begin{align*} \\sum_{i=1}^n (X_i+Y_i) \u0026amp;=(X_1+Y_1) + (X_2+Y_2) + ... (X_n+Y_n)\\\\ \u0026amp;=(X_1+X_2+...+X_n) + (Y_1+Y_2+...+Y_n)\\\\ \u0026amp;=\\sum_{i=1}^n X_i + \\sum_{i=1}^n Y_i\\\\ \\end{align*}\\]\nRule 3: The summation of constant over \\(n\\) observations is the product of the constant and \\(n\\):\n\\[\\sum_{i=1}^n k = nk\\]\nProof:\n\\[\\sum_{i=1}^n k = \\underbrace{k + k + ... + k}_{n \\text{ times}} = nk\\]\nCombining these 3 rules: for the sum of a linear combination of a random variable (\\(a+bX\\)):\n\\[\\sum_{i=1}^n (a+bX_i) = na+b\\sum_{i=1}^n X_i\\]\nProof: left to you as an exercise!\n Advanced: Useful Properties for Regression There are some additional properties of summations that may not be immediately obvious, but will be quite essential in proving properties of linear regressions.\nUsing the properties above, we can describe the mean, variance, and covariance of random variables.1\nFirst, define the mean of a sequence \\(\\{X_i: i=1,...,n\\}\\) and \\(\\{Y_i: i=1,...,n\\}\\) as:\n\\[\\bar{X}=\\frac{1}{n}\\sum^n_{i=1}X_i\\]\nSecond, the variance of \\(X\\) is:\n\\[var(X)=\\frac{1}{n}\\sum^n_{i=1}(X_i-\\bar{X})^2\\]\nThird, the covariance of \\(X\\) and \\(Y\\) is:\n\\[cov(X,Y)=\\frac{1}{n}\\sum^n_{i=1}(X_i-\\bar{X})(Y_i-\\bar{Y})\\]\nRule 4: The sum of the deviations of observations of \\(X_i\\) from its mean (\\(\\bar{X}\\)) is 0:\n\\[\\sum^n_{i=1} (X_i-\\bar{X})=0\\]\nProof:\n\\[\\begin{align*} \\sum^n_{i=1} (X_i-\\bar{x}) \u0026amp;=\\sum^n_{i=1}X_i-\\sum^n_{i=1}\\bar{X} \u0026amp;\u0026amp; \\\\ \u0026amp;=\\sum^n_{i=1}X_i-n\\bar{X} \u0026amp;\u0026amp; \\text{Since $\\bar{x}$ is a constant}\\\\ \u0026amp;=n\\underbrace{\\frac{\\displaystyle\\sum^n_{i=1}X_i}{n}}_{\\bar{X}}-n\\bar{X} \u0026amp;\u0026amp; \\text{Multiply the first term by }\\frac{n}{n}=1\\\\ \u0026amp;=n\\bar{X}-n\\bar{X}\u0026amp;\u0026amp; \\text{By the definition of the mean }\\bar{X}\\\\ \u0026amp;=0 \u0026amp;\u0026amp; \\\\ \\end{align*}\\]\nRule 5: The squared deviations of \\(X\\) are equal to the product of \\(X\\) times its deviations:\n\\[\\sum^n_{i=1} (X_i-\\bar{X})^2=\\sum^n_{i=1} X_i(X_i-\\bar{X})\\]\nProof:\n\\[\\begin{align*} \\sum^n_{i=1} (X_i-\\bar{X})^2\u0026amp;=\\sum^n_{i=1} (X_i-\\bar{X})(X_i-\\bar{X}) \u0026amp;\u0026amp; \\text{Expanding the square}\\\\ \u0026amp;=\\sum^n_{i=1} X_i(X_i-\\bar{X})-\\sum^n_{i=1}\\bar{X}(X_i-\\bar{X}) \u0026amp;\u0026amp; \\text{Breaking apart the first term} \\\\ \u0026amp;=\\sum^n_{i=1} X_i(X_i-\\bar{X})-\\bar{X}\\sum^n_{i=1}(X_i-\\bar{X}) \u0026amp;\u0026amp; \\text{Since }\\bar{X} \\text{ is constant, not depending on } i\u0026#39;s\\\\ \u0026amp;=\\sum^n_{i=1} X_i(X_i-\\bar{X})-\\bar{X}(0) \u0026amp;\u0026amp; \\text{From rule 4} \\\\ \u0026amp;=\\sum^n_{i=1} X_i(X_i-\\bar{X}) \u0026amp;\u0026amp; \\text{Remainder after multiplying by 0}\\\\ \\end{align*}\\]\nRule 6: The following summations involving \\(X\\) and \\(Y\\) are equivalent:\n\\[\\sum^n_{i=1} Y_i(X_i-\\bar{X})=\\sum^n_{i=1}X_i(Y_i-\\bar{Y})=\\sum^n_{i=1} (X_i-\\bar{X})(Y_i-\\bar{Y})\\]\nProof:\n\\[\\begin{align*} \\sum^n_{i=1} (X_i-\\bar{X})(Y_i-\\bar{Y}) \u0026amp;=\\sum^n_{i=1} Y_i(X_i-\\bar{X})-\\sum^n_{i=1}\\bar{Y}(X_i-\\bar{X}) \u0026amp;\u0026amp; \\text{Breaking apart the second term} \\\\ \u0026amp;=\\sum^n_{i=1} Y_i(X_i-\\bar{X})-\\bar{Y}(0) \u0026amp;\u0026amp; \\text{From rule 4}\\\\ \u0026amp;=\\sum^n_{i=1} Y_i(X_i-\\bar{X}) \u0026amp;\u0026amp; \\text{Remainder after multiplying by 0}\\\\ \\end{align*}\\]\nequivalently:\n\\[\\begin{align*} \\sum^n_{i=1} (X_i-\\bar{X})(Y_i-\\bar{Y}) \u0026amp;=\\sum^n_{i=1} X_i(Y_i-\\bar{Y})-\\sum^n_{i=1}\\bar{X}(Y_i-\\bar{Y}) \u0026amp;\u0026amp; \\text{Breaking apart the first term} \\\\ \u0026amp;=\\sum^n_{i=1} X_i(Y_i-\\bar{Y})-\\bar{X}(0) \u0026amp;\u0026amp; \\text{From rule 4}\\\\ \u0026amp;=\\sum^n_{i=1} X_i(Y_i-\\bar{Y}) \u0026amp;\u0026amp; \\text{Remainder after multiplying by 0}\\\\ \\end{align*}\\]\n   For more beyond the mere definition, see the appendix on Covariance and Correlation↩︎\n   ","date":1591574400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1631592133,"objectID":"459760450d985b9fdf3f6e02cb511e1a","permalink":"https://metricsf21.classes.ryansafner.com/content/2.1-content/","publishdate":"2020-06-08T00:00:00Z","relpermalink":"/content/2.1-content/","section":"content","summary":"Overview  Readings  R Practice  Assignments  Problem Set 1  Math Appendix  The Summation Operator Useful Properties of Summation Operators Advanced: Useful Properties for Regression     Thursday, September 9, 2021    Problem Set 1 is due by class Tuesday September 14 via Blackboard Assignments.    Overview Today we begin with a review and overview of using data and descriptive statistics.","tags":null,"title":"2.1 — Data 101 \u0026 Descriptive Statistics — Class Content","type":"docs"},{"authors":null,"categories":null,"content":"    Overview  Readings  Slides  R Practice  Assignments  Problem Set 1  Math Appendix  Properties of Expected Value and Variance  R Appendix  Graphing Mathematical Functions Bult-in Statistical Functions Graphing Statistical Functions     Tuesday, September 14, 2021    Problem Set 1 is now due. Problem Set 2 is due by class Tuesday September 21.    Overview Today we finish your crash course/review of basic statistics with random variables and distributions.\n  Readings   Math and Probability Background Appendices B-I in Bailey, Real Econometrics  Now that we return to the statistics, we will do a minimal overview of basic statistics and distributions. Review all of Bailey’s appendices.\n  Slides Below, you can find the slides in two formats. Clicking the image will bring you to the html version of the slides in a new tab. Note while in going through the slides, you can type h to see a special list of viewing options, and type o for an outline view of all the slides.\nThe lower button will allow you to download a PDF version of the slides. I suggest printing the slides beforehand and using them to take additional notes in class (not everything is in the slides)!\n \n Download as PDF\n  R Practice Answers from last class’ practice problems on base R are posted on that page. Today’s “practice problems” get you to practice the tools we are working with today. They are again not required, but will help you if you are interested.\n  Assignments Problem Set 1 Problem Set 1 is posted and is due by class Tuesday September 14. Please see the instructions for more information on how to submit your assignment (there are multiple ways!).\nProblem set 2 (on classes 2.1-2.2) will be posted shortly, and will be due by Tuesday September 21.\n  Math Appendix Properties of Expected Value and Variance There are several useful mathematical properties of expected value and variance.\nProperty 1: the expected value of a constant is itself, and the variance of a constant is 0.\n\\[\\begin{align*} E(c)\u0026amp;=c\\\\ var(c)\u0026amp;=0\\\\ sd(c)\u0026amp;=0\\\\ \\end{align*}\\]\nFor any constant, \\(c\\)\n Example: \\(E(2)=2\\), \\(var(2)=0\\), \\(sd(2)=0\\)  Property 2: adding or subtracting a constant to a random variable and then taking the mean or variance:\n\\[\\begin{align*} E(X \\pm c)\u0026amp;=E(X) \\pm c\\\\ var(X \\pm c)\u0026amp;=X\\\\ sd(X \\pm c)\u0026amp;=X\\\\ \\end{align*}\\]\nFor any constant, \\(c\\)\n Example: \\(E(2+X)=2+E(X)\\), \\(var(2+X)=var(X)\\), \\(sd(2+X)=sd(X)\\)  Property 3: multiplying a constant to a random variable and then taking the mean or variance:\n\\[\\begin{align*} E(aX)\u0026amp;=E(X) aE(X)\\\\ var(aX)\u0026amp;=a^2var(X)\\\\ sd(aX)\u0026amp;=|a|sd(X)\\\\ \\end{align*}\\]\nFor any constant, \\(a\\)\n Example: \\(E(2X)=2E(X)\\), \\(var(2X)=4var(X)\\), \\(sd(2X)=2sd(X)\\)  Property 4: the expected value of the sum of two random variables is equal to the sum of each random variable’s expected value:\n\\[E(X \\pm Y)=E(X) \\pm E(Y)\\]\n  R Appendix Creating Mathematical Functions You can create custom mathematical functions using mosaic by defining an R function() with multiple arguments. As a simple example, make the function \\(f(x) = 10x-x^2\\) (with one argument, \\(x\\) since it is a univariate function) as follows:\n# store as a named function, I\u0026#39;ll call it \u0026quot;my_function\u0026quot; my_function\u0026lt;-function(x){10*x-x^2} # look at it my_function ## function(x){10*x-x^2} There are some notational requirements from R for making functions. Any coefficient in front of a variable (such as the 10 in 10x must be explicitly multiplied by the variable, as in 10*x).\nTo use the function to calculate its value at a particular value of x, simply define what the (x) is and run your named function on it:\n# f of 2 my_function(2) ## [1] 16 # f of 2 and 4 my_function(c(2,4)) ## [1] 16 24 # f of 2 through 7 my_function(2:7) ## [1] 16 21 24 25 24 21 # ALTERNATIVELY # define x first as a vector and then run function on it x\u0026lt;-c(2,4) my_function(x) ## [1] 16 24  Graphing Mathematical Functions In ggplot there is a dedicated stat_function() (equivalent to a geom_ layer) to graph mathematical and statistical functions. All that is needed is a data.frame of a range of x values to act as the source for data, and set x equal to those values for aesthetics.\nlibrary(tidyverse) ## ── Attaching packages ─────────────────────────────────────── tidyverse 1.3.1 ── ## ✓ ggplot2 3.3.5 ✓ purrr 0.3.4 ## ✓ tibble 3.1.4 ✓ dplyr 1.0.7 ## ✓ tidyr 1.1.3 ✓ stringr 1.4.0 ## ✓ readr 2.0.0 ✓ forcats 0.5.1 ## ── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ── ## x dplyr::filter() masks stats::filter() ## x dplyr::lag() masks stats::lag() # x values are integers 1 through 10 ggplot(data = data.frame(x = 1:10))+ aes(x = x) Then we add the stat_function, where fun = is the most important argument where you define the to function to graph as your function created above, for example, our my_function.\nggplot(data = data.frame(x = 1:10))+ aes(x = x)+ stat_function(fun = my_function)  You can also adjust things like size, color, and line type.\nggplot(data = data.frame(x = 1:10))+ aes(x = x)+ stat_function(fun = my_function, color = \u0026quot;blue\u0026quot;, size = 2, linetype = \u0026quot;dashed\u0026quot;)   Bult-in Statistical Functions There are some standard statistical distributions built into R. They require a combination of a specific prefix and a distribution.\nPrefixes:\n  Action/Type Prefix    random draw r  density (pdf) d  cumulative density (cdf) p  quantile (inverse cdf) q    Distributions:\n  Distribution Name in R    Normal norm  Uniform unif  Student’s t t  Binomial binom  Negative binomial nbinom  Hypergeometric hyper  Weibull weibull  Beta beta  Gamma gamma    Thus, what you want is a combination of the prefix and the distribution.\nSome common examples: Take random draws from a normal distribution:  rnorm(n = 10, # take 10 draws from a normal distribution with: mean = 2, # mean of 2 sd = 1) # sd of 1 ## [1] 0.9306446 1.9587986 1.7016603 1.2481484 1.3399839 1.9677189 0.3313595 ## [8] 1.6183993 2.5601664 0.5277235 Get probability of a random variable being less than or equal to a value (cdf) from a normal distribution:  # find probability of area to the LEFT of a number on pdf (note this = cdf of that number!) pnorm(q = 80, # number is 80 from a distribution where: mean = 200, # mean is 100 sd = 100, # sd is 100 lower.tail = TRUE) # looking to the LEFT in lower tail ## [1] 0.1150697 Find the value of a distribution that is a specified percentile.  # find the 38th percentile value qnorm(p = 0.38, # 38th percentile from a distribution where: mean = 200, # mean is 200 sd = 100) # sd is 100 ## [1] 169.4519   Graphing Statistical Functions You can also graph these commonly used statistical functions by setting fun = the named functions in your stat_function() layer. If you want to specify the mean and standard deviation, use args = list() to include the required arguments from the named function above (e.g. dnorm needs mean and sd).\nggplot(data = data.frame(x = -400:600))+ aes(x = x)+ stat_function(fun = dnorm, args = list(mean = 200, sd = 200), color = \u0026quot;blue\u0026quot;, size = 2, linetype = \u0026quot;dashed\u0026quot;)  If you don’t include this, it will graph the standard distribution:\nggplot(data = data.frame(x = -4:4))+ aes(x = x)+ stat_function(fun = dnorm, color = \u0026quot;blue\u0026quot;, size = 2, linetype = \u0026quot;dashed\u0026quot;)  To add shading under a distribution, simply add a duplicate of the stat_function() layer, but add geom=\"area\" to indicate the area beneath the function should be filled, and you can limit the domain of the fill with xlim=c(start,end), where start and end are the x-values for the endpoints of the fill.\n# graph normal distribution and shade area between -2 and 2 ggplot(data = data.frame(x = -4:4))+ aes(x = x)+ stat_function(fun = dnorm, color = \u0026quot;blue\u0026quot;, size = 2, linetype = \u0026quot;dashed\u0026quot;)+ stat_function(fun = dnorm, xlim = c(-2,2), geom = \u0026quot;area\u0026quot;, fill = \u0026quot;green\u0026quot;, alpha=0.5) Hence, here is one graph from my slides:\nggplot(data = tibble(x=35:115))+ aes(x = x)+ stat_function(fun = dnorm, args = list(mean = 75, sd = 10), size = 2, color=\u0026quot;blue\u0026quot;)+ stat_function(fun = dnorm, args = list(mean = 75, sd = 10), geom = \u0026quot;area\u0026quot;, xlim = c(65,85), fill=\u0026quot;blue\u0026quot;, alpha=0.5)+ labs(x = \u0026quot;X\u0026quot;, y = \u0026quot;Probability\u0026quot;)+ scale_x_continuous(breaks = seq(35,115,5))+ theme_classic(base_family = \u0026quot;Fira Sans Condensed\u0026quot;, base_size=20)   ","date":1591574400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1631765103,"objectID":"32461c9caa508bc78a6ca817b0aa151d","permalink":"https://metricsf21.classes.ryansafner.com/content/2.2-content/","publishdate":"2020-06-08T00:00:00Z","relpermalink":"/content/2.2-content/","section":"content","summary":"Overview  Readings  Slides  R Practice  Assignments  Problem Set 1  Math Appendix  Properties of Expected Value and Variance  R Appendix  Graphing Mathematical Functions Bult-in Statistical Functions Graphing Statistical Functions     Tuesday, September 14, 2021    Problem Set 1 is now due. Problem Set 2 is due by class Tuesday September 21.    Overview Today we finish your crash course/review of basic statistics with random variables and distributions.","tags":null,"title":"2.2 — Random Variables and Distributions — Class Content","type":"docs"},{"authors":null,"categories":null,"content":"    Overview  Readings  Slides  Assignments  Problem Set 1  Math Appendix  Variance Covariance Correlation     Thursday, September 16, 2021    Problem Set 1 answers are posted on that page. Problem Set 2 is due by class Tuesday September 21.    Overview Today we start looking at associations between variables, which we will first attempt to quantify with measures like covariance and correlation. Then we turn to fitting a line to data via linear regression. We overview the basic regression model, the parameters and how they are derived, and see how to work with regressions in R with lm and the tidyverse package broom.\nWe consider an extended example about class sizes and test scores, which comes from a (Stata) dataset from an old textbook that I used to use, Stock and Watson, 2007. Download and follow along with the data from today’s example:1\n  caschool.dta    Readings   Ch. 3.1, Math and Probability Background Appendices D-E in Bailey, Real Econometrics    Slides Below, you can find the slides in two formats. Clicking the image will bring you to the html version of the slides in a new tab. Note while in going through the slides, you can type h to see a special list of viewing options, and type o for an outline view of all the slides.\nThe lower button will allow you to download a PDF version of the slides. I suggest printing the slides beforehand and using them to take additional notes in class (not everything is in the slides)!\n \n Download as PDF\n  Assignments Problem Set 1 Problem Set 2 is due by Tuesday September 21. Please see the instructions for more information on how to submit your assignment (there are multiple ways!).\n  Math Appendix Variance Recall the variance of a discrete random variable \\(X\\), denoted \\(var(X)\\) or \\(\\sigma^2\\), is the expected value (probability-weighted average) of the squared deviations of \\(X_i\\) from it’s mean (or expected value) \\(\\bar{X}\\) or \\(E(X)\\).2\n\\[\\begin{align*} \\sigma_X^2\u0026amp;=E(X-E(X))\\\\ \u0026amp;=\\sum^n_{i=1} (X_i-\\bar{X})^2 p_i \\end{align*}\\]\nFpr continuous data (if all possible values of \\(X_i\\) are equally likely or we don’t know the probabilities), we can write variance as a simple average of squared deviations from the mean:\n\\[\\begin{align*} \\sigma_X^2\u0026amp;=\\frac{1}{n}\\sum^n_{i=1}(X_i-\\bar{X})^2 \\end{align*}\\]\nVariance has some useful properties:\nProperty 1: The variance of a constant is 0\n\\[var(c)=0 \\text{ iff } P(X=c)=1\\]\nIf a random variable takes the same value (e.g. 2) with probability 1.00, \\(E(2)=2\\), so the average squared deviation from the mean is 0, because there are never any values other than 2.\nProperty 2: The variance is unchanged for a random variable plus/minus a constant\n\\[var(X\\pm c)\\]\nSince the variance of a constant is 0.\nProperty 3: The variance of a scaled random variable is scaled by the square of the coefficient\n\\[var(aX)=a^2var(X)\\]\nProperty 4: The variance of a linear transformation of a random variable is scaled by the square of the coefficient\n\\[var(aX+b)=a^2var(X)\\]\n Covariance For two random variables, \\(X\\) and \\(Y\\), we can measure their covariance (denoted \\(cov(X,Y)\\) or \\(\\sigma_{X,Y}\\))3 to quantify how they vary together. A good way to think about this is: when \\(X\\) is above its mean, would we expect \\(Y\\) to also be above its mean (and covary positively), or below its mean (and covary negatively). Remember, this is describing the joint probability distribution for two random variables.\n\\[\\begin{align*} \\sigma_{X,Y}\u0026amp;=E\\big[(X-\\bar{X})(Y-\\bar{Y})\\big] \\end{align*}\\]\nAgain, in the case of equally probable values for both \\(X\\) and \\(Y\\), covariance is sometimes written:\n\\[\\begin{align*} \\sigma_{X,Y}\u0026amp;=\\frac{1}{N}\\sum_{i=1}^n(X-\\bar{X})(Y-\\bar{Y}) \\end{align*}\\]\nCovariance also has a number of useful properties:\nProperty 1: The covariance of a random variable \\(X\\) and a constant \\(c\\) is 0\n\\[cov(X,c)=0\\]\nProperty 2: The covariance of a random variable and itself is the variable’s variance\n\\[\\begin{align*} cov(X,X)\u0026amp;=var(X)\\\\ \\sigma_{X,X}\u0026amp;=\\sigma^2_X\\\\ \\end{align*}\\]\nProperty 3: The covariance of a two random variables \\(X\\) and \\(Y\\) each scaled by a constant \\(a\\) and \\(b\\) is the product of the covariance and the constants\n\\[cov(aX,bY)=a\\times b \\times cov(X,Y)\\]\nProperty 4: If two random variables are independent, their covariance is 0\n\\[cov(X,Y)=0 \\text{ iff } X \\text{ and } Y \\text{ are independent:} E(XY)=E(X)\\times E(Y)\\]\n Correlation Covariance, like variance, is often cumbersome, and the numerical value of the covariance of two random variables does not really mean much. It is often convenient to normalize the covariance to a decimal between \\(-1\\) and 1. We do this by dividing by the product of the standard deviations of \\(X\\) and \\(Y\\). This is known as the correlation coefficient between \\(X\\) and \\(Y\\), denoted \\(corr(X,Y)\\) or \\(\\rho_{X,Y}\\) (for populations) or \\(r_{X,Y}\\) (for samples):\n\\[\\begin{align*} r_{X,Y}\u0026amp;=\\frac{cov(X,Y)}{sd(X)sd(Y)}\\\\ \u0026amp;=\\frac{E\\big[(X-\\bar{X})(Y-\\bar{Y})\\big]}{\\sqrt{E\\big[X-\\bar{X}\\big]}\\sqrt{E\\big[Y-\\bar{Y}\\big]}}\\\\ \u0026amp;=\\frac{\\sigma_{X,Y}}{\\sigma_X \\sigma_Y}\\\\ \\end{align*}\\]\nNote this also means that covariance is the product of the standard deviation of \\(X\\) and \\(Y\\) and their correlation coefficient:\n\\[\\begin{align*} \\sigma_{X,Y}\u0026amp;=r_{X,Y}\\sigma_X \\sigma_Y \\\\ cov(X,Y)\u0026amp;=corr(X,Y)\\times sd(X) \\times sd(Y) \\\\ \\end{align*}\\]\nAnother way to reach the (sample) correlation coefficient is by finding the average joint \\(Z\\)-score of each pair of \\((X_i,Y_i)\\):\n\\[\\begin{align*} r_{X,Y}\u0026amp;=\\frac{1}{n}\\frac{\\displaystyle\\sum^n_{i=1}(X_i-\\bar{X})(Y_i-\\bar{Y}))}{s_X s_Y} \u0026amp;\u0026amp; \\text{Definition of sample correlation}\\\\ \u0026amp;=\\frac{1}{n}\\displaystyle\\sum^n_{i=1}\\bigg(\\frac{X_i-\\bar{X}}{s_X}\\bigg)\\bigg(\\frac{Y_i-\\bar{Y}}{s_Y}\\bigg) \u0026amp;\u0026amp; \\text{Breaking into separate sums} \\\\ \u0026amp;=\\frac{1}{n}\\displaystyle\\sum^n_{i=1}(Z_X)(Z_Y) \u0026amp;\u0026amp; \\text{Recognize each sum is the z-score for that r.v.} \\\\ \\end{align*}\\]\nCorrelation has some useful properties that should be familiar to you:\n Correlation is between \\(-1\\) and 1 A correlation of -1 is a downward sloping straight line A correlation of 1 is an upward sloping straight line A correlation of 0 implies no relationship  Calculating Correlation Example We can calculate the correlation of a simple data set (of 4 observations) using R to show how correlation is calculated. We will use the \\(Z\\)-score method. Begin with a simple set of data in \\((X_i, Y_i)\\) points:\n\\[ (1,1), (2,2), (3,4), (4,9) \\]\nlibrary(tidyverse) corr_example\u0026lt;-tibble(x=c(1,2,3,4), y=c(1,2,4,9)) ggplot(corr_example,aes(x=x,y=y))+geom_point() corr_example %\u0026gt;% summarize(mean_x = mean(x), #find mean of x, its 2.5 sd_x = sd(x), #find sd of x, its 1.291 mean_y = mean(y), #find mean of y, its 4 sd_y = sd(y)) #find sd of y, its 3.559 ## # A tibble: 1 × 4 ## mean_x sd_x mean_y sd_y ## \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 2.5 1.29 4 3.56 #take z score of x,y for each pair and multiply them corr_example \u0026lt;- corr_example %\u0026gt;% mutate(z_product = ((x-mean(x))/sd(x)) * ((y-mean(y))/sd(y))) corr_example %\u0026gt;% summarize(avg_z_product = sum(z_product)/(n()-1), # average z products over n-1 actual_corr = cor(x,y), #compare our answer to actual cor() command! covariance = cov(x,y)) # just for kicks, what\u0026#39;s the covariance?  ## # A tibble: 1 × 3 ## avg_z_product actual_corr covariance ## \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 0.943 0.943 4.33     Note this is a .dta Stata file. You will need to (install and) load the package haven to read_dta() Stata files into a dataframe.↩︎\n Note there will be a different in notation depending on whether we refer to a population (e.g. \\(\\mu_{X}\\)) or to a sample (e.g. \\(\\bar{X}\\)). As the overwhelming majority of cases we will deal with samples, I will use sample notation for means).↩︎\n Again, to be technically correct, \\(\\sigma_{X,Y}\\) refers to populations, \\(s_{X,Y}\\) refers to samples, in line with population vs. sample variance and standard deviation. Recall also that sample estimates of variance and standard deviation divide by \\(n-1\\), rather than \\(n\\). In large sample sizes, this difference is negligible.↩︎\n   ","date":1591574400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1632673341,"objectID":"ad5a3aa08c565b62bae597bfe8b9574b","permalink":"https://metricsf21.classes.ryansafner.com/content/2.3-content/","publishdate":"2020-06-08T00:00:00Z","relpermalink":"/content/2.3-content/","section":"content","summary":"Overview  Readings  Slides  Assignments  Problem Set 1  Math Appendix  Variance Covariance Correlation     Thursday, September 16, 2021    Problem Set 1 answers are posted on that page. Problem Set 2 is due by class Tuesday September 21.    Overview Today we start looking at associations between variables, which we will first attempt to quantify with measures like covariance and correlation.","tags":null,"title":"2.3 — OLS Linear Regression — Class Content","type":"docs"},{"authors":null,"categories":null,"content":"    Overview  Readings  Slides  Assignments  Problem Set 2  Math Appendix  Deriving the OLS Estimators Finding \\(\\hat{\\beta_0}\\) Finding \\(\\hat{\\beta_1}\\) Algebraic Properties of OLS Estimators Bias in \\(\\hat{\\beta_1}\\) Proof of the Unbiasedness of \\(\\hat{\\beta_1}\\)     Tuesday, September 21, 2021    Problem Set 2 is due by the end of the day today.    Overview Today we continue looking at basic OLS regression. We will cover how to measure if a regression line is a good fit (using \\(R^2\\) and \\(\\sigma_u\\) or SER), and whether OLS estimators are biased. These will depend on four critical assumptions about \\(u\\).\nIn doing so, we begin an ongoing exploration into inferential statistics, which will finally become clear in another week. The most confusing part is recognizing that there is a sampling distribution of each OLS estimator. We want to measure the center of that sampling distribution, to see if the estimator is biased. Next class we will measure the spread of that distribution.\nWe continue the extended example about class sizes and test scores, which comes from a (Stata) dataset from an old textbook that I used to use, Stock and Watson, 2007. Download and follow along with the data from today’s example:1\n  caschool.dta  I have also made a RStudio Cloud project documenting all of the things we have been doing with this data that may help you when you start working with regressions (next class):\n  Class Size Regression Analysis    Readings   Ch. 3.2-3.4, 3.7-3.8 in Bailey, Real Econometrics    Slides Below, you can find the slides in two formats. Clicking the image will bring you to the html version of the slides in a new tab. Note while in going through the slides, you can type h to see a special list of viewing options, and type o for an outline view of all the slides.\nThe lower button will allow you to download a PDF version of the slides. I suggest printing the slides beforehand and using them to take additional notes in class (not everything is in the slides)!\n \n Download as PDF\n  Assignments Problem Set 2 Problem Set 2 is due by Tuesday September 21. Please see the instructions for more information on how to submit your assignment (there are multiple ways!).\n  Math Appendix Deriving the OLS Estimators The population linear regression model is:\n\\[Y_i=\\beta_0+\\beta_1 X_i + u _i\\]\nThe errors \\((u_i)\\) are unobserved, but for candidate values of \\(\\hat{\\beta_0}\\) and \\(\\hat{\\beta_1}\\), we can obtain an estimate of the residual. Algebraically, the error is:\n\\[\\hat{u_i}= Y_i-\\hat{\\beta_0}-\\hat{\\beta_1}X_i\\]\nRecall our goal is to find \\(\\hat{\\beta_0}\\) and \\(\\hat{\\beta_1}\\) that minimizes the sum of squared errors (SSE):\n\\[SSE= \\sum^n_{i=1} \\hat{u_i}^2\\]\nSo our minimization problem is:\n\\[\\min_{\\hat{\\beta_0}, \\hat{\\beta_1}} \\sum^n_{i=1} (Y_i-\\hat{\\beta_0}-\\hat{\\beta_1}X_i)^2\\]\nUsing calculus, we take the partial derivatives and set it equal to 0 to find a minimum. The first order conditions are:\n\\[\\begin{align*} \\frac{\\partial SSE}{\\partial \\hat{\\beta_0}}\u0026amp;=-2\\displaystyle\\sum^n_{i=1} (Y_i-\\hat{\\beta_0}-\\hat{\\beta_1} X_i)=0\\\\ \\frac{\\partial SSE}{\\partial \\hat{\\beta_1}}\u0026amp;=-2\\displaystyle\\sum^n_{i=1} (Y_i-\\hat{\\beta_0}-\\hat{\\beta_1} X_i)X_i=0\\\\ \\end{align*}\\]\n Finding \\(\\hat{\\beta_0}\\) Working with the first FOC, divide both sides by \\(-2\\):\n\\[\\displaystyle\\sum^n_{i=1} (Y_i-\\hat{\\beta_0}-\\hat{\\beta_1} X_i)=0\\]\nThen expand the summation across all terms and divide by \\(n\\):\n\\[\\underbrace{\\frac{1}{n}\\sum^n_{i=1} Y_i}_{\\bar{Y}}-\\underbrace{\\frac{1}{n}\\sum^n_{i=1}\\hat{\\beta_0}}_{\\hat{\\beta_0}}-\\underbrace{\\frac{1}{n}\\sum^n_{i=1} \\hat{\\beta_1} X_i}_{\\hat{\\beta_1}\\bar{X}}=0\\]\nNote the first term is \\(\\bar{Y}\\), the second is \\(\\hat{\\beta_0}\\), the third is \\(\\hat{\\beta_1}\\bar{X}\\).2\nSo we can rewrite as: \\[\\bar{Y}-\\hat{\\beta_0}-\\beta_1=0\\]\nRearranging:\n\\[\\hat{\\beta_0}=\\bar{Y}-\\bar{X}\\beta_1\\]\n Finding \\(\\hat{\\beta_1}\\) To find \\(\\hat{\\beta_1}\\), take the second FOC and divide by \\(-2\\):\n\\[\\displaystyle\\sum^n_{i=1} (Y_i-\\hat{\\beta_0}-\\hat{\\beta_1} X_i)X_i=0\\]\nFrom the formula for \\(\\hat{\\beta_0}\\), substitute in for \\(\\hat{\\beta_0}\\):\n\\[\\displaystyle\\sum^n_{i=1} \\bigg(Y_i-[\\bar{Y}-\\hat{\\beta_1}\\bar{X}]-\\hat{\\beta_1} X_i\\bigg)X_i=0\\]\nCombining similar terms:\n\\[\\displaystyle\\sum^n_{i=1} \\bigg([Y_i-\\bar{Y}]-[X_i-\\bar{X}]\\hat{\\beta_1}\\bigg)X_i=0\\]\nDistribute \\(X_i\\) and expand terms into the subtraction of two sums (and pull out \\(\\hat{\\beta_1}\\) as a constant in the second sum:\n\\[\\displaystyle\\sum^n_{i=1} [Y_i-\\bar{Y}]X_i-\\hat{\\beta_1}\\displaystyle\\sum^n_{i=1}[X_i-\\bar{X}]X_i=0\\]\nMove the second term to the righthand side:\n\\[\\displaystyle\\sum^n_{i=1} [Y_i-\\bar{Y}]X_i=\\hat{\\beta_1}\\displaystyle\\sum^n_{i=1}[X_i-\\bar{X}]X_i\\]\nDivide to keep just \\(\\hat{\\beta_1}\\) on the right:\n\\[\\frac{\\displaystyle\\sum^n_{i=1} [Y_i-\\bar{Y}]X_i}{\\displaystyle\\sum^n_{i=1}[X_i-\\bar{X}]X_i}=\\hat{\\beta_1}\\]\nNote that from the rules about summation operators:\n\\[\\displaystyle\\sum^n_{i=1} [Y_i-\\bar{Y}]X_i=\\displaystyle\\sum^n_{i=1} (Y_i-\\bar{Y})(X_i-\\bar{X})\\]\nand:\n\\[\\displaystyle\\sum^n_{i=1} [X_i-\\bar{X}]X_i=\\displaystyle\\sum^n_{i=1} (X_i-\\bar{X})(X_i-\\bar{X})=\\displaystyle\\sum^n_{i=1}(X_i-\\bar{X})^2\\]\nPlug in these two facts:\n\\[\\frac{\\displaystyle\\sum^n_{i=1} (Y_i-\\bar{Y})(X_i-\\bar{X})}{\\displaystyle\\sum^n_{i=1}(X_i-\\bar{X})^2}=\\hat{\\beta_1}\\]\n Algebraic Properties of OLS Estimators The OLS residuals \\(\\hat{u}\\) and predicted values \\(\\hat{Y}\\) are chosen by the minimization problem to satisfy:\nThe expected value (average) error is 0: \\[E(u_i)=\\frac{1}{n}\\displaystyle \\sum_{i=1}^n \\hat{u_i}=0\\]\n The covariance between \\(X\\) and the errors is 0: \\[\\hat{\\sigma}_{X,u}=0\\]\n  Note the first two properties imply strict exogeneity. That is, this is only a valid model if \\(X\\) and \\(u\\) are not correlated.\nThe expected predicted value of \\(Y\\) is equal to the expected value of \\(Y\\): \\[\\bar{\\hat{Y}}=\\frac{1}{n} \\displaystyle\\sum_{i=1}^n \\hat{Y_i} = \\bar{Y}\\]\n Total sum of squares is equal to the explained sum of squares plus sum of squared errors: \\[\\begin{align*}TSS\u0026amp;=ESS+SSE\\\\ \\sum_{i=1}^n (Y_i-\\bar{Y})^2\u0026amp;=\\sum_{i=1}^n (\\hat{Y_i}-\\bar{Y})^2 + \\sum_{i=1}^n {u}^2\\\\ \\end{align*}\\]\n  Recall \\(R^2\\) is \\(\\frac{ESS}{TSS}\\) or \\(1-SSE\\)\nThe regression line passes through the point \\((\\bar{X},\\bar{Y})\\), i.e. the mean of \\(X\\) and the mean of \\(Y\\).   Bias in \\(\\hat{\\beta_1}\\) Begin with the formula we derived for \\(\\hat{\\beta_1}\\):\n\\[\\hat{\\beta_1}=\\frac{\\displaystyle \\sum^n_{i=1} (Y_i-\\bar{Y})(X_i-\\bar{X})}{\\displaystyle\\sum^n_{i=1} (X_i-\\bar{X})^2}\\]\nRecall from Rule 6 of summations, we can rewrite the numerator as\n\\[\\begin{align*} =\u0026amp;\\displaystyle \\sum^n_{i=1} (Y_i-\\bar{Y})(X_i-\\bar{X})\\\\ =\u0026amp; \\displaystyle \\sum^n_{i=1} Y_i(X_i-\\bar{X})\\\\ \\end{align*}\\]\n\\[\\hat{\\beta_1}=\\frac{\\displaystyle \\sum^n_{i=1} Y_i(X_i-\\bar{X})}{\\displaystyle\\sum^n_{i=1} (X_i-\\bar{X})^2}\\]\nWe know the true population relationship is expressed as:\n\\[Y_i=\\beta_0+\\beta_1 X_i+u_i\\]\nSubstituting this in for \\(Y_i\\) in equation 2:\n\\[\\hat{\\beta_1}=\\frac{\\displaystyle \\sum^n_{i=1} (\\beta_0+\\beta_1X_i+u_i)(X_i-\\bar{X})}{\\displaystyle\\sum^n_{i=1} (X_i-\\bar{X})^2}\\] Breaking apart the sums in the numerator:\n\\[\\hat{\\beta_1}=\\frac{\\displaystyle \\sum^n_{i=1} \\beta_0(X_i-\\bar{X})+\\displaystyle \\sum^n_{i=1} \\beta_1X_i(X_i-\\bar{X})+\\displaystyle \\sum^n_{i=1} u_i(X_i-\\bar{X})}{\\displaystyle\\sum^n_{i=1} (X_i-\\bar{X})^2}\\]\nWe can simplify equation 4 using Rules 4 and 5 of summations\nThe first term in the numerator \\(\\left[\\displaystyle \\sum^n_{i=1} \\beta_0(X_i-\\bar{X})\\right]\\) has the constant \\(\\beta_0\\), which can be pulled out of the summation. This gives us the summation of deviations, which add up to 0 as per Rule 4:  \\[\\begin{align*} \\displaystyle \\sum^n_{i=1} \\beta_0(X_i-\\bar{X})\u0026amp;= \\beta_0 \\displaystyle \\sum^n_{i=1} (X_i-\\bar{X})\\\\ \u0026amp;=\\beta_0 (0)\\\\ \u0026amp;=0\\\\ \\end{align*}\\]\nThe second term in the numerator \\(\\left[\\displaystyle \\sum^n_{i=1} \\beta_1X_i(X_i-\\bar{X})\\right]\\) has the constant \\(\\beta_1\\), which can be pulled out of the summation. Additionally, Rule 5 tells us \\(\\displaystyle \\sum^n_{i=1} X_i(X_i-\\bar{X})=\\displaystyle \\sum^n_{i=1}(X_i-\\bar{X})^2\\):  \\[\\begin{align*} \\displaystyle \\sum^n_{i=1} \\beta_1X_1(X_i-\\bar{X})\u0026amp;= \\beta_1 \\displaystyle \\sum^n_{i=1} X_i(X_i-\\bar{X})\\\\ \u0026amp;=\\beta_1\\displaystyle \\sum^n_{i=1}(X_i-\\bar{X})^2\\\\ \\end{align*}\\]\nWhen placed back in the context of being the numerator of a fraction, we can see this term simplifies to just \\(\\beta_1\\):\n\\[\\begin{align*} \\frac{\\beta_1\\displaystyle \\sum^n_{i=1}(X_i-\\bar{X})^2}{\\displaystyle\\sum^n_{i=1} (X_i-\\bar{X})^2} \u0026amp;=\\frac{\\beta_1}{1} \\times \\frac{\\displaystyle \\sum^n_{i=1}(X_i-\\bar{X})^2}{\\displaystyle\\sum^n_{i=1} (X_i-\\bar{X})^2}\\\\ \u0026amp;=\\beta_1 \\\\ \\end{align*}\\]\nThus, we are left with:\n\\[\\hat{\\beta_1}=\\beta_1+\\frac{\\displaystyle \\sum^n_{i=1} u_i(X_i-\\bar{X})}{\\displaystyle\\sum^n_{i=1} (X_i-\\bar{X})^2}\\]\nNow, take the expectation of both sides:\n\\[E[\\hat{\\beta_1}]=E\\left[\\beta_1+\\frac{\\displaystyle \\sum^n_{i=1} u_i(X_i-\\bar{X})}{\\displaystyle\\sum^n_{i=1} (X_i-\\bar{X})^2} \\right]\\]\nWe can break this up, using properties of expectations. First, recall \\(E[a+b]=E[a]+E[b]\\), so we can break apart the two terms.\n\\[E[\\hat{\\beta_1}]=E[\\beta_1]+E\\left[\\frac{\\displaystyle \\sum^n_{i=1} u_i(X_i-\\bar{X})}{\\displaystyle\\sum^n_{i=1} (X_i-\\bar{X})^2} \\right]\\]\nSecond, the true population value of \\(\\beta_1\\) is a constant, so \\(E[\\beta_1]=\\beta_1\\).\nThird, since we assume \\(X\\) is also “fixed” and not random, the variance of \\(X\\), \\(\\displaystyle\\sum_{i=1}^n (X_i-\\bar{X})\\), in the denominator, is just a constant, and can be brought outside the expectation.\n\\[E[\\hat{\\beta_1}]=\\beta_1+\\frac{E\\left[\\displaystyle \\sum^n_{i=1} u_i(X_i-\\bar{X})\\right] }{\\displaystyle\\sum^n_{i=1} (X_i-\\bar{X})^2}\\]\nThus, the properties of the equation are primarily driven by the expectation \\(E\\bigg[\\displaystyle \\sum^n_{i=1} u_i(X_i-\\bar{X})\\bigg]\\). We now turn to this term.\nUse the property of summation operators to expand the numerator term:\n\\[\\begin{align*} \\hat{\\beta_1}\u0026amp;=\\beta_1+\\frac{\\displaystyle \\sum^n_{i=1} u_i(X_i-\\bar{X})}{\\displaystyle\\sum^n_{i=1} (X_i-\\bar{X})^2} \\\\ \\hat{\\beta_1}\u0026amp;=\\beta_1+\\frac{\\displaystyle \\sum^n_{i=1} (u_i-\\bar{u})(X_i-\\bar{X})}{\\displaystyle\\sum^n_{i=1} (X_i-\\bar{X})^2} \\\\ \\end{align*}\\]\nNow divide the numerator and denominator of the second term by \\(\\frac{1}{n}\\). Realize this gives us the covariance between \\(X\\) and \\(u\\) in the numerator and variance of \\(X\\) in the denominator, based on their respective definitions.\n\\[\\begin{align*} \\hat{\\beta_1}\u0026amp;=\\beta_1+\\cfrac{\\frac{1}{n}\\displaystyle \\sum^n_{i=1} (u_i-\\bar{u})(X_i-\\bar{X})}{\\frac{1}{n}\\displaystyle\\sum^n_{i=1} (X_i-\\bar{X})^2} \\\\ \\hat{\\beta_1}\u0026amp;=\\beta_1+\\cfrac{cov(X,u)}{var(X)} \\\\ \\hat{\\beta_1}\u0026amp;=\\beta_1+\\cfrac{s_{X,u}}{s_X^2} \\\\ \\end{align*}\\]\nBy the Zero Conditional Mean assumption of OLS, \\(s_{X,u}=0\\).\nAlternatively, we can express the bias in terms of correlation instead of covariance:\n\\[E[\\hat{\\beta_1}]=\\beta_1+\\cfrac{cov(X,u)}{var(X)}\\]\nFrom the definition of correlation:\n\\[\\begin{align*} cor(X,u)\u0026amp;=\\frac{cov(X,u)}{s_X s_u}\\\\ cor(X,u)s_Xs_u \u0026amp;=cov(X,u)\\\\ \\end{align*}\\]\nPlugging this in:\n\\[\\begin{align*} E[\\hat{\\beta_1}]\u0026amp;=\\beta_1+\\frac{cov(X,u)}{var(X)} \\\\ E[\\hat{\\beta_1}]\u0026amp;=\\beta_1+\\frac{\\big[cor(X,u)s_xs_u\\big]}{s^2_X} \\\\ E[\\hat{\\beta_1}]\u0026amp;=\\beta_1+\\frac{cor(X,u)s_u}{s_X} \\\\ E[\\hat{\\beta_1}]\u0026amp;=\\beta_1+cor(X,u)\\frac{s_u}{s_X} \\\\ \\end{align*}\\]\n Proof of the Unbiasedness of \\(\\hat{\\beta_1}\\) Begin with equation:3\n\\[\\hat{\\beta_1}=\\frac{\\sum Y_iX_i}{\\sum X_i^2}\\]\nSubstitute for \\(Y_i\\):\n\\[\\hat{\\beta_1}=\\frac{\\sum (\\beta_1 X_i+u_i)X_i}{\\sum X_i^2}\\]\nDistribute \\(X_i\\) in the numerator:\n\\[\\hat{\\beta_1}=\\frac{\\sum \\beta_1 X_i^2+u_iX_i}{\\sum X_i^2}\\]\nSeparate the sum into additive pieces:\n\\[\\hat{\\beta_1}=\\frac{\\sum \\beta_1 X_i^2}{\\sum X_i^2}+\\frac{u_i X_i}{\\sum X_i^2}\\]\n\\(\\beta_1\\) is constant, so we can pull it out of the first sum:\n\\[\\hat{\\beta_1}=\\beta_1 \\frac{\\sum X_i^2}{\\sum X_i^2}+\\frac{u_i X_i}{\\sum X_i^2}\\]\nSimplifying the first term, we are left with:\n\\[\\hat{\\beta_1}=\\beta_1 +\\frac{u_i X_i}{\\sum X_i^2}\\]\nNow if we take expectations of both sides:\n\\[E[\\hat{\\beta_1}]=E[\\beta_1] +E\\left[\\frac{u_i X_i}{\\sum X_i^2}\\right]\\]\n\\(\\beta_1\\) is a constant, so the expectation of \\(\\beta_1\\) is itself.\n\\[E[\\hat{\\beta_1}]=\\beta_1 +E\\bigg[\\frac{u_i X_i}{\\sum X_i^2}\\bigg]\\]\nUsing the properties of expectations, we can pull out \\(\\frac{1}{\\sum X_i^2}\\) as a constant:\n\\[E[\\hat{\\beta_1}]=\\beta_1 +\\frac{1}{\\sum X_i^2} E\\bigg[\\sum u_i X_i\\bigg]\\]\nAgain using the properties of expectations, we can put the expectation inside the summation operator (the expectation of a sum is the sum of expectations):\n\\[E[\\hat{\\beta_1}]=\\beta_1 +\\frac{1}{\\sum X_i^2}\\sum E[u_i X_i]\\]\nUnder the exogeneity condition, the correlation between \\(X_i\\) and \\(u_i\\) is 0.\n\\[E[\\hat{\\beta_1}]=\\beta_1\\]\n   Note this is a .dta Stata file. You will need to (install and) load the package haven to read_dta() Stata files into a dataframe.↩︎\n From the rules about summation operators, we define the mean of a random variable \\(X\\) as \\(\\bar{X}=\\frac{1}{n}\\displaystyle\\sum_{i=1}^n X_i\\). The mean of a constant, like \\(\\beta_0\\) or \\(\\beta_1\\) is itself.↩︎\n Admittedly, this is a simplified version where \\(\\hat{\\beta_0}=0\\), but there is no loss of generality in the results.↩︎\n   ","date":1591574400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1632673341,"objectID":"a5f04f4e7a03061c12108bead4b3ee08","permalink":"https://metricsf21.classes.ryansafner.com/content/2.4-content/","publishdate":"2020-06-08T00:00:00Z","relpermalink":"/content/2.4-content/","section":"content","summary":"Overview  Readings  Slides  Assignments  Problem Set 2  Math Appendix  Deriving the OLS Estimators Finding \\(\\hat{\\beta_0}\\) Finding \\(\\hat{\\beta_1}\\) Algebraic Properties of OLS Estimators Bias in \\(\\hat{\\beta_1}\\) Proof of the Unbiasedness of \\(\\hat{\\beta_1}\\)     Tuesday, September 21, 2021    Problem Set 2 is due by the end of the day today.    Overview Today we continue looking at basic OLS regression.","tags":null,"title":"2.4 — OLS: Goodness of Fit and Bias — Class Content","type":"docs"},{"authors":null,"categories":null,"content":"    Overview  Readings  Slides  R Practice  Assignments  Problem Set 2 Answers  New Packages Mentioned Appendix  Robust Standard Errors in R     Thursday, September 23, 2021 and Tuesday, September 28, 2021 (for R Practice)\nNote: there is a video posted on Blackboard for the Thursday September 23 lecture.    Problem Set 2 answers are posted on that page.    Overview Last class and this class we are looking at the sampling distibution of OLS estimators (particularly \\(\\hat{\\beta_1})\\). Last class we looked at what the center of the distribution was - the true \\(\\beta_1\\) - so long as the assumptions about \\(u\\) hold:\n When \\(cor(X,u)=0\\), \\(X\\) is exogenous and the OLS estimators are unbiased. What \\(cor(X,u)\\neq 0\\), \\(X\\) is endogenous and the OLS estimators are biased.  Today we continue looking at the sampling distibution by determining the variation in \\(\\hat{beta_1}\\) (it’s variance or its standard error1). We look at the formula and see the three major determinants of variation in \\(\\hat{\\beta_1}\\):\nGoodness of fit of the regression \\((SER\\) or \\(\\hat{\\sigma_u}\\) Sample size \\(n\\) Variation in \\(X\\)  We also look at the diagnostics of a regression by looking at its residuals \\((\\hat{u_i})\\) for anomalies. We focus on the problem of heteroskedasticity (where the variation in \\(\\hat{u_i])\\) changes over the range of \\(X\\), which violates assumption 2 (errors are homoskedastic): how to detect it, test it, and fix it with some packages. We also look at outliers, which can bias the regression. Finally, we also look at how to present regression results.\nWe continue the extended example about class sizes and test scores, which comes from a (Stata) dataset from an old textbook that I used to use, Stock and Watson, 2007. Download and follow along with the data from today’s example:2\n  caschool.dta  I have also made a RStudio Cloud project documenting all of the things we have been doing with this data that may help you when you start working with regressions:\n  Class Size Regression Analysis (Cloud R project)  Class Size Regression Analysis (output)    Readings   Finish Ch.3 in Bailey, Real Econometrics    Slides Below, you can find the slides in two formats. Clicking the image will bring you to the html version of the slides in a new tab. Note while in going through the slides, you can type h to see a special list of viewing options, and type o for an outline view of all the slides.\nThe lower button will allow you to download a PDF version of the slides. I suggest printing the slides beforehand and using them to take additional notes in class (not everything is in the slides)!\n \n Download as PDF\n  R Practice Today you will be working on R practice problems on regression. Answers will be posted later on that page.\n  Assignments Problem Set 2 Answers Problem Set 2 answers are posted on that page.\n  New Packages Mentioned  broom: for tidy regression outputs, summary statistics, and adding \\(\\hat{Y_i}\\) and \\(\\hat{u_i}\\) into the dataframe huxtable: to present regression output in a table with huxreg() lmtest: for testing for heteroskedasticity in errors with bptest() car: for testing for outliers with outlierTest() estimatr: for calculating robust standard errors with lm_robust()   Appendix Robust Standard Errors in R This, since I started using huxtable instead of another package (stargazer) to make regression tables, I have gone all in on estimatr’s lm_robust() option to calculate robust standard errors. Before this, there were some other methods that I had to resort to. You can read about that in this blog post.\n   The square root of variance, as always!↩︎\n Note this is a .dta Stata file. You will need to (install and) load the package haven to read_dta() Stata files into a dataframe.↩︎\n   ","date":1591574400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1633121116,"objectID":"bc46232eb5ef6a58a23767cd86692b0d","permalink":"https://metricsf21.classes.ryansafner.com/content/2.5-content/","publishdate":"2020-06-08T00:00:00Z","relpermalink":"/content/2.5-content/","section":"content","summary":"Overview  Readings  Slides  R Practice  Assignments  Problem Set 2 Answers  New Packages Mentioned Appendix  Robust Standard Errors in R     Thursday, September 23, 2021 and Tuesday, September 28, 2021 (for R Practice)\nNote: there is a video posted on Blackboard for the Thursday September 23 lecture.    Problem Set 2 answers are posted on that page.","tags":null,"title":"2.5 — OLS: Precision and Diagnostics — Class Content","type":"docs"},{"authors":null,"categories":null,"content":"  To minimize confusion, I suggest creating a new R Project (e.g. regression_practice) and storing any data in that folder on your computer.\nI have already made an R project you can download (as a .zip), unzip, and open the .Rproj file in R Studio, or there is an R project you can use on the cloud:\n R Project  R Studio Cloud Answers  Answers (html)  Answers (R Project)  Answers (R Script)  Data Cleaning How did the results of the 2016 election in their district affect how often Members of Congress during the Trump Administration voted with the President?\nFirst, as always, load tidyverse!\nQuestion 1 Our data comes from fivethirtyeight’s Trump Congress tracker. Fivethirtyeight has a great public archive of all of the data they use for their articles. I have saved a copy to this website so you can download it below. Download and import (read_csv()) the data into an object.\n  congress-trump-score.csv  The variables that we care about are\n    Variable Description    congress Congressional session  chamber Chamber of Congress  last_name Member of Congress  party Political Party  state U.S. State  district Congressional district  agree_pct Proportion of votes that agree with President Trump (0-1)  net_trump_vote District’s margin of victory in 2016 election (positive is Trump win, negative is Clinton win)     Question 2 Look at the data with glimpse(). How many variables are there? How many observations?\n Question 3 agree_pct is oddly named, given that its values range from 0 to 1. Make a new variable (you can overwrite agree_pct) that is a true percentage, from 0 to 100, and use this going forward (it will make interpretation of our results easier).\n Question 4 Make a scatterplot of agree_pct on net_trump_vote. Add a regression line by adding an additional layer of geom_smooth(method = \"lm\").\n Question 5 Find the correlation between agree_pct and net_trump_vote. Hint: if using tidyverse, like calculating any statistic, we want the summarize() the data.\n  Regression Question 6 We want to predict the following model:\n\\[\\widehat{\\text{agree_pct}}= \\hat{\\beta_0}+\\hat{\\beta_1} \\,\\text{net_trump_vote}\\] Run a regression and save it as an object. Then get a summary() of the object.\nPart A What is \\(\\hat{\\beta_0}\\) for this model? What does it mean in the context of our question?\n Part B What is \\(\\hat{\\beta_1}\\) for this model? What does it mean in the context of our question?\n Part C What is \\(R^2\\) for this model? What does it mean in the context of our question?\n Part D What is the \\(SER\\) for this model? What does it mean in the context of our question?\n  Question 7 We can look at regression outputs in a tidier way, with the broom package.\nPart A Install (if you have not yet done so) and load the broom package.\n Part B Run the function tidy() on your regression object (saved in question 6). Save this result as an object and then look at it.\n Part C Run the glance() function on your original regression object. What does it show you? Find \\(R^2\\) and the SER and confirm they are the same from the Base R lm output.\n Part D Now run the augment() function on your original regression object, and save this as an object. Look at it. What does it show you?\n  Question 8 Let’s use our broom results to calculate the goodness of fit statistics you found in question 6 to confirm.\nCalculate \\(R^2\\) as \\(\\frac{ESS}{TSS}\\) by taking the variance of \\(\\widehat{\\text{agree_pct}_i}\\) (.fitted in the augmented object you made in Question 6D) over the variance of \\(\\text{agree_pct}\\).\nAlternately, you can calculate \\(R^2\\) as \\(1-\\frac{SSE}{TSS}\\) by taking 1 minus sum of squared \\(\\hat{u_i}\\) (.resid in that same augmented object) over the variance of \\(\\text{agree_pct}\\).\nOptional: If you really want to be fancy, make your own function to calculate the sum of squares of \\(\\hat{Y}\\) and \\(Y_i\\) (instead of variance), as I did, with:\nsum_sq = function(x){sum((x - mean(x))^2)} and then running this function on agree_pct and .fitted.\n Question 9 Now let’s try presenting your results in a regression table. Install and load the huxtable package, and run the huxreg() command. Your main input is your regression object you saved in Question 6. Feel free to customize the output of this table (see the slides).\n  Regression Diagnostics Question 10 Now let’s start looking at the residuals of the regression.\nPart A Take the augmented regression object from Question 7D and use it as the source of your data to create a histogram with ggplot(), setting aes(x = .resid). Does it look roughly normal?\n Part B Take the augmented regression object and make a residual plot, which is a scatterplot where x is the normal x variable, and y is the .resid. Feel free to add a horizontal line at 0 with geom_hline(yintercept = 0) as an additional layer.\n  Question 11 Now let’s check for heteroskedasticity.\nPart A Looking at the scatterplot and residual plots in Questions 3 and 7B, do you think the errors are heteroskedastic or homoskedastic?\n Part B Install and load the lmtest package and run bptest() on your saved lm object from Question 6. According to the test, is the data heteroskedastic or homoskedastic?\n Part C Now let’s make some heteroskedasticity-robust standard errors. Install and load the estimatr package and use the lm_robust() command (instead of the lm() command) to run a new regression (and save it). Make sure you add se_type = \"stata\" inside the command to calculate robust SEs (in the same way that the Stata software does…long story). Look at it. What changes?\n Part D Now let’s see this in a nice regression table. Use huxreg() again, but add both your original regression and your regression saved in part C. Notice any changes?\n  Question 12 Now let’s check for outliers.\nPart A Just looking at the scatterplot in Question 3, do you see any outliers?\n Part B Install and load the car package. Run the outlierTest command on your regression object. Does it detect any outliers?\n Part C Look in your original data to match this outlier with an observation. Hint: use the slice() command, as the outlier test gave you an observation (row) number!\n  Question 13 (Optional: Flexing your tidyverse skills)\nThis data is still a bit messy. Let’s check your tidyverse skills again! For example, we’d probably like to plot our scatterplots with colors for Republican and Democratic party. Or plot by the House and the Senate.\nPart A First, the variable congress (session of Congress) seems a bit off. Get a count() of congress.\n Part B Let’s get rid of the 0 values for congress (someone made a mistake coding this, probably).\n Part C The variable party is also quite a mess. count() by party to see. Then let’s mutate a variable to make a better measure of political party - just \"Republican\", \"Democrat\", and \"Independent\". Try doing this with the case_when() command (as your mutate formula).\nThe syntax for case_when() is to have a series of condition ~ \"Outcome\", separated by commas. For example, one condition is to assign both \"Democrat\" and \"D\" to \"Democrat\", as in party %in% c(\"Democrat\", \"D\") ~ \"Democrat\". You could also do this with a few ifelse() commands, but that’s a bit more awkward.] When you’re done count() by your new party variable to make sure it worked.\n Part D Now plot a scatterplot (same as Question 4) and set color to your party variable. Notice R uses its own default colors, which don’t match to the actual colors these political parties use! Make a vector where you define the party colors as follows:\nparty_colors \u0026lt;- c(\u0026quot;Democrat\u0026quot; = \u0026quot;blue\u0026quot;, \u0026quot;Republican\u0026quot; = \u0026quot;red\u0026quot;, \u0026quot;Independent\u0026quot; = \u0026quot;gray\u0026quot;) Then, run your plot again, adding the following layer to customize the colors scale_colour_manual(\"Parties\", values = party_colors). \"Parties\" is the title that will show up on the legend, feel free to edit it, or remove the legend with another layer +guides(color = F).\n Part E Now facet your scatterplot by chamber by adding a layer: facet_wrap(~chamber).\n   ","date":1591574400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1633522772,"objectID":"9cd9ae9dfc7cf714c15a990f623d2025","permalink":"https://metricsf21.classes.ryansafner.com/r/2.5-r/","publishdate":"2020-06-08T00:00:00Z","relpermalink":"/r/2.5-r/","section":"r","summary":"To minimize confusion, I suggest creating a new R Project (e.g. regression_practice) and storing any data in that folder on your computer.\nI have already made an R project you can download (as a .zip), unzip, and open the .Rproj file in R Studio, or there is an R project you can use on the cloud:\n R Project  R Studio Cloud Answers  Answers (html)  Answers (R Project)  Answers (R Script)  Data Cleaning How did the results of the 2016 election in their district affect how often Members of Congress during the Trump Administration voted with the President?","tags":null,"title":"2.5 — OLS: Precision and Diagnostics — Practice","type":"docs"},{"authors":null,"categories":null,"content":"    Overview  Readings  Slides  R Practice  Assignments  Problem Set 3 Due Thurs Oct 7  New Packages Mentioned Appendix  Differences Between What We Learned in Class and Classical Statistics Inferential Statistics Basics The Central Limit Theorem If We Don’t Know \\(\\sigma\\): The Student’s \\(t\\)-Distribution Confidence Intervals     Thursday, September 30, 2021 (moved from Tuesday)    Problem Set 3 is due by the end of the day Thursday, October 7.    Overview We begin with some more time for you to work on the R Practice from last class.\nThis week is about inferential statistics: using statistics calculated from a sample of data to infer the true (and unmeasurable) parameters that describe a population. In doing so, we can run hypothesis tests on our sample to determine a point estimate of a parameter, or construct a confidence interval from our sample to cast a range for the true parameter.\nThis is standard principles of statistics - you hopefully should have learned it before. If it has been a while (or never) since your last statistics class, this is one of the hardest concepts to understand at first glance. I recommend Khan Academy [From sampling distributions through significance tests, for this. Though the whole class is helpful!] or Google for these concepts, as every statistics class will cover them in the standard way.\nThat being said, I do not cover inferential statistics in the standard way (see the appendix today below for an overview of the standard way). I think it will be more intuitive if I show you where these concepts come from by simulating a sampling distribution, as opposed to reciting the theoretical sampling distributions.\n  Readings   Ch.4 in Bailey, Real Econometrics  Ch. 10 (optionally 8-9) in Modern Dive  Visualizing Probability and Inference  Bailey teaches inferential statistics in the classical way (with reference to theoretical \\(Z\\) and \\(t\\) distributions, and \\(Z\\) and \\(t\\) tests). This is all valid. Again, you may wish to brush up with Khan Academy [From sampling distributions through significance tests, for this. Though the whole class is helpful!].\nThe latter “book” (also free online, like R4DS) uses the infer package to run simulations for inferential statistics. Chapter 10 is focused on regression (but I also recommend the chapters leading up to it, which are on inferential statistics broadly, using this method).\nThe final link is a great website for visualizing basic statistic concepts like probability, distributions, confidence intervals, hypothesis tests, central limit theorem, and regression.\n  Slides Below, you can find the slides in two formats. Clicking the image will bring you to the html version of the slides in a new tab. Note while in going through the slides, you can type h to see a special list of viewing options, and type o for an outline view of all the slides.\nThe lower button will allow you to download a PDF version of the slides. I suggest printing the slides beforehand and using them to take additional notes in class (not everything is in the slides)!\n \n Download as PDF\n  R Practice Today you will be working on R practice problems on regression. Answers will be posted later on that page.\n  Assignments Problem Set 3 Due Thurs Oct 7 Problem Set 3 is due by the end of the day on Thursday, October 7.\n  New Packages Mentioned  infer: for simulation for statistical inference   Appendix Differences Between What We Learned in Class and Classical Statistics In class, you learned the basics behind inferential statistics–p-values, confidence intervals, hypothesis testing–via empirical simulation of many samples permuted from our existing data. We took our sample, ran 1,000 simulations by permutation of our sample without replacement1, calculated the statistic (\\(\\hat{\\beta}_0\\), the slope) of each simulation; this gave us a (sampling) distribution of our sample statistics, and then found the probability on that distribution that we would observe our actual statistic in our actual data – this is the \\(p\\)-value.\nClassically, before the use of computers that could run and visualize 1,000s of simulations within seconds,2 inferential statistics was taught using theoretical distributions. Essentially, we calculate a test-statistic by normalizing our finding against a theoretical (null) sampling distribution of our sample statistic, and find p-values by estimating the probability of observing that statistic on that theoretical distribution. These distributions are almost always normal or normal-like distributions. The distribution that we almost always use in econometrics is the (Student’s) \\(t\\)-distribution.\nFurthermore, testing the null hypothesis \\(H_0: \\,\\beta_1=0\\), is not the only type of hypothesis test, nor is the slope the only statistic we can test. In fact, there are many different types of hypothesis tests that are well-known and well-used, we focused entirely on regression (since that is the largest tool of the course).\nThis appendix will give you more background on the theory of inferential statistics, and is more in line with what you may have learned in earlier statistics courses.\n Inferential Statistics Basics It is important to remember several statistical distributions, tools, and facts. Most of them have to do with the normal distribution. If \\(X\\) is normally distributed with mean \\(\\mu\\) and standard deviation \\(\\sigma\\):\n\\[X \\sim N(\\mu, \\sigma)\\]\nlibrary(tidyverse) ## ── Attaching packages ─────────────────────────────────────── tidyverse 1.3.1 ── ## ✓ ggplot2 3.3.5 ✓ purrr 0.3.4 ## ✓ tibble 3.1.4 ✓ dplyr 1.0.7 ## ✓ tidyr 1.1.3 ✓ stringr 1.4.0 ## ✓ readr 2.0.0 ✓ forcats 0.5.1 ## ── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ── ## x dplyr::filter() masks stats::filter() ## x dplyr::lag() masks stats::lag() # plot a theoretical normal distribution without any data # our \u0026quot;geom\u0026quot; here is the \u0026quot;stat_function()\u0026quot; command, which draws statistical functions normal.pdf \u0026lt;- ggplot(data = data.frame(x = -4:4), # our \u0026quot;data\u0026quot; is just a sequence of x from -4 to 4 aes(x = x))+ stat_function(fun = dnorm, # dnorm is the normal distributions args = list(mean = 0, sd = 1), # set mean = 0 and sd = 1 color=\u0026quot;blue\u0026quot;)+ scale_x_continuous(breaks = seq(-4,4,1))+ xlab(\u0026quot;X (or Z-score)\u0026quot;)+ylab(\u0026quot;p(X)\u0026quot;)+ theme_light() normal.pdf Then recall the 68-95-99.7 empirical rule, that:\n \\(P(\\mu-1 \\sigma \\leq X \\leq \\mu+ 1\\sigma) \\approx 0.68\\) \\(P(\\mu-2 \\sigma \\leq X \\leq \\mu+ 2\\sigma) \\approx 0.95\\) \\(P(\\mu-3 \\sigma \\leq X \\leq \\mu+ 3\\sigma) \\approx 0.997\\)  Again, in English: “68% of the observations fall within 1 standard deviation of the mean; 95% fall within 2 standard deviations of the mean, and 99.7% fall within 3 standard deviations of the mean.”\nIf we have the standard normal distribution with mean 0 and standard deviation 1:\nAgain, we can standardize any normally-distributed random variable by finding the Z-score of each observation:\n\\[Z=\\frac{X_i - \\mu}{\\sigma}\\]\nThis ensures the mean will be 0 and standard deviation will be 1. Thus, \\(Z\\) is the number of standard deviations above \\((+)\\) or below \\((-)\\) the mean an observation is.\nWe can use \\(Z\\)-scores to find the probability of any range of observations of \\(X\\) occuring in the distribution.\npnorm(-2, mean = 0, sd = 1, lower.tail = TRUE) # area to left of -2 ## [1] 0.02275013 pnorm(2, mean = 0, sd = 1, lower.tail = TRUE) # area to left of 2 ## [1] 0.9772499 pnorm(2, mean = 0, sd = 1, lower.tail = FALSE) # area to RIGHT of 2 ## [1] 0.02275013 pnorm(2, mean = 0, sd = 1, lower.tail = TRUE)- pnorm(-2, mean = 0, sd = 1, lower.tail = TRUE) # area between -2 and 2 ## [1] 0.9544997  The Central Limit Theorem Inferential statistics can be summarized in 2 sentences:\n There are unknown parameters that describe a population distribution that we want to know. We use statistics that describe a sample to estimate the population parameters.\n Recall there is an element of randomness in our sample statistics due to sampling variability. For example, if we take the mean of one sample, \\(\\bar{x}\\), and then take the mean of a different sample, the \\(\\bar{x}\\)’s will be slightly different. We can conceive of a distribution of \\(\\bar{x}\\)’s across many different samples, and this is called the sampling distribution of the statistic \\((\\bar{x})\\).\nVia the sampling distribution, the sample statistic \\((\\bar{X})\\) itself is distributed with\n mean \\(E[\\bar{x}]=\\mu_X\\) (the true population mean) standard deviation \\(\\sigma_{\\bar{x}}\\)  Central Limit Theorem: with large enough sample size (\\(n\\geq30\\)), the sampling distribution of a sample statistic is approximately normal3\nThus, the sampling distribution of the sample mean (\\(\\bar{x}\\)):\n\\[\\bar{X} \\sim \\left(\\mu_X, \\frac{\\sigma_X}{\\sqrt{n}}\\right)\\]\nThe second term we call the standard error of the sample mean4. Note that it takes the true standard deviation of the population (\\(\\sigma_X\\))5 and divides it by the square root of the sample size, \\(\\sqrt{n}\\).\nThus if we know the true population standard deviation (\\(\\sigma_X\\)) then we can simply use the normal distribution for confidence intervals and hypothesis tests of a sample statistic. Since we often do not, we need to use another distribution for inferential statistics, often the \\(t\\)-distribution.\n If We Don’t Know \\(\\sigma\\): The Student’s \\(t\\)-Distribution We rarely, if ever, know the true population standard deviation for variable \\(X\\), \\(\\sigma_X\\). Additionally, we sometimes have sample sizes of \\(n \u0026lt; 30\\). If either of these conditions are true, we cannot use leverage the Central Limit Theorem and simplify with a standard normal distribution. Instead of the normal distribution, we use a Student’s t-Distribution6\n\\(t\\) is functionally equivalent to the idea of a \\(Z\\)-score, with some slight modifications:\n\\[t = \\cfrac{\\bar{x}-\\mu}{\\left(\\frac{s}{\\sqrt{n}}\\right)}\\] - \\(\\bar{x}\\) is our estimated statistic (e.g. sample mean) - \\(\\mu\\) is the true population parametner (e.g. population mean) - \\(s\\) is the sample standard deviation - \\(n\\) is the sample size\n\\(t\\)-scores similarly measure the number of standard deviations an observation is above or below the mean.\nThe other main difference between normal distributions/\\(Z\\)-scores and \\(t\\) distributions /\\(t\\)-scores is that \\(t\\) distributions have \\(n-1\\) degrees of freedom.7\n\\[t \\sim t_{n-1}\\]\n## Loading required package: viridisLite The standard \\(t\\)-distribution looks normal-ish, with a mean of 0, but with more area in the tails of the distribution. The exact shape depends on the degrees of freedom \\((n-1)\\). As \\(\\uparrow n\\), \\(\\uparrow df\\), the \\(t\\)-distribution approximates a normal distribution.\nBy convention, in regression we always use \\(t\\)-distributions for confidence intervals and hypothesis tests. For nearly all of the confidence intervals and hypothesis tests below, we functionally replace \\(Z\\) with \\(t\\).\n Confidence Intervals A confidence interval describes the range of estimates for a population parameter in the form:\n\\[(\\text{estimate} - \\text{margin of error}, \\, \\text{estimate} + \\text{margin of error})\\]\nOur confidence level is \\(1-\\alpha\\)\n \\(\\alpha\\) again is the “significance level”, the probability that the true population parameter is not within our confidence interval8 Typical confidence levels: 90%, 95%, 99%   A confidence interval tells us that if we were to conduct many samples, (\\(1-\\alpha\\))% would contain the true population parameter within the interval\n To construct a confidence interval, we do the following:\nCalculate the sample statistic.\n Find \\(Z\\)-score that corresponds to desired confidence level.9 We need to find what are called the “critical values” of \\(Z\\), which we will call \\(Z_{0.5\\alpha}\\) on the normal distribution that puts (\\(1-\\alpha\\)) probability between \\(\\pm Z_{0.5\\alpha}\\) and \\(0.5\\alpha\\) in each of the tails of the distribution. The distribution would look like this:\n  ## Warning in is.na(x): is.na() applied to non-(list or vector) of type ## \u0026#39;expression\u0026#39; ## Warning in is.na(x): is.na() applied to non-(list or vector) of type ## \u0026#39;expression\u0026#39; ## Warning in is.na(x): is.na() applied to non-(list or vector) of type ## \u0026#39;expression\u0026#39;  The confidence interval between the two \\(Z\\)-scores \\(-Z_{0.5\\alpha}\\) and \\(Z_{0.5\\alpha}\\) contains the desired \\((1-\\alpha)%\\) of observations The area beyond each \\(Z\\)-score contains \\(0.5\\alpha\\)% of observations in each direction, for a total of \\(\\alpha\\)% beyond the critical values of \\(Z\\)  Note that the image above is abstract. So for example, if we wanted a (typical) 95% confidence interval with \\(\\alpha=0.05\\), the critical value(s) of \\(Z\\) are \\(\\pm 1.96\\)10, and looking on the distribution:\nThe critical values of \\(Z\\) are often given in \\(Z\\)-tables, which you can find in classical statistics textbooks or online. Critical values of \\(Z\\) for common confidence intervals values are well known:\n  Confidence Level \\(\\alpha\\) \\(\\pm Z_{0.5\\alpha}\\)    90% 0.10 \\(\\pm 1.645\\)  95% 0.05 \\(\\pm 1.96\\)  99% 0.01 \\(\\pm 2.58\\)    Calculate the margin of error (MOE)  The margin of error is the critical value of \\(Z\\) times the standard error of the estimate (\\(\\sigma\\)).\n\\[MOE=Z_{0.5\\alpha}\\sigma\\]\nConstruct the confidence interval  The confidence interval is simply our estimate plus and minus the margin of error.\n\\[CI = \\left([\\bar{x}-Z_{0.5}\\sigma_{\\bar{x}}], \\, [\\bar{x}+Z_{0.5}\\sigma_{\\bar{x}}]\\right)\\]\nIntepret the confidence interval in the context of the problem   “We estimate with [1-alpha]% confidence that the true [population parameter] is between [lowerbound] and [upperbound]”.\n    That is, for each simulation, we randomly selected observations from our existing sample to be in the simulation, and then did not put that observation back in the pool to possibly be selected again.↩︎\n Even when I was in graduate school, 2011–2015↩︎\n If samples are i.i.d. (independently and identically distributed if they are drawn from the same population randomly and then replaced) we don’t even need to know the population distribution to assume normality↩︎\n Instead of the “standard deviation”. “Standard error” refers to the sampling variability of a sample statistic, and is always talking about a sampling distribution.↩︎\n Which we need to know! We often do not know it!↩︎\n “Student” was the penname of William Sealy Gosset, who has one of the more interesting stories in statistics. He worked for Guiness in Ireland testing the quality of beer. He found that with small sample sizes, normal distributions did not yield accurate results. He came up with a more accurate distribution, and since Guiness would not let him publish his findings, published it under the pseudonym of “Student.”↩︎\n Degrees of freedom, \\(df\\) are the number of independent values used for the calculation of a statistic minus the number of other statistics used as intermediate steps. For sample standard deviation \\(s\\), we use \\(n\\) deviations \\((x_i-\\bar{x})\\) and 1 parameter \\((\\bar{x})\\), hence \\(df=n-1\\)↩︎\n Equivalently, \\(\\alpha\\) is the probability of a Type I error: a false positive finding where we incorrectly reject a null hypothesis when it the null hypothesis is in fact true.↩︎\n Of course, if we don’t know the population \\(\\sigma\\), we need to use the \\(t\\)-distribution and find critical \\(t\\)-scores instead of \\(Z\\)-scores. See above.↩︎\n Note this is the precise value behind the rule of thumb that 95% of observations fall within 2 standard deviations of the mean!↩︎\n   ","date":1591574400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1633014579,"objectID":"bd0682d43b70cdef65a2d03255813cde","permalink":"https://metricsf21.classes.ryansafner.com/content/2.6-content/","publishdate":"2020-06-08T00:00:00Z","relpermalink":"/content/2.6-content/","section":"content","summary":"Overview  Readings  Slides  R Practice  Assignments  Problem Set 3 Due Thurs Oct 7  New Packages Mentioned Appendix  Differences Between What We Learned in Class and Classical Statistics Inferential Statistics Basics The Central Limit Theorem If We Don’t Know \\(\\sigma\\): The Student’s \\(t\\)-Distribution Confidence Intervals     Thursday, September 30, 2021 (moved from Tuesday)    Problem Set 3 is due by the end of the day Thursday, October 7.","tags":null,"title":"2.6 — Statistical Inference — Class Content","type":"docs"},{"authors":null,"categories":null,"content":"    Overview  Readings  Slides  R Practice  Assignments  Problem Set 3 Due Thurs Oct 7  New Packages Mentioned    Thursday, September 30, 2021 and Tuesday, October 5, 2021    Problem Set 3 is due by the end of the day Thursday, October 7.    Overview Today we finish our discussion of statistical inference, focusing on hypothesis testing about our regression. We will hopefully have time at the end to work on a few practice problems.\n  Readings   Ch.4 in Bailey, Real Econometrics  Ch. 10 (optionally 8-9) in Modern Dive  Visualizing Probability and Inference  Last Week Tonight with John Oliver: Scientific Studies “There is Still Only One Test”  The readings are the same as last class, plus one highly recommended additional video: be sure to watch the excellent and hilarious discussion of the limits and misuses of scientific studies and statistical significance \\((p\\)-values) in the Last Week Tonight clip. I also added the blog post that I reference in class about all hypothesis testing is really one common procedure.\n  Slides Below, you can find the slides in two formats. Clicking the image will bring you to the html version of the slides in a new tab. Note while in going through the slides, you can type h to see a special list of viewing options, and type o for an outline view of all the slides.\nThe lower button will allow you to download a PDF version of the slides. I suggest printing the slides beforehand and using them to take additional notes in class (not everything is in the slides)!\n \n Download as PDF\n  R Practice Today you will be working on R practice problems on regression. Answers will be posted later on that page.\n  Assignments Problem Set 3 Due Thurs Oct 7 Problem Set 3 is due by the end of the day on Thursday, October 7.\n  New Packages Mentioned  infer: for simulation for statistical inference   ","date":1591574400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1634418322,"objectID":"3532d625095902642ea227f2ad0554fe","permalink":"https://metricsf21.classes.ryansafner.com/content/2.7-content/","publishdate":"2020-06-08T00:00:00Z","relpermalink":"/content/2.7-content/","section":"content","summary":"Overview  Readings  Slides  R Practice  Assignments  Problem Set 3 Due Thurs Oct 7  New Packages Mentioned    Thursday, September 30, 2021 and Tuesday, October 5, 2021    Problem Set 3 is due by the end of the day Thursday, October 7.    Overview Today we finish our discussion of statistical inference, focusing on hypothesis testing about our regression.","tags":null,"title":"2.7 — Inference for Regression — Class Content","type":"docs"},{"authors":null,"categories":null,"content":"  Set Up To minimize confusion, I suggest creating a new R Project (e.g. infer_practice) and storing any data in that folder on your computer.\nI have already made an R project you can download (as a .zip), unzip, and open the .Rproj file in R Studio, or there is an R project you can use on the cloud:\n R Project  R Studio Cloud Answers  Answers (html)  Answers (R Project)  Answers (R Script)  Question 1 Let’s use the diamonds data built into ggplot. Simply load tidyverse and then to be explicit, save this as a tibble (feel free to rename it) with diamonds \u0026lt;- diamonds.\nLet’s answer the following questions:\n What is the effect of carat size on a diamond’s price?\n Part A Just to see what we’re looking at, make a scatterplot using ggplot(), with x as carat and y as price, and add a regression line.\n  Question 2 Suppose we want to estimate the following relationship:\n\\[\\widehat{\\text{price}}_i = \\beta_0 + \\beta_1 \\text{carat}_i + u_i\\]\nRun a regression of price on carat using lm() and get a summary() of it. Be sure to save your regression model as an object, we’ll need it later.\nPart A Write out the estimated regression equation.\n Part B Make a regression table of the output (using the huxtable package).\n Part C What is \\(\\hat{\\beta_1}\\) for this model? Interpret it in the context of our question.\n Part D Use the broom package’s tidy() command on your regression object, and calculate confidence intervals for your estimates by setting conf.int = T inside tidy().\nWhat is the 95% confidence interval for \\(\\hat{\\beta_1}\\), and what does it mean?\nSave these endpoints as an object (either by taking your tidy()-ed regression and filter()-ing the term == \"carat\" and then select()-ing the columns with the confidence interval and then saving this; or simply assigning the two values as a vector, c( , ), to an object).\n Part E Save your estimated \\(\\hat{\\beta_1}\\) as an object, we’ll need it later with infer (either by taking your tidy()-ed regression and filter()-ing the term == carat and then select()-ing the column with the estimate and then saving this; or simply assigning the values to an object).\n  Question 3 Now let’s use infer. Install it if you don’t have it, then load it.\nPart A Let’s generate a confidence interval for \\(\\hat{\\beta_1}\\) by simulating the sampling distribution of \\(\\hat{\\beta_1}\\). Run the following code, which will specify() the model relationship and generate() 1,000 repetitions using the bootstrap method of resampling data points randomly from our sample, with replacement.\n# save our simulations as an object (I called it \u0026quot;boot\u0026quot;) boot \u0026lt;- diamonds %\u0026gt;% # or whatever you named your dataframe! specify(carat ~ price) %\u0026gt;% # our regression model generate(reps = 1000, # run 1000 simulations type = \u0026quot;bootstrap\u0026quot;) %\u0026gt;% # using bootstrap method calculate(stat = \u0026quot;slope\u0026quot;) # estimate slope in each simulation # look at it boot Note this will take a few minutes, its doing a lot of calculations! What does it show you when you look at it?\n Part B Continue by piping your object from Part A into get_confidence_interval(). Set level = 0.95, type = \"se\" and point_estimate equal to our estimated \\(\\hat{\\beta_1}\\) (saved) from Question 2 Part E.\nboot %\u0026gt;% get_confidence_interval(level = 0.95, type = \u0026quot;se\u0026quot;, point_estimate = beta_1_hat) # or whatever you saved it as  Part C Now instead of get_confidence_interval(), pipe your object from Part A into visualize() to see the sampling distribution of \\(\\hat{\\beta_1}\\) we simulated. You can add + shade_ci(endpoints = ...) setting the argument equal to whatever you called your object containing the confidence interval from Question 2 Part D (I have it named here as CI_endpoints).\nboot %\u0026gt;% visualize()+ shade_ci(endpoints = CI_endpoints) # or whatever you saved it as Compare your simulated confidence interval with the theoretically-constructed confidence interval from the output of summary, and/or of tidy() from Question 2.\n  Question 4 Now let’s test the following hypothesis:\n\\[\\begin{align*} H_0: \\beta_1 \u0026amp;= 0\\\\ H_1: \\beta_1 \u0026amp;\\neq 0\\\\ \\end{align*}\\]\nPart A What does the output of summary, and/or of tidy() from Question 2 tell you?\n Part B Let’s now do run this hypothesis test with infer, which will simulate the sampling distribution under the null hypothesis that \\(\\beta_1 = 0\\). Run the following code, which will specify() the model relationship and hypothesize() the null hypothesis that there is no relationship between \\(X\\) and \\(Y\\) (i.e. \\(\\beta_1=0)\\), and generate() 1,000 repetitions using the permute method, which will center the distribution at 0, and then calculate(stat = \"slope\").\n# save our simulations as an object (I called it \u0026quot;test_sims\u0026quot;) test_sims \u0026lt;- diamonds %\u0026gt;% # or whatever you named your dataframe! specify(carat ~ price) %\u0026gt;% # our regression model hypothesize(null = \u0026quot;independence\u0026quot;) %\u0026gt;% # H_0 is that slope is 0 generate(reps = 1000, # run 1000 simulations type = \u0026quot;permute\u0026quot;) %\u0026gt;% # using permute method, centering distr at 0 calculate(stat = \u0026quot;slope\u0026quot;) # estimate slope in each simulation # look at it test_sims Note this may also take a few minutes. What does it show you?\n Part C Pipe your object from the previous part into the following code, which will get_p_value(). Inside this function, we are setting obs_stat equal to our \\(\\hat{\\beta_1}\\) we found (from Question 2 part E), and set direction = \"both\" to run a two-sided test, since our alternative hypothesis is two-sided, \\(H_1: \\beta_1 \\neq 0\\).\ntest_sims %\u0026gt;% get_p_value(obs_stat = beta_1_hat, direction = \u0026quot;both\u0026quot;) Note the warning message that comes up!\n Part D Instead of get_p_value(), pipe your object from Part B into the following code, which will visualize() the null distribution, and (in the second command), place our finding on it and shade_p_value().\ntest_sims %\u0026gt;% visualize() test_sims %\u0026gt;% visualize()+ shade_p_value(obs_stat = beta_1_hat, # or whatever you saved it as direction = \u0026quot;both\u0026quot;) # for two-sided test  Part E Compare your simulated p-value with the theoretically-constructed p-value from the output of summary, and/or of tidy() from Question 2.\nBoth summary and tidy() also report the \\(t\\)-statistic (t value or statistic) on this test for carat (by default, that \\(H_0: \\beta_1=0)\\). What is the estimated test statistic for this model, and what does this number mean? Try to calculate it yourself with the formula:\n\\[t = \\frac{\\text{estimate} - \\text{null hypothesis value}}{\\text{standard error of estimate}}\\]\nThe p-value is the probability of a \\(t\\) statistic at least as large as ours if the null hypothesis were true. R constructs a \\(t\\)-distribution with n-k-1 degrees of freedom (n is number of observations, k is number of \\(X\\)-variables) and calculates the probability in the tails of the distribution beyond this \\(t\\) value. You can calculate it yourself (for a two-sided test) with:\n2 * pt(your_t, # put your t-statistic here df = your_df, # put the df number here lower.tail = F) # we\u0026#39;ll use the right tail    ","date":1591574400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1633522772,"objectID":"1927d8d053af0a9c3804bdf98bd0baa6","permalink":"https://metricsf21.classes.ryansafner.com/r/2.7-r/","publishdate":"2020-06-08T00:00:00Z","relpermalink":"/r/2.7-r/","section":"r","summary":"Set Up To minimize confusion, I suggest creating a new R Project (e.g. infer_practice) and storing any data in that folder on your computer.\nI have already made an R project you can download (as a .zip), unzip, and open the .Rproj file in R Studio, or there is an R project you can use on the cloud:\n R Project  R Studio Cloud Answers  Answers (html)  Answers (R Project)  Answers (R Script)  Question 1 Let’s use the diamonds data built into ggplot.","tags":null,"title":"2.7 — Inference for Regression - R Practice","type":"docs"},{"authors":null,"categories":null,"content":"    Overview  Readings  Slides    Tuesday, October 19, 2021    Overview Today we begin extending the foundation of simple linear regression with one variable into more advanced models that can plausibly claim (when we are done) to measure causal relationships between \\(X\\) and \\(Y\\). The rest of the semester is primarily extending regression from one \\(X\\) to many, and changing the functional form to fit various idiosyncracies of different variables, or use clever techniques to isolate marginal effects of interest.\nWe begin by covering the fundamental problem of causal inference, that we can never observe counterfactual states of the world. If we could, then we could easily measure the causal effect of \\(X \\mapsto Y\\) by comparing how \\(Y\\) is different when \\(X\\) is different. The next best thing we can do is run a random control trial (RCT) where individuals are randomly assigned to groups to be given (different) treatment(s), and then we can compare the average outcome across groups. Random assignment ensures the only thing that differs across group outcomes is whether or not the group was given treatment, estimating the causal effect of treatment on the outcome.\nFor now, we will understand causality to mean the average treatment effect (ATE) from a RCT. RCTs are both popular and controversial. Last year’s Nobel Prize winners in economics won for their use of RCTs in development economics, but they have drawn significant criticism from other top economists as not being sufficiently generalizable.\nOf course, the bigger problem is it is very difficult, often impossible, to run a RCT to test a hypothesis. So economists have developed a toolkit of clever techniques to identify causal effects in “natural experiments” or “quasi-experiments” that sufficiently simulate a RCT. Knowledge of this repertoire of tools is truly why modern economists are in demand by government and business (not supply and demand models, etc)!\n  Readings   Ch.1 in Bailey, Real Econometrics  Ch. 4 in Cunningham (2020), Causal Inference, the Mixtape  Rubin Causal Model  Bailey begins the book with a discussion of causality and random control trials that is pretty good.\nThe potential outcomes notation (e.g. \\(Y_i^{1}\\) and \\(Y_i^{0})\\) and model comes from a very famous 1974 paper by Donald Rubin in psychology. You can read more about it in Cunningham (2020) above, or the Wikipedia entry on the model.\nScott Cunningham’s excellent (and free!) Causal Inference, the Mixtape has a great discussion of the history, and examples, of potential outcomes in an accessible way.\nThe classic example that most economists (including myself) were taught about causality is the treatment of the Rubin model in Angrist and Pischke’s Mostly Harmless Econometrics (one of the classic books on econometrics). You do not need to buy that book for this class, but if you will be doing data work in your future, or going to graduate school, this book is a must own and read:\n  Angrist and Pischke, 2009, Mostly Harmless Econometrics  My health insurance example is lifted directly out of this book.\nHere’s also a great list of famous social science (including economics) papers that use natural experiments:\n  List of 19 Natural Experiments  For more on John Snow and the birth of epidemiology, the excellent PBS show Victoria has a full episode (and great resources) about the cholera outbreak.\n  Slides Below, you can find the slides in two formats. Clicking the image will bring you to the html version of the slides in a new tab. Note while in going through the slides, you can type h to see a special list of viewing options, and type o for an outline view of all the slides.\nThe lower button will allow you to download a PDF version of the slides. I suggest printing the slides beforehand and using them to take additional notes in class (not everything is in the slides)!\n \n Download as PDF\n ","date":1591574400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1634418322,"objectID":"c86d4a6144014e7073dabaff0e3036c8","permalink":"https://metricsf21.classes.ryansafner.com/content/3.1-content/","publishdate":"2020-06-08T00:00:00Z","relpermalink":"/content/3.1-content/","section":"content","summary":"Overview  Readings  Slides    Tuesday, October 19, 2021    Overview Today we begin extending the foundation of simple linear regression with one variable into more advanced models that can plausibly claim (when we are done) to measure causal relationships between \\(X\\) and \\(Y\\). The rest of the semester is primarily extending regression from one \\(X\\) to many, and changing the functional form to fit various idiosyncracies of different variables, or use clever techniques to isolate marginal effects of interest.","tags":null,"title":"3.1 — The Fundamental Problem of Causal Inference \u0026 Potential Outcomes — Class Content","type":"docs"},{"authors":null,"categories":null,"content":"    Overview  Readings  Slides New Packages Mentioned  R Practice    Thursday, October 21, 2021    Overview Today we return to a more nuanced discussion of causality, given what we have learned about the fundamental problem of causal inference (counterfactuals and potential outcomes). RCTs are great, but they are not everything — and in any case, you are never going to be able to design and run an RCT in the overwhelming majority of studies.\nNow that we understand counterfactuals, we can apply our idea of exogeneity to argue that indeed, yes, correlation does imply causation when \\(X\\) is exogenous! That is, \\(X\\) being correlated with \\(Y\\) implies there is a causal connection between \\(X\\) and \\(Y\\), and if we are certain that \\(cor(X,u)=0\\), then we are clearly measuring the causal effect of \\(X \\rightarrow Y\\)! If \\(cor(X,u) \\neq 0\\) and \\(X\\) is endogenous, there is still a causal connection between \\(X\\) and \\(Y\\), but it goes through other variables that jointly cause \\(X\\) and \\(Y\\).\nWe also introduce a new tool for thinking about simple causal models, the directed acyclic graph (DAG). These are a hip new trend for thinking about causal inference, so new and trendy that they aren’t really in any mainstream textbooks yet!\nDAGS and DAG rules (front doors, back doors, colliders, mediators, etc.) will allow you to visually map the causal relationships between variables and describe to you the variables you must control for in order to properly identify the causal effect you are trying to measure. I show you a simply tool, daggity.net that will help you do this, as well as ggdag in R.\n  Readings DAGs are a trendy new concept in econometrics and causal inference, so much so that they have yet to find their way into any major econometrics textbook! There are some resources, however, that you can look to for understanding how they work (and I base much of my lecture off of them).\n  Ch. 3 in Cunningham (2020), Causal Inference, the Mixtape  Pearl and MacKenzie, (2018), The Book of Why  Heiss (2020), Causal Inference\"  Huntington-Klein (2019), Dagitty.net Cheat Sheet\"  Huntington-Klein (2019), Causal Diagrams Cheat Sheet\"  My blog post on “Econometrics, Data Science, and Causal Inference”  The best book to get more into the philosophy of causality and the major origin of DAGs is Judea Pearl (and David McKenzie)’s The Book of Why. We owe much to Pearl, he is the flagship of the causal revolution (outside of econometrics).1 And his twitter is pretty amusing.\nThe best instantiation of DAGs and causal inference into a “textbook” on econometrics and methods is Scott Cunningham’s (open source!) Causal Inference: The Mixtape chapter on DAGs. Nick Huntington-Klein has some great lecture slides, and some cheat sheets on using Dagitty.net and understanding DAGs.\nAndrew Heiss, a political science professor, has a great recent book chapter on causal inference using DAGs, complete with instructions on how to do it in R and dagitty.net.\nFinally, I have a blog post discussing the difference between econometrics, causal inference, and data science. The end touches on causality, DAGs, and Pearl.\n  Slides Below, you can find the slides in two formats. Clicking the image will bring you to the html version of the slides in a new tab. Note while in going through the slides, you can type h to see a special list of viewing options, and type o for an outline view of all the slides.\nThe lower button will allow you to download a PDF version of the slides. I suggest printing the slides beforehand and using them to take additional notes in class (not everything is in the slides)!\n \n Download as PDF\n New Packages Mentioned  dagitty.net: not a package in R, but an online tool to help you work with DAGs (see the cheat sheets in today’s readings page)\n ggdag: for drawing DAGs in ggplot, and for identifying pathways ggdag_paths() and required adjustments ggdag_adjustment_set()\n    R Practice See today’s practice problems to help you use DAGs and daggity.net. Answers will be posted on that page.\n  He has an interesting and contentious relationship to economics.↩︎\n   ","date":1591574400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1634655798,"objectID":"2ed982478e771342f96d703684ca57e4","permalink":"https://metricsf21.classes.ryansafner.com/content/3.2-content/","publishdate":"2020-06-08T00:00:00Z","relpermalink":"/content/3.2-content/","section":"content","summary":"Overview  Readings  Slides New Packages Mentioned  R Practice    Thursday, October 21, 2021    Overview Today we return to a more nuanced discussion of causality, given what we have learned about the fundamental problem of causal inference (counterfactuals and potential outcomes). RCTs are great, but they are not everything — and in any case, you are never going to be able to design and run an RCT in the overwhelming majority of studies.","tags":null,"title":"3.2 — Causal Inference \u0026 DAGs — Class Content","type":"docs"},{"authors":null,"categories":null,"content":"  Answers  Answers (html)  Answers (Rmd) For each of the following examples:\nWrite out all of the causal pathways from X (treatment of interest) to Y (outcome of interest). Identify which variable(s) need to be controlled to estimate the causal effect of X on Y. You can use dagitty.net to help you, but you should start trying to recognize these on your own! Draw the DAGs in r using ggdag. After setting up the dag with dagify() (and specifying exposure and outcome inside dagify), pipe that into ggdag(). Try again piping it instead into ggdag_status() (to highlight what is X and what is Y). Try again piping it instead into ggdag_adjustment_set() to show what needs to be controlled.  Don’t forget to install ggdag!\n Question 1  Question 2  Question 3  Question 4  Question 5  ","date":1591574400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1634828589,"objectID":"2a5fa0ec474b3c8eb80a227f61401a4a","permalink":"https://metricsf21.classes.ryansafner.com/r/3.2-r/","publishdate":"2020-06-08T00:00:00Z","relpermalink":"/r/3.2-r/","section":"r","summary":"Answers  Answers (html)  Answers (Rmd) For each of the following examples:\nWrite out all of the causal pathways from X (treatment of interest) to Y (outcome of interest). Identify which variable(s) need to be controlled to estimate the causal effect of X on Y. You can use dagitty.net to help you, but you should start trying to recognize these on your own! Draw the DAGs in r using ggdag.","tags":null,"title":"3.2 — Causal Inference \u0026 DAGs — R Practice","type":"docs"},{"authors":null,"categories":null,"content":"   Thursday, October 14 (11:25 AM — 12:50 PM) on Blackboard Assignments   Exam 1 will be held on Blackboard assignments from 11:25 AM — 12:50 PM on Thursday October 14 (our class time).\nThe exam will be a timed assignment. You will not need the whole time, I have designed the test to give you some extra time to accommodate the technical difficulties of taking an exam at home. You may close the exam page and come back to it, but the timer will continue to run once the exam is first opened.\nIf you have any approved testing accommodations, or know in advance you must be absent, please confirm with me ASAP and we will make arrangements\nWhile the exam is out, neither the TA nor I will answer any questions. The one exception is, email/message me ASAP if you encounter technical problems.\nExam Instructions ( Important, Please read) This exam will be open-book/website, open-notes, but do not use the internet, or contact others. [I have designed the exam so that Google won’t help you. Everything you need should be in your notes, homeworks, or slides anyway.] I will automatically interpret substantially-similar answers, obviously-googled answers, and contact between students as cheating or plagiarism, respectively. Either will earn you a 0, and be reported. Answers should be in your own words, I will also deduct points for blatant copy-pastes off my slides (those are my words, not yours!).\nYou must show all of your work (for calculation problems), either by typing your steps into the answer form, or you may upload handwritten steps to the exam, solving your problems. Only as a last resort (please!), you may email me your work before your time runs out.\nIt is more important to me that you spend your time working and writing the answers, and less about all the technology working flawlessly.\n Concepts Study Guide   Download as PDF  Review Questions Slides](/slides/midterm-review.html) --   Midterm Review Questions   Midterm Review Questions   “Practice” Exam   Download as PDF   Formula Sheet   Download as PDF   My Advice Make sure you do all of the homework problems and learn from the answer keys to the homeworks, as well as the in-class practice problems. While some of the questions should be novel applications, conceptual questions on homeworks will get you in the right headspace to think about answering a question on an exam.\n ","date":1591574400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1633617494,"objectID":"79f7798dc3de9a0dfcc76548f76d5e82","permalink":"https://metricsf21.classes.ryansafner.com/assignments/01-exam/","publishdate":"2020-06-08T00:00:00Z","relpermalink":"/assignments/01-exam/","section":"assignments","summary":"Thursday, October 14 (11:25 AM — 12:50 PM) on Blackboard Assignments   Exam 1 will be held on Blackboard assignments from 11:25 AM — 12:50 PM on Thursday October 14 (our class time).\nThe exam will be a timed assignment. You will not need the whole time, I have designed the test to give you some extra time to accommodate the technical difficulties of taking an exam at home.","tags":null,"title":"Midterm Exam","type":"docs"},{"authors":null,"categories":null,"content":"   Please complete this by Sunday August 29.   This is an ungraded and anonymous survey for me to evaluate the distribution of your math and statistics backgrounds. Please complete all problems to the best of your ability. Your responses will help me craft the course to see which material we need to focus on at greater length, especially review material.\nPlease submit your answers in this Google Form. Feel free to download the PDF and work out the problems on paper first.1\n Download PDF  Answer Key Question 1 Using the following sample data:\n\\[8, 12, 9, 10, 11, 5, 15\\]\nFind the median. Calculate the sample mean, \\(\\bar{x}\\) Calculate the sample standard deviation, \\(s\\)   Question 2 For a fair, six-sided die:\nWhat is the probability of rolling a 5? What is the probability of rolling an even number? What is the probability of rolling an even number or a 3? If you have two fair dice, what is the probability of rolling a 6 on both dice?   Question 3 Hedge fund A earns an average rate of return of 2.5% per year with a standard deviation of 0.5%, while hedge fund B earns an average rate of return of 3.0% per year with a standard deviation of 2.0%. Which is more unusual, Hedge fund A earning a 4.0% return or hedge fund B earning a return -1.0% return? Why?2\n Question 4 A discrete random variable \\(X\\) has the following pdf:\n   x p(x)    10 0.1  20 0.2  30 0.3  40 0.4    Calculate the sample standard deviation, \\(s\\) of \\(X\\).\n Question 5 The random variable \\(Y\\) is normally distributed with a mean of 50 and standard deviation of 12\n\\[Y \\sim N (50,12)\\]\nWhat is the \\(Z\\)-score for \\(Y=74\\)? In your own words, what does this \\(Z\\)-score mean? What is the probability that \\(Y\\) takes on a value greater than 74?   Question 6 On a scale of 1 (least) to 10 (most), how anxious are you about this class? Feel free to share any specific anxieties (they have a better chance to be specifically addressed if you do!).\n Question 7 On a scale of 1 (least) to 10 (most), how familiar would you say you are with computer programming and/or statistical software?\n Question 8 List any statistical software packages (e.g. R, Microsoft Excel, Stata, SAS, SPSS, Minitab, etc.) and any programming languages (e.g. html, php, C/++, Python, LaTeX, etc.) you have had any experience with, and rate your proficiency between 1 (least) and 5 (most), if applicable.\n  Ordinarily I would have you submit paper copies in class to maintain anonymity, but this is the best method right now instead of email or Blackboard.↩︎\n Hint: Standardize the two hedge funds.↩︎\n   ","date":1591574400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1630449216,"objectID":"29433abb7116e601cbc0d17dfe3696f5","permalink":"https://metricsf21.classes.ryansafner.com/assignments/00-preliminary-survey/","publishdate":"2020-06-08T00:00:00Z","relpermalink":"/assignments/00-preliminary-survey/","section":"assignments","summary":"Please complete this by Sunday August 29.   This is an ungraded and anonymous survey for me to evaluate the distribution of your math and statistics backgrounds. Please complete all problems to the best of your ability. Your responses will help me craft the course to see which material we need to focus on at greater length, especially review material.\nPlease submit your answers in this Google Form.","tags":null,"title":"Preliminary Survey on Statistics and Software","type":"docs"},{"authors":null,"categories":null,"content":"   Please submit on Blackboard Assignments by class on Tuesday September 14.   Please read the instructions for completing homeworks.\n PDF  R Project  R Studio Cloud The  PDF is useful if you want to print out the problem set. The  R project is a zipped .zip file which contains a  .Rmd file to write answers in, and the data, all in a logical working directory. (See this resource for help unzipping files). You can also just write an .R file in the project if you don’t want to use markdown. If you use the  cloud project, I have already installed tidyverse and tinytex (to produce pdfs).\nAnswers  Answers (html)  Answers (PDF)  Answers (R Project)  Answers (R Script)  The Popularity of Baby Names Install and load the package babynames. Get help for ?babynames to see what the data includes.\nQuestion 1 Part A What are the top 5 boys names for 2017, and what percent of overall names is each?\n Part B What are the top 5 girls names for 2017, and what percent of overall names is each?\n  Question 2 Make two barplots of these top 5 names, one for each sex. Map aesthetics x to name and y to prop [or percent, if you made that variable, as I did.] and use geom_col (since you are declaring a specific y, otherwise you could just use geom_bar() and just an x.)\n Question 3 Find your name. [If your name isn’t in there 😟, pick a random name.] count by sex how many babies since 1880 were named your name. [Hint: if you do this, you’ll get the number of rows (years) there are in the data. You want to add the number of babies in each row (n), so inside count, add wt = n to weight the count by n.] Also add a variable for the percent of each sex.\n Question 4 Make a line graph of the number of babies with your name over time, colored by sex.\n Question 5 Part A Find the most common name for boys by year between 1980-2017. [Hint: you’ll want to first group_by(year). Once you’ve got all the right conditions, you’ll get a table with a lot of data. You only want to slice(1) to keep just the 1st row of each year’s data.]\n Part B Now do the same for girls.\n  Question 6 Now let’s graph the evolution of the most common names since 1880.\nPart A First, find out what are the top 10 overall most popular names for boys and for girls in the data. [Hint: first group_by(name).] You may want to create two objects, each with these top 5 names as character elements.\n Part B Now make two linegraphs of these 5 names over time, one for boys, and one for girls. [Hint: you’ll first want to subset the data to use for your data layer in the plot. First group_by(year) and also make sure you only use the names you found in Part A. Try using the %in% command to do this.]\n  Question 7 Bonus (a challenge!): What are the 10 most common “gender-neutral” names? [This is hard to define. For our purposes, let’s define this as names where between 48% and 52% of the babies with the name are Male.]\n  Political and Economic Freedom Around the World For the remaining questions, we’ll look at the relationship between Economic Freedom and Political Freedom in countries around the world today. Our data for economic freedom comes from the Fraser Institute, and our data for political freedom comes from Freedom House.\nQuestion 8 Download these two datasets that I’ve cleaned up a bit: [If you want a challenge, try downloading them from the websites and cleaning them up yourself!]\n  econ_freedom.csv  pol_freedom.csv  Below is a brief description of the variables I’ve put in each dataset:\nEcon Freedom   Variable Description    year Year  ISO Three-letter country code  country Name of the country  ef_index Total economic freedom index (0 - least to 100 - most)  rank Rank of the country in terms of economic freedom  continent Continent the country is in     Pol Freedom   Variable Description    country Name of the country  C/T Whether the location is a country (C) or territory (T)  year Year  status Whether the location is Free (F), Partly Free (F) or Not Free (NF)  fh_score Total political freedom index (0 - least to 100 - most)    Import and save them each as an object using my_df_name \u0026lt;- read_csv(\"name_of_the_file.csv\"). I suggest one as econ and the other as pol, but it’s up to you. Look at each object you’ve created.\n  Question 9 Now let’s join them together so that we can have a single dataset to work with. You can learn more about this in the 1.4 slides. Since both datasets have both country and year (spelled exactly the same in both!), we can use these two variables as a key to combine observations. Run the following code (substituting whatever you want to name your objects):\nfreedom \u0026lt;- left_join(econ, pol, by=c(\u0026quot;country\u0026quot;, \u0026quot;year\u0026quot;) Take a look at freedom to make sure it appears to have worked.\n Question 10 Part A Make a barplot of the 10 countries with the highest Economic Freedom index score in 2018. You may want to find this first and save it as an object for your plot’s data layer. Use geom_col() since we will map ef_index to y. If you want to order the bars, set x = fct_reorder(ISO, desc(ef_index)) to reorder ISO (or country, if you prefer) by EF score in descending order.\n Part B Make a barplot of the 10 countries with the highest Freedom House index score in 2018, similar to what you did for Part A.\n  Question 11 Now make a scatterplot of Political freedom (fh_score as y) on Economic Freedom (ef_index as x) and color by continent.\n Question 12 Save your plot from Question 11 as an object, and add a new layer where we will highlight a few countries. Pick a few countries (I suggest using the ISO code) and create a new object filtering the data to only include these countries (again the %in% command will be most helpful here).\n Additionally, install and load a package called \"ggrepel\", which will adjust labels so they do not overlap on a plot.\n  Then, add the following layer to your plot:\n geom_label_repel(data = countries, # or whatever object name you created aes(x = ef_index, y = fh_score, label = ISO, # show ISO as label (you could do country instead) color = continent), alpha = 0.5, # make it a bit transparent box.padding = 0.75, # control how far labels are from points show.legend = F) # don\u0026#39;t want this to add to the legend This should highlight these countries on your plot.\n Question 13 Let’s just look only at the United States and see how it has fared in both measures of freedom over time. filter() the data to look only at ISO == \"USA\". Use both a geom_point() layer and a geom_path() layer, which will connect the dots over time. Let’s also see this by labeling the years with an additional layer geom_text_repel(aes(label = year)).\n  ","date":1591574400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1631909996,"objectID":"65f00e2106af56948d0e931c96630394","permalink":"https://metricsf21.classes.ryansafner.com/assignments/01-problem-set/","publishdate":"2020-06-08T00:00:00Z","relpermalink":"/assignments/01-problem-set/","section":"assignments","summary":"Please submit on Blackboard Assignments by class on Tuesday September 14.   Please read the instructions for completing homeworks.\n PDF  R Project  R Studio Cloud The  PDF is useful if you want to print out the problem set. The  R project is a zipped .zip file which contains a  .Rmd file to write answers in, and the data, all in a logical working directory.","tags":null,"title":"Problem Set 1","type":"docs"},{"authors":null,"categories":null,"content":"   Please submit on Blackboard Assignments by class on Tuesday September 21.   Please read the instructions for completing homeworks.\n PDF  R Project  R Studio Cloud The  PDF is useful if you want to print out the problem set. The  R project is a zipped .zip file which contains a  .Rmd file to write answers in, and the data, all in a logical working directory. (See this resource for help unzipping files). You can also just write an .R file in the project if you don’t want to use markdown. If you use the  cloud project, I have already installed tidyverse and tinytex (to produce pdfs).\nAnswers  Answers (html)  Answers (PDF)  Answers (R Project)  Theory and Concepts Question 1 In your own words, explain the difference between endogeneity and exogeneity.\n Question 2 Part A In your own words, explain what (sample) standard deviation means.\n Part B In your own words, explain how (sample) standard deviation is calculated. You may also write the formula, but it is not necessary.\n   Problems For the remaining questions, you may use R to verify, but please calculate all sample statistics by hand and show all work.\nQuestion 3 Suppose you have a very small class of four students that all take a quiz. Their scores are reported as follows:\n\\[\\{83, 92, 72, 81\\}\\]\nPart A Calculate the median.\n Part B Calculate the sample mean, \\(\\bar{x}\\).\n Part C Calculate the sample standard deviation, \\(s\\).\n Part D Make or sketch a rough histogram of this data, with the size of each bin being 10 (i.e. 70’s, 80’s, 90’s, 100’s). You can draw this by hand or use R. [If you are using ggplot, you want to use +geom_histogram(breaks=seq(start,end,by)) and add +scale_x_continuous(breaks=seq(start,end,by)). For each, it creates bins in the histogram, and ticks on the x axis by creating a sequence starting at start (a number), ending at end (number), by a certain interval (i.e. by 10s.).] Is this distribution roughly symmetric or skewed? What would we expect about the mean and the median?\n Part E Suppose instead the person who got the 72 did not show up that day to class, and got a 0 instead. Recalculate the mean and median. What happened and why?\n  Question 4 Suppose the probabilities of a visitor to Amazon’s website buying 0, 1, or 2 books are 0.2, 0.4, and 0.4 respectively.\nPart A Calculate the expected number of books a visitor will purchase.\n Part B Calculate the standard deviation of book purchases.\n Part C Bonus: try doing this in R by making an initial dataframe of the data, and then making new columns to the “table” like we did in class.\n  Question 5 Scores on the SAT (out of 1600) are approximately normally distributed with a mean of 500 and standard deviation of 100.\nPart A What is the probability of getting a score between a 400 and a 600?\n Part B What is the probability of getting a score between a 300 and a 700?\n Part C What is the probability of getting at least a 700?\n Part D What is the probability of getting at most a 700?\n Part E What is the probability of getting exactly a 500?\n  Question 6 Redo problem 5 by using the pnorm() command in R. [Hint: This function has four arguments: 1. the value of the random variable, 2. the mean of the distribution, 3. the sd of the distribution, and 4. lower.tail TRUE or FALSE.]\n  ","date":1591574400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1632675497,"objectID":"1d2148f2bde48545ca176203f0df9440","permalink":"https://metricsf21.classes.ryansafner.com/assignments/02-problem-set/","publishdate":"2020-06-08T00:00:00Z","relpermalink":"/assignments/02-problem-set/","section":"assignments","summary":"Please submit on Blackboard Assignments by class on Tuesday September 21.   Please read the instructions for completing homeworks.\n PDF  R Project  R Studio Cloud The  PDF is useful if you want to print out the problem set. The  R project is a zipped .zip file which contains a  .Rmd file to write answers in, and the data, all in a logical working directory.","tags":null,"title":"Problem Set 2","type":"docs"},{"authors":null,"categories":null,"content":"   Please submit on Blackboard Assignments by the end of the day Thursday October 7.   Please read the instructions for completing homeworks.\n PDF  R Project  R Studio Cloud The  PDF is useful if you want to print out the problem set. The  R project is a zipped .zip file which contains a  .Rmd file to write answers in, and the data, all in a logical working directory. (See this resource for help unzipping files). You can also just write an .R file in the project if you don’t want to use markdown. If you use the  cloud project, I have already installed tidyverse and tinytex (to produce pdfs).\nAnswers  Answers (html)  Answers (PDF)  Answers (R Project)  Concepts Question 1 In your own words, describe what exogeneity and endogeneity mean, and how they are related to bias in our regression. What things can we learn about the bias if we know \\(X\\) is endogenous?\n Question 2 In your own words, describe what \\(R^2\\) means. How do we calculate it, what does it tell us, and how do we interpret it?\n Question 3 In your own words, describe what the standard error of the regression (\\(SER\\)) means. How do we calculate it, what does it tell us, and how do we interpret it?\n Question 4 In your own words, describe what homoskedasticity and heteroskedasticity mean: both in ordinary English, and in terms of the graph of the OLS regression line.\n Question 5 In your own words, describe what the variation in \\(\\hat{\\beta_1}\\) (either variance or standard error) means, or is measuring. What three things determine the variation, and in what way?\n Question 6 In your own words, describe what a \\(p\\)-value means, and how it is used to establish statistical significance.\n  Theory and Applications Question 7 A researcher is interested in examining the impact of illegal music downloads on commercial music sales. The author collects data on commercial sales of the top 500 singles from 2017 (\\(Y\\)) and the number of downloads from a web site that allows `file sharing’ (\\(X\\)). The author estimates the following model\n\\[\\text{music sales}_i = \\beta_0+\\beta_1 \\text{illegal downloads}_i + u_i\\]\nThe author finds a large, positive, and statistically significant estimate of \\(\\hat{\\beta_1}\\). The author concludes these results demonstrate that illegal downloads actually boost music sales. Is this an unbiased estimate of the impact of illegal music on sales? Why or why not? Do you expect the estimate to overstate or understate the true relationship between illegal downloads and sales?\n Question 8 A researcher wants to estimate the relationship between average weekly earnings \\((AWE\\), measured in dollars) and \\(Age\\) (measured in years) using a simple OLS model. Using a random sample of college-educated full-time workers aged 25-65 yields the following:\n\\[\\widehat{AWE} = 696.70+9.60 \\, Age\\]\nPart A Interpret what \\(\\hat{\\beta_0}\\) means in this context.\n Part B Interpret what \\(\\hat{\\beta_1}\\) means in this context.\n Part C The \\(R^2=0.023\\) for this regression. What are the units of the \\(R^2\\), and what does this mean?\n Part D The \\(SER, \\, \\hat{\\sigma_u}=624.1\\) for this regression. What are the units of the SER in this context, and what does it mean? Is the SER large in the context of this regression?\n Part E Suppose Maria is 20 years old. What is her predicted \\(\\widehat{AWE}\\)?\n Part F Suppose the data shows her actual \\(AWE\\) is $430. What is her residual? Is this a relatively good or a bad prediction?1\n Part G What does the error term, \\(\\hat{u_i}\\) represent in this case? What might individuals have different values of \\(u_i\\)?\n Part H Do you think that \\(Age\\) is exogenous? Why or why not? Would we expect \\(\\hat{\\beta_1}\\) to be too large or too small?\n  Question 9 Suppose a researcher is interested in estimating a simple linear regression model:\n\\[Y_i=\\beta_0+\\beta_1X_i+u_i\\] In a sample of 48 observations, she generates the following descriptive statistics:\n \\(\\bar{X}=30\\) \\(\\bar{Y}=63\\) \\(\\displaystyle\\sum^n_{i=1}(X_i-\\bar{X})^2= 6900\\) \\(\\displaystyle\\sum^n_{i=1}(Y_i-\\bar{Y})^2= 29000\\) \\(\\displaystyle\\sum^n_{i=1}(X_i-\\bar{X})(Y_i-\\bar{Y})=13800\\) \\(\\displaystyle\\sum^n_{i=1}\\hat{u}^2=1656\\)  Part A What is the OLS estimate of \\(\\hat{\\beta_1}\\)?\n Part B What is the OLS estimate of \\(\\hat{\\beta_0}\\)?\n Part C Suppose the OLS estimate of \\(\\hat{\\beta_1}\\) has a standard error of \\(0.072\\). Could we probably reject a null hypothesis of \\(H_0: \\beta_1=0\\) at the 95% level?\n Part D Calculate the \\(R^2\\) for this model. How much variation in \\(Y\\) is explained by our model?\n Part E How large is the average residual?\n   R Problems Answer the following questions using R. When necessary, please write answers in the same document (knitted Rmd to html or pdf, typed .doc(x), or handwritten) as your answers to the above questions. Be sure to include (email or print an .R file, or show in your knitted markdown) your code and the outputs of your code with the rest of your answers.\nQuestion 10   mlbattend.csv  Download the MLBattend dataset. This data contains data on attendance at major league baseball games for all 32 MLB teams from the 1970s-2000. We want to answer the following question:\n “How big is home-field advantage in baseball? Does a team with higher attendance at home games over their season have score more runs over their season?”\n Part A Clean up the data a bit by mutate()-ing a variable to measure home attendance in millions. This will make it easier to interpret your regression later on.\n Part B Get the correlation between Runs Scored and Home Attendance.\n Part C Plot a scatterplot of Runs Scored (y) on Home Attendance (x). Add a regression line.\n Part D We want to estimate a regression of Runs Scored on Home Attendance:\n\\[ \\widehat{\\text{runs scored}_i} = \\beta_0 + \\beta_1 \\text{home attendance}_i \\] Run this regression in R.\nWhat are \\(\\hat{\\beta_0}\\) and \\(\\hat{\\beta_1}\\) for this model? Interpret them in the context of our question. [Hint: make sure to save your regression model as an object, and get a summary() of it. This object will be needed later.]\n Part E Write out the estimated regression equation.\n Part F Make a regression table of the output (using the huxtable package).\n Part G Check the goodness of fit statistics. What is the \\(R^2\\) and the SER of this model? Interpret them both in the context of our question.\n Part H Now let’s start running some diagnostics of the regression. Make a histogram of the residuals. Do they look roughly normal? [Hint: you will need to use the broom package’s augment() command on your saved regression object to add containing the residuals (.resid), and save this as a new object - to be your data source for the plot in this question and the next question.]\n Part I Make a residual plot.\n Part J Test the regression for heteroskedasticity. Are the errors homoskedastic or heteroskedastic?\n[Hint: use the lmtest package’s bptest() command on your saved regression object.]\nRun another regression using robust standard errors. [Hint: use the estimatr package’s lm_robust() command and save the output like the following:\nreg_robust \u0026lt;-lm_robust(y ~ x, data = the_data, # change y, x, and data names to yours se_type = \u0026quot;stata\u0026quot;) # we\u0026#39;ll use this method to calculate Now make another regression output table with huxtable, with one column using regular standard errors (just use your original saved regression object) and another using robust standard errors (use this new saved object).\n Part K Test the data for outliers. If there are any, identify which team(s) and season(s) are outliers. [Hint: use the car package’s outlierTest() command on your saved regression object.]\n Part L Look back at your regression results. What is the marginal effect of home attendance on runs scored? Is this statistically significant? Why or why not?\n Part M Now we’ll try out the infer package to understand the \\(p\\)-value for our observed slope in our regression model.\nFirst, save the (value of) our sample \\(\\hat{\\beta_1}\\) from your regression in Part D as an object, I suggest:\nour_slope = 123 # replace \u0026quot;123\u0026quot; with whatever number you found for the slope in part D Then, install and load the infer package, and then run the following simulation:\n# save our simulations as an object (I called it \u0026quot;sims\u0026quot;) sims \u0026lt;- data %\u0026gt;% # \u0026quot;data\u0026quot; here is whatever you named your dataframe! specify(y ~ x) %\u0026gt;% # replacing y and x with your variable names hypothesize(null = \u0026quot;independence\u0026quot;) %\u0026gt;% # H_0 is that slope is 0, x and y are independent generate(reps = 1000, type = \u0026quot;permute\u0026quot;) %\u0026gt;% # make 1000 samples assuming H_0 is true calculate(stat = \u0026quot;slope\u0026quot;) # estimate slope in each sample # look at it sims # calculate p value sims %\u0026gt;% get_p_value(obs_stat = our_slope, direction = \u0026quot;both\u0026quot;) # a two-sided H_a: slope =/= 0 Compare to the \\(p\\)-value in your original regression output in previous parts of this question.\n Part N Make a histogram of the simulated slopes, and plot our sample slope on that histogram, shading the \\(p\\)-value.\n[You can pipe sims into visualize(obs_stat = our_slope), or use ggplot2 to plot a histogram in the normal way, using sims as the data source and add a geom_vline(xintercept = our_slope) to show our finding on the distribution.]\n    Hint: compare your answer here to your answer in Part D.↩︎\n   ","date":1591574400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1633817446,"objectID":"9361b03d8c56a951bf206e9a0ac596fe","permalink":"https://metricsf21.classes.ryansafner.com/assignments/03-problem-set/","publishdate":"2020-06-08T00:00:00Z","relpermalink":"/assignments/03-problem-set/","section":"assignments","summary":"Please submit on Blackboard Assignments by the end of the day Thursday October 7.   Please read the instructions for completing homeworks.\n PDF  R Project  R Studio Cloud The  PDF is useful if you want to print out the problem set. The  R project is a zipped .zip file which contains a  .Rmd file to write answers in, and the data, all in a logical working directory.","tags":null,"title":"Problem Set 3","type":"docs"},{"authors":null,"categories":null,"content":"   Overview Length, References, \u0026amp; Mechanics Sources for Inspiration Data Sources Grading Rubric and Deadlines Emailing Your Final Version to Me    Due by DATE and TIME    “How can I know what I think until I see what I say?”\n— E. M. Forster\n  “No economist has achieved scientific success as a result of a statistically significant coefficient. Massed observations, clever common sense, elegant theorems, new policies, sagacious economic reasoning, historical perspective, relevant accounting, these have all led to scientific success. Statistical significance has not.” — McCloskey and Ziliak, “The Cult of Statistical Significance” (1996: 112)\n  Writing and Reading Empirical Papers Slides](/slides/paper-slides.html) - [ Example Paper Project](https://github.com/ryansafner/example_empirical_paper)-- Overview Your task is to write a research paper on any topic in political economy of your choosing so long as it has an empirical component. The purpose of this paper is threefold: (1) to demonstrate to me that you have mastered the material of this course, (2) to develop your writing, communication, and data analysis skills, and (3) to help you develop, strengthen, and/or modify your own views by grappling with them in writing. As a reminder, this paper constitutes 25% of your final course grade.\nI will spend a significant portion of one class discussing more about the writing process, and provide a detailed guide to help you choose a topic, craft arguments, and write a good paper.\nI am NOT looking for a survey of existing research, a series of block quotes showing what some economist said about X, a regurgitation of my lectures, a list of pros and cons with a last minute conclusion, a book review, and so on. I also do NOT want you to compress everything about econometrics you learned from this class into a single paper. You should only use those insights that are relevant to your topic and your argument (it may only be one or two things!).\nI am looking for a paper that attempts to answer a specific research question of interest to economists by using data analysis. You should be able to summarize your paper in one or two sentences – the specific research question your paper addresses, your method for answering it, and your results. It should be a reasonably original paper (it is difficult with limited knowledge, time, and data to offer something truly original), and it should be your own take on the topic. I expect you to, at the very least, have one multivariate regression used to reach your conclusions. I do not expect or require you to find statistically significant results.\nPlease note that, while you are not required to, I highly recommend discussing your ideas with me over email or in person. I will also read any drafts you would like to submit to me early, and provide you with helpful comments, subject to my own availability. I will stop accepting drafts to provide comments by November 30.\n Length, References, \u0026amp; Mechanics I hesitate to give formal length requirements, because most students will write the bare minimum, and also because different papers have different optimal lengths. What truly matters is that your paper is long enough to say what you need to say, to say it well, and to say it briefly. Ceterus paribus, if papers \\(A\\) and \\(B\\) say the same thing, but paper \\(B\\) says it in half the length (without sacrificing key arguments), paper \\(B\\) is a better paper. You will also find that simply by including the necessary components of an empirical paper (plots, tables, etc.), your page count will by necessity increase on its own.\nSince you of course still want to know what a good length will be, my rule of thumb is that your paper should be about \\(\\mathbf{10\\pm 5}\\) pages (size 12 font, double spaced, 1\" margins). This will depend on the topic you have chosen to written on, your data, and your own personal writing style.\nI would also like you to use scholarly references, that is, articles from economics journals and cite them properly. I do not have a minimum requirement of the amount of references, but I expect you to have at least 2-5 scholarly references, depending on your paper topic and thesis. I am not particularly picky about exactly how you format your citations or bibliography, just please be consistent, and do not use footnotes or endnotes (only because they annoy me). I suggest the APA author-year-page in-text citation format that is fairly standard in economics journals, i.e.: “The division of labor is limited by the extent of the market,” (Smith 1776: 27). Look at my slides or my handouts for a suggested bibliography style. If you use .bib files, the default formatting is fine.\n Sources for Inspiration Here is a list of a few places you might consider to dig up some information on your topic, or to help you find a topic. Just be sure to read and cite the actual sources that these secondary sources cite.\n Major news outlets (e.g. CNN, Wall Street Journal, New York Times, USA Today, Huffington Post, the Guardian) and wherever you may find your news (e.g. reddit, Buzzfeed, etc) both for current events, and also their Opinion/Commentary sections for other op-eds to learn from and/or critique Wikipedia – no seriously, it is the first place I go to learn about a new topic. Just be sure to actually investigate the underlying research and use that for your references! Economics Podcasts  Econtalk – a fantastic podcast series by Russ Roberts that interviews famous economists, philosophers, businesspeople, and other figures who have an impact on the world of ideas Freakonomics (Radio) – another great podcast by one of the authors of the famed Freakonomics books about intermediate-level economic topics, often an in-depth series of interviews on a major issue NPR Planet Money – another good podcast series on economics topics and current events, much shorter and more 10,000-foot level approach than Econtalk or Freakonomics  Popular Economists’ Blogs/Blogs on Economics:  Marginal Revolution – Tyler Cowen \u0026amp; Alex Tabarrok (head and shoulders above the rest!) Cafe Hayek – Don Boudreaux (libertarian-leaning, mostly just Don on trade and micro-policy) EconLog – Bryan Caplan, David Henderson (libertarian-leaning, good analysis) The Conscience of a Liberal – Paul Krugman at New York Times (strong left-wing politics) The Grumpy Economist – John Cochrane (Chicago School approach) Greg Mankiw’s Blog – Greg Mankiw (moderate conservative, New Keynesian approach) Undercover Economist – Tim Harford (British, non-political, very easy to understand) Chris Blattman – Chris Blattman (great on economic development, poverty, and conflict in poor countries) Slate Star Codex – “Scott Alexander” (a pseudonym, apparently a Medical Student, but one of the most lucid social science blogs ever written) Fivethirtyeight – Nate Silver \u0026amp; co. (a journalist, but a leader on using data and statistics in social science and journalism) Andrew Gelman – Andrew Gelman (a statistician, but another leader on using data and statistics for social science)    Data Sources While it is one thing to find a topic to write on, it is an altogether different animal to find data to use to test empirical research questions. You will find out quickly that the constraint to writing an empirical paper is not the set of topics or questions to write on (though that is often a challenge itself!), but the data available to use.\nDepending on the topic, you can also collect your own data, and many times you will want to create a custom dataset by simply combining data from different sources.\nDon’t forget to check out the Data Resources for ideas and examples of how and where to find data sets.\n Grading Rubric and Deadlines While it may be possible for many papers in your college career, is not a paper you can write the last minute and do well on. To ensure that you do not get too far behind, I have split the assignment into stages that are due at different intervals over the semester. Note that your topic can and may change depending on what you are able to find and work with. The hardest part is finding data that allows you to test a research question. It is primarily for this reason that writing an empirical paper on what you want is very difficult.\n    Assignment Points Due Date Description    Abstract 5 Fri Oct 22 Short summary of your ideas  Literature Review 10 Fri Nov 5 1-3 paragraphs on 2-3 scholarly sources  Data Description 10 Fri Nov 19 Description of data sources, and some summary statistics  Presentation 5 Tues/Thurs Nov 30/Dec 2 Short presentation of your project so far  Final Paper Due 70 Mon Dec 6 Email to me paper, data, and code    All assignments are due as emails to me.\n Abstract: write a short paragraph (3-6 sentences) summarizing: what rough topic you want to look at, a specific research question that you think you can get data to test, where you think you might be able to get some data. It’s okay that much of this is speculative and you might change your mind or do very different things later!\n Data Description: describe what data you managed to find (a few paragraphs): where is/are the dataset(s) from? What variables are included, and how are they measured (e.g. what units, categories, etc)? Give us some summary statistics of the data (a table would be nice, some scatterplots and histograms would be nice), are there any interesting patterns?\n Literature Review: find 2-3 scholarly sources (ideally scholarly journal articles) that discuss your research question (or related topics), especially if they have empirical findings. Explain what these sources found, how they found it, and how your paper relates to this literature. Don’t worry about being original!\n Presentation: a 5-10 minute presentation of what you have so far. Different people will be at different stages, since your paper is not due yet! Use this as an opportunity to get feedback from me and your classmates, that might help you finish your project. Using some slides to show tables, models, and plots is highly recommended.  Example](/papers/Presentation_example.pdf) --\n Final Paper Due: email the full paper to me (see below for what should be in the email).\n  The remaining 70% for the final product are broken down as follows:\n  Category Points    Persuasiveness 10  Clarity 10  Econometric Validity 20  Economic Soundness 20  Organization 5  References 5  TOTAL 70     Persuasiveness: How persuasive is your argument? Would a reasonably educated college-level reader who is familiar with economics and statistics but not necessarily this course find themselves understanding your argument and agreeing with you? [Write for an audience wider than just members of this class. Therefore, don’t use terms, sources, or “inside jokes” that only other students in this class (and no one else) would understand.] Remember, your goal is not to convince me (though you may), your goal is to convince any educated reader, and I grade you the probability that this is likely. You are the lawyer, I am the judge, and your audience is the jury. Clarity How clear is your paper? Is it clear what your research question is, how you answer it, and what your results are? Can you summarize these in a sentence or two? Are there confusing passages, excessive jargon or passive voice, or irrelevant arguments and examples? Econometric Validity: Is your econometric model sensible and plausible? Do you adequately address the assumptions and limitations of your model? Do you use appropriate data? Do you adequately describe your data, identify patterns, aberrations, and clearly generate testable hypotheses from your data? Would someone else, given your data, be able to replicate your findings? Economic Soundness: Do you place your empirical question in a broader context of applying economic principles? Do you describe relevant economic policies, institutions, relationships, and/or other relevant economic principles to the questions you are asking? Are your theories and hypotheses about relationships between data plausible and intuitive? Do you makes sense of your hypotheses and the results of your analysis and connect them to sound economic principles? Is your question, strategy, and/or results of interest to economists? You will lose up to 20 points for papers that have no economic content. Organization: Is your paper organized? Have you presented your separate arguments/examples in a logical order? Is it clear when you are moving on from one section to another? Is it clear when and where you are summarizing and concluding? Is your paper presented according to professional norms (e.g. summary statistics tables, regression tables, etc)? References: Does your paper use multiple scholarly references? Does it properly cite them in the text for main ideas borrowed and for direct quotations (if applicable)? Are they consistently listed at the end in a references section? Style: Is your paper interesting and easy to read? Does it engage the reader? Is it written in active voice? This is somewhat subjective, and hence, the smallest portion of your grade.   Emailing Your Final Version to Me When you send your final email (by Tuesday November 22), it should contain the following files:\nYour final paper as a .pdf. It should include an abstract and bibliography and all tables and figures contained within it. The (commented!) code used for your data analysis (i.e. loading data, making tables, making plots, running regressions). These can be either .R files: one or multiple (one-per-task) are equally fine OR a .Rmd file. I want to know how you reached the results you got! Reproducibility is the goal! Your data used, in whatever original format you found it (e.g. .csv, .xlsx, .dta)  Again, you are not obligated to use R Markdown to write your paper. Microsoft Word is fine.\n ","date":1591574400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1634399402,"objectID":"9c3115b8477764224b64eaf4850ac2f6","permalink":"https://metricsf21.classes.ryansafner.com/assignments/research-paper/","publishdate":"2020-06-08T00:00:00Z","relpermalink":"/assignments/research-paper/","section":"assignments","summary":"Overview Length, References, \u0026amp; Mechanics Sources for Inspiration Data Sources Grading Rubric and Deadlines Emailing Your Final Version to Me    Due by DATE and TIME    “How can I know what I think until I see what I say?”\n— E. M. Forster\n  “No economist has achieved scientific success as a result of a statistically significant coefficient. Massed observations, clever common sense, elegant theorems, new policies, sagacious economic reasoning, historical perspective, relevant accounting, these have all led to scientific success.","tags":null,"title":"Research Paper Project","type":"docs"},{"authors":null,"categories":null,"content":"Nobody produces something like this without a lot of inspiration and guidance from those who have already done this before. Here, I try to give credit where it is due, and slowly begin to document how I made everything, for those who eventually want to do what I did.\nThis website is written in R Markdown, built with Hugo, pushed to Github, and hosted by Netlify.\nThe theme is based on Academia-hugo, which I modified.\nThe inspiration for making individual course websites, along with the basic structure, comes from the incomparable Andrew Heiss. See his website for examples, as well as a number of helpful tutorials. Sadly, I had to figure out how these websites work on my own, and attempted to document my process in the README of this repository for those interested. One day I may make a guide.\nThe course hex sticker I made via hexSticker, and keep all of my course hex stickers in this repository.\n","date":1530140400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1627237460,"objectID":"860be0daf7a4a4c1bc9ae66a88da9a2f","permalink":"https://metricsf21.classes.ryansafner.com/credits/","publishdate":"2018-06-28T00:00:00+01:00","relpermalink":"/credits/","section":"","summary":"Nobody produces something like this without a lot of inspiration and guidance from those who have already done this before. Here, I try to give credit where it is due, and slowly begin to document how I made everything, for those who eventually want to do what I did.\nThis website is written in R Markdown, built with Hugo, pushed to Github, and hosted by Netlify.\nThe theme is based on Academia-hugo, which I modified.","tags":null,"title":"Credits","type":"page"},{"authors":null,"categories":null,"content":"  Overview There will be several problem sets (typically 5-6) spaced throughout the semester. Typically, after we cover a major concept or set of tools, I will assign a problem set to practice this material.\nProblem sets will be a combination of math/statistical theory \u0026amp; application problems, as well as problems that require the use of R with real data.\nProblem sets are typically due one week from the class period it is assigned (although the due date announced on the problem set is the final authority on this), and must handed in or emailed to me by the start of class (so please type or, if you must, hand write and scan them).\n Instructions Due to the combination of traditional and R problems, there are several ways you can complete and turn each assignment:\nType up any applicable answers (saving any plots as images and including them) in a (e.g. Word) document and save it as a PDF and turn in a (commented!) .R file of commands for each relevant question.\n If you wish to write out answers by hand, you may either print the  pdf above or write your answers (all I need is your work and answers) on your own paper and then please scan/photograph \u0026amp; convert them to a single PDF, if they are easily readable, but this is not preferred. See my guide to making a PDF\n Download the  .Rmd file, do the homework in markdown, and email to me a single knitted html or pdf file. Be sure that it shows all of your code (i.e. all chunks have echo = TRUE options), otherwise I will also ask for the markdown file.\n  To minimize confusion, I suggest creating a new R Project (e.g. hw1) and storing any data and plots in that folder on your computer. See my example workflow.\nYou may work together (and I highly encourage that) but you must turn in your own answers. I grade homeworks 70% for completion, and for the remaining 30%, pick one question to grade for accuracy - so it is best that you try every problem, even if you are unsure how to complete it accurately.\n Grading I grade homeworks 70% for completion, and for the remaining 30%, pick one question to grade for accuracy — so it is best that you try every problem, even if you are unsure how to complete it accurately.\n ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1627070183,"objectID":"b2319fdfa56d87457394d988ddf4b00e","permalink":"https://metricsf21.classes.ryansafner.com/assignments/problem-sets/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/assignments/problem-sets/","section":"assignments","summary":"Overview There will be several problem sets (typically 5-6) spaced throughout the semester. Typically, after we cover a major concept or set of tools, I will assign a problem set to practice this material.\nProblem sets will be a combination of math/statistical theory \u0026amp; application problems, as well as problems that require the use of R with real data.\nProblem sets are typically due one week from the class period it is assigned (although the due date announced on the problem set is the final authority on this), and must handed in or emailed to me by the start of class (so please type or, if you must, hand write and scan them).","tags":null,"title":"Instructions","type":"docs"},{"authors":null,"categories":null,"content":"  This page contains all of the following resources for each class meeting:\n  Content materials contains suggested readings, more details about assignments, math appendices, and other helpful resources. I suggest you view these before each class.    Slides are “Xaringan” presentations in html that can be opened in any browser (You can find a downloadable PDF in each respective content page)    R materials contain extra tutorials, videos, practice exercises for using R    Assignments are generally 11:59 PM Sundays on Blackboard   Please note that the lesson numbers, topics, and titles (e.g. 1.1) are my design, and do not match up with the textbook!\nRelevant materials, if applicable will be posted before class meets and become colored links.\n  I. Data Analysis in R Content Slides R Assignment   Preliminary Survey       1.1 — Introduction to Econometrics       1.2 — Meet R       1.3 — Data Visualization with ggplot2       1.4 — Data Wrangling in the tidyverse       1.5 — Optimize Workflow: Markdown, Projects, and Git        Problem Set 1 due Tues Sep 14       II. Linear Regression and Statistical Inference Content Slides R Assignment   2.1 — Data 101 and Descriptive Statistics       2.2 — Random Variables and Distributions        Problem Set 2 due Tues Sep 21       2.3 — OLS Linear Regression       2.4 — OLS: Goodness of Fit and Bias       2.5 — OLS: Precision and Diagnostics       2.6 — Statistical Inference       2.7 — Inference for Regression        Problem Set 3 due Thurs Oct 7        Midterm Exam Thurs Oct 14       III. Causal Inference Content Slides R Assignment   3.1 — The Fundamental Problem of Causal Inference \u0026amp; Potential Outcomes       3.2 — Causal Inference \u0026amp; DAGs       3.3 — Omitted Variable Bias       3.4 — Multivariate OLS Estimators: Bias, Precision, and Fit        Problem Set 4       3.5 — Writing an Empirical Paper       3.6 — Regression with Categorical Data       3.7 — Regression with Interaction Effects       3.8 — Polynomial Regression       3.9 — Logarithmic Regression        Problem Set 5       IV. Panel Data \u0026amp; Advanced Models Content Slides R Assignment   4.1 — Panel Data and Fixed Effects Models       4.2 — Difference-in-Difference Models        Problem Set 6       Empirical Research Paper Project       Final Exam       4.3 — Instrumental Variables Models       4.4 — Regression Discontinuity Models       4.5 — Binary Dependent Variables Models       4.6 — Prediction, Classification, \u0026amp; Machine Learning        ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1629657987,"objectID":"3e223d7ba58b0122b42458e4cf52e04c","permalink":"https://metricsf21.classes.ryansafner.com/schedule/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/schedule/","section":"","summary":"This page contains all of the following resources for each class meeting:\n  Content materials contains suggested readings, more details about assignments, math appendices, and other helpful resources. I suggest you view these before each class.    Slides are “Xaringan” presentations in html that can be opened in any browser (You can find a downloadable PDF in each respective content page)    R materials contain extra tutorials, videos, practice exercises for using R    Assignments are generally 11:59 PM Sundays on Blackboard   Please note that the lesson numbers, topics, and titles (e.","tags":null,"title":"Schedule","type":"page"},{"authors":null,"categories":null,"content":"    Course Format (and Covid)  Learning During a Global Pandemic  Course objectives Required Course materials  Books Articles Software  Assignments and Grades Policies and Expectations  Attendence Late Assignments Grading Communication: Email, Slack, and Virtual Office Hours Netiquette Privacy Enrollment Honor Code Accessibility, Equity, and Accommodations  Tentative Schedule   Instructor  Dr. Ryan Safner  Office: Rosenstock 218-B  Office Hours: MW 1:30 PM—2:30 PM \u0026amp; by appt  safner@hood.edu  @ryansafner   Course details  TuTh 11:25 AM—12:50 PM  Rosenstock Trading Room  Aug 23—Dec 14, 2021  Slack     Download PDFLast Updated: August 20, 2021  “There are three kinds of lies: lies, damned lies, and statistics.” — Benjamin Disraeli\n Econometrics is the application of statistical tools to quantify and measure economic relationships in the real world. It uses real data to test economic hypotheses, quantitatively estimate causal relationships between economic variables, and to make forecasts of future events. The primary tool that economists use for empirical analysis is ordinary least squares (OLS) linear regression, so the majority of this course will focus on understanding, applying, and extending OLS regressions.\nI assume you have some working knowledge of economics at the intermediate level and some basic statistical tools.The formal prerequisites for this course are ECON 205 and ECON 206; ECMG 212 or MATH 112; and ECON 305 or ECON 306. We will do some basic review of some necessary statistics and probability at the beginning until everyone is comfortable, before jumping right into regressions.\nCourse Format (and Covid) As of Fall 2021, all students are expected to be on campus except those with special approved exemptions. As such, this course will be taught in-person and synchronously until or unless otherwise announced.\nYou are expected to come to class except due to medical reasons or other legitimate conflicts. Watching videos are not a substitute for attending class.1 Please see the attendance policy for more.\nIn any event that we are unable to meet in person, I will hold class meetings at the same day/time live on Zoom, and post all recorded lectures via Panopto on Blackboard, and all assignments will be submitted online (often via Blackboard).\nLearning During a Global Pandemic While we have made some progress in returning to normal, this remains a unique semester and a lot of things are still awful right now. None of us signed up for this. None of us are really okay, we’re all just pretending for everyone else.\nMany of you may be dealing with hardships at home and at work, and are generally juggling many more problems than usual. Everyone’s future plans have been completely put on hold or cancelled to a large degree.\nI am prioritizing us supporting each other as human beings during this crazy era, and will try to use simple, accessible solutions that make sense for the most people, and above all, to be flexible.\nIf you tell me you’re having trouble, I will do whatever I can to help, and not judge you or think less of you. I hope you will extend me the same courtesy.\nYou never owe me personal information about your health (mental or physical). You are however always welcome to talk to me about things that you’re going through. If I can’t help you, I usually know somebody who can.\nI want you to learn a lot from this course, but it is more important for you to remain healthy, balanced, and grounded during this crisis.\nI reserve the right to change any part of this syllabus and course, at my discretion, with proper advance warning.\n  Course objectives By the end of this course, you will:\nunderstand how to evaluate statistical and empirical claims; use the fundamental models of causal inference and research design; gather, analyze, and communicate with real data in R.  I am less concerned with forcing you to memorize and recite proofs of statistical estimator properties, and more concerned with the development of your intuitions and the ability to think critically as an empirical social scientist—although this will require you to demonstrate proficiency with some intermediate statistical and mathematical tools.\nGiven these objectives, this course fulfills all three of the learning outcomes for the George B. Delaplaine, Jr. School of Business Economics B.A. program:\n Use quantitative tools and techniques in the preparation, interpretation, analysis and presentation of data and information for problem solving and decision making […] Apply economic reasoning and models to understand and analyze problems of public policy […] Demonstrate effective oral and written communications skills for personal and professional success[…]  Fair warning: Econometrics is hard. It will be one of the hardest economics courses that you will take, primarily due to the mathematical content. I will do my best to make this class intuitive and helpful, if not interesting. If at any point you find yourself struggling in this course for any reason, please come see me. Do not suffer in silence! Coming to see me for help does not diminish my view of you, in fact I will hold you in higher regard for understanding your own needs and taking charge of your own learning. There are also a some fantastic resources on campus, such as the Center for Academic Achievement and Retention (CAAR) and the Beneficial-Hodson Library.\nSee my tips for success in this course.\n Required Course materials This course requires regular online internet access. If you know you will be unable to access the internet regularly, please let me know and we can make arrangements.\nYou can find all course materials at my dedicated website for this course: metricsF21.classes.ryansafner.com. Links to the website are posted on our Blackboard course page. Please familiarize yourself with the website, see that it contains this syllabus, resources to help you, and our schedule. On the schedule page, you can find each module with its own class page (start there!) along with all related readings, lecture slides, practice problems, and assignments.\nMy lecture slides will be shared with you, and serve as your primary resource, but our main “textbook” below is recommended as the next best resource and will be available from the campus bookstore. I will discuss more about textbooks and materials in the first module.\nBooks The following book is required and will be available from the campus bookstore. (You are not obligated to buy it, I just strongly recommend it in the sense that you will still have access to all data and assignments without possessing the book. But this is a course where you really will want to understand the derivations or get additional context beyond just my slides…)\n Bailey, Michael A, 2019, Real Econometrics, New York: Oxford University Press, 2nd ed.  You are welcome to purchase the book by other means (e.g. Amazon, half.com, etc). I have no financial stake in requiring you to purchase this book. The (cheaper) 1st edition is sufficient, but makes significantly less use of R (in favor of STATA).\nThe following two books are recommended, and are free online. (You can purchase a hard copy of the first one if you really want.):\n Grolemund, Garrett and Hadley Wickham, R For Data Science Ismay, Chester and Albert Y Kim, Modern Dive: Statistical Inference Via Data Science  The first book is the number one resource for using R and tidyverse, and is written for beginners. I still look at it frequently. The second is another great reference for using tidyverse in the context of basic statistics.\n Articles Throughout the course, I will post both required and supplemental (non-required) readings that enrich your understanding for each topic.\n Software You are strongly recommended to download copies of R and R Studio on your own computers. These software packages are available on all computers in the trading room, and you will have access to them during the week to work on assignments.\nWe will also have a shared class workspace in RStudio.cloud that runs a full instance of R Studio in your web browser (so no need to install anything!) will let you access files and assignments.\n  Assignments and Grades Your final course grade is the weighted average of the following assignments. You can find general descriptions for all the assignments on the assignments page and more specific information (such as due dates) and examples on each assignment’s page on the schedule page.\n   Frequency  Assignment  Weight      1  Research Project  30%    n  Problem sets (Average)  25%    1  Midterm  20%    1  Final  25%     Each assignment is graded on a 100 point scale. Letter-grade equivalents are based on the following scale:\n  Grade  Range  Grade  Range      A  93–100%  C  73–76%    A−  90–92%  C−  70–72%    B+  87–89%  D+  67–69%    B  83–86%  D  63–66%    B−  80–82%  D−  60–62%    C+  77–79%  F  \u0026lt; 60%     See also my  Grade Calculator app where you can calculate your overall grade using existing assignment grades and forecast “what if” scenarios.\nThese grades are firm cutoffs, but I do of course round upwards \\((\\geq\\) 0.5) for final grades. A necessary reminder, as an academic, I am not in the business of giving out grades, I merely report the grade that you earn. I will not alter your grade unless you provide a reasonable argument that I am in error (which does happen from time to time).\nSave As - PDF, or File - Export - PDF.] to **upload on Blackboard** under Assignments. You may handwrite answers if you will be able to scan/photograph \u0026 convert them **to a single PDF**, if they are easily readable, but this is *not preferred*. See my [guide to making a PDF](https://gameF21.classes.ryansafner.com/resources/#how-to-make-a-pdf-for-submitting-assignments) - an essential skill in the modern world. If you are handwriting answers, you may print the `pdf` above and write on it, or just write on a piece of paper (we only need your answers). For the few questions that ask you to draw a **graph**, *try* to do so *on your computer* (use MS Paint, the drawing tools in MS Word or MS Powerpoint, plot points in MS Excel, drawing/notetaking apps, etc.), and save it as an image to include on your homework document. Again, they need not be perfect or to scale, just show that you understand the broad idea. Being able to understand and sketch the graphs is still a very important and useful skill! If all else fails, I will be lenient in grading graph questions if you are unable to technologically include a graph. Your TA, under my supervision, will grade homeworks 70% for completion, and for the remaining 30%, and one question will be graded for accuracy - so it is best that you try every problem, even if you are unsure how to complete it accurately. ## Exams There will be three exams (one at the end of each unit) including the final exam. Each exam only covers the material in its associated unit (i.e. no exam is cumulative). These exams provide feedback both to you and to me that ensures everyone is progressing on schedule and apprehending the material. This is critical, as the rest of the course, and indeed, any future economics course you take, will build off of this foundation. Exams will be released **on Blackboard** as a timed assignment. You will have 2 hours once you open the exam on Blackboard. You will not need the whole time, I have given you some extra to accommodate the difficulties of taking an exam at home. Please pick a time to take it where you know you will have 2 hours. You may close the exam page and come back to it, but the timer will continue to run once the exam is first opened. ## Opinion-Editorial Economic fallacies have always been popular, and journalists, politicians, and talking heads consistently engage in faulty economic reasoning in print and on television. Your task, as a student of economics, is to find some issue discussed in the past year, and write a critique of media discussion of that issue. Alternatively, instead of critiquing the reasoning or statements of others, you may write an advocacy piece, where you propose some economic policy and argue to persuade readers to endorse it. You will be graded both on the soundness of your economic reasoning and the quality of your writing. I would be happy to co-author an Op-Ed with anyone who writes an exemplary Op-Ed. Students who successfully publish their work in a media outlet will earn *extra credit* on their Op-Ed grade. I will provide ample resources and examples to help you accomplish this, you can find more on the associated [assignment page](assignment/op-ed). -- No extra credit is available\n Policies and Expectations This syllabus is a contract between you, the student, and me, your instructor. It has been carefully and deliberately thought out. (A syllabus can and will be used as a legal document for disputes tried at a court of law. Ask me how I know.), and I will uphold my end of the agreement and expect you to uphold yours.\nIn the language of game theory, this syllabus is my commitment device. I am a very understanding person, and I know that exceptions to rules often need to be made for students. However, to be fair to all students the syllabus artificially constrains my ability to make exceptions at a whim for anyone. This prevents clever students from exploiting my congenial personality at everyone else’s expense. Please read and familiarize yourself with the course policies and expectations of you. Chances are, if you have a question, it is answered herein.\nAttendence Your day-to-day classroom attendance is not graded. My philosophy is that you are all adults and must take ownership of your own learning or else you will not succeed. Some assignments may require in-class participation for credit, and an (unexcused) absence may be detrimental to your grade. Attending class is one of the strongest predictors of success.\nHowever, as required under Hood College’s “Promise of Fall Plan,” (Ch. 2-C) your classroom attendance will be recorded at every class meeting. This is primarily to facilitate contact tracing.\nIf you know you will be absent, you are not required to let me know, but it is polite to give notice (Note if I do not reply to an email of yours letting me know, I am probably busy but will still see it and appreciate your email). Your absence will be noted and recorded for the purposes stated above. If, however, we have an assignment due in class, you must notify me ahead of time in order to make alternate arrangements to still receive credit. Hasty ex-post attempts to notify me will generate little sympathy.\n Late Assignments I will accept late assignments, but will subtract a specified amount of points as a penalty. Even if it is the last week of the semester, I encourage you to turn in late work: some points are better than no points!\nHomeworks: If you turn in a homework after it is due but before it is graded or the answer key posted, I generally will not take off any points. However, if you turn in a homework after the answer key is posted, I will automatically deduct 15 points (so the maximum grade you can earn on it is an 80).\nExams: If you know that you will be unable to complete an in-class exam as scheduled for a legitimate reason, please notify me at least one week in advance, and we will schedule a make-up exam date. Failure to do so, including desperate attempts to make arrangements only after the exam will result in a grade of 0 and little sympathy.\nPapers: Starting at the deadline, I will take off 1 point for every hour that your assignment is late.\nI reserve the right to re-weight assignments for students whom I believe are legitimately unable to complete a particular assignment.\n Grading I will try my best to post grades on Blackboard’s Grading Center and return graded assignments to you within about one week of you turning them in. There will be exceptions. Where applicable, I will post answer keys once I know most homeworks are turned in (see Late Assignments above for penalties). Blackboard’s Grading Center is the place to look for your most up-to-date grades. See also my  Grade Calculator app where you can calculate your overall grade using existing assignment grades and forecast “what if” scenarios.\n Communication: Email, Slack, and Virtual Office Hours Students must regularly monitor their Hood email accounts to receive important college information, including messages related to this class. Email through the Blackboard system is my main method of communicating announcements and deadlines regarding your assignments. Please do not reply to any automated Blackboard emails - I may not recieve it!. My Hood email (safner@hood.edu) is the best means of contacting me. I will do my best to respond within 24 hours. If I do not reply within 48 hours, do not take it personally, and feel free to send a follow up email in the very likely event that I genuinely did not see your original message.\nOur slack channel is available to all students and faculty in Economics and Business. I have invited all of my classes and advisees. It will not be extended to non-Business/Economics students or faculty. All users must use their hood emails and true first and last names. Each course has its own channel, exclusive for verified students in the course, and myself, by my invite only. As a third party platform, you agree to its Terms of Service. I have created this space as a way to stay connected, to help one another, and to foster community. Behaviors such as posting inappropriate content, harassing others, or engaging in academic dishonesty, to be determined solely at my discretion, will result in one warning, the content will be deleted, and subsequent behavior will result in a ban.\nIn addition to in-person office hours, you can also make an appointment for “office hours” on Zoom. You can join in with video, audio, and/or chat, whichever you feel comfortable with. Of course, if you are not available during those times, we can schedule our own time if you prefer this method over email or Slack. If you want to go over material from class, please have specific questions you want help with. I am not in the business of giving private lectures (particularly if you missed class without a valid excuse).\nWatch this excellent and accurate video explaining office hours:\n  Netiquette When using Zoom and Slack, please follow appropriate internet etiquette (“Netiquette”). Written communications, like blog posts or use of the Zoom chat, lacks important nonverbal cues (such as body language, tone of voice, sarcasm, etc).\nAbove all else, please respect one another and think/reread carefully about how others may see your post before you submit a comment. You are expected to disagree and have different opinions, this is inherently valuable in a discussion. Please be civil and constructive in responding to others’ comments: writing “have you considered ‘X?’” is a lot more helpful to all involved than just writing “well you’re just wrong.”\nPosting content that is wilfully incindiary, illegal, or that constitutes academic dishonesty (such as plagarism) will automatically earn a grade of 0 and may be elevated to other authorities on campus.\nWhen using the chat function on Zoom or public Slack channels, please treat it as official course communications, even though I may not be grading it. It may be a quick and informal tool - don’t feel you need to worry about spelling or perfect grammar - but please try to avoid too informal “text-speak” (i.e. say “That’s good for you” instead of “thas good 4 u”).\n Privacy Maryland law requires all parties consent for a conversation or meeting to be recorded. If you join in, and certainly if you participate, you are consenting to be recorded. However, as described below, videos are not accessible beyond our class.\nLive lectures are recorded on Zoom and posted to Blackboard via Panopto, a secure course management system for video. Among other nice features (such as multiple video screens, close captioning, and time-stamped search functions!), Panopto is authenticated via your Blackboard credentials, ensuring that our course videos are not accessible to the open internet.\nFor the privacy of your peers, and to foster an environment of trust and academic freedom to explore ideas, do not record our course lectures or discussions. You are already getting my official copies.\nThe Family Educational Rights and Privacy Act prevents me from disclosing or discussing any student information, including grades and records about student performance. If the student is at least 18 years of age, parents (or spouses) do not have a right to obtain this information, except with consent by the student.\nMany of you may be tuning in remotely, living with parents, and may have occasional interruptions due to sharing a space. This is normal and fine, but know that I will protect your privacy and not discuss your performance when parents (or anyone other than you, for that matter) are present, without your explicit consent.\n Enrollment Students are responsible for verifying their enrollment in this class. The last day to add or drop this class with no penalty is Wednesday, September 1. Be aware of important dates.\n Honor Code Hood College has an Academic Honor Code which requires all members of this community to maintain the highest standards of academic honesty and integrity. Cheating, plagiarism, lying, and stealing are all prohibited. All violations of the Honor Code are taken seriously, will be reported to appropriate authority, and may result in severe penalties, including expulsion from the college. See here for more detailed information.\n Van Halen and M\u0026amp;Ms When you have completed reading the syllabus, email me a picture of the band Van Halen and a picture of a bowl of M\u0026amp;Ms. If you do this before the date of the first exam, you will get bonus points on the exam. If 75-100% of the class does this, you each get 2 points. If 50-75% of the class does this, you each get 4 points. If 25-50% of the class does this, you each get 6 points. If 0-25% of the class does this, you each get 8 points. Yes, you read this correctly.\n Accessibility, Equity, and Accommodations College courses can, and should, be challenging and bring you out of your comfort zone in a safe and equitable environment. If, however, you feel at any point in the semester that certain assignments or aspects of the course will be disproportionately uncomfortable or burdensome for you due to any factor beyond your control, please come see me or email me. I am a very understanding person and am happy to work out a solution together. I reserve the right to modify and reweight assignments at my sole discretion for students that I belive would legitimately be at a disadvantage, through no fault of their own, to complete them as described.\nIf you are unable to afford required textbooks or other resources for any reason, come see me and we can find a solution that works for you.\nThis course is intended to be accessible for all students, including those with mental, physical, or cognitive disabilities, illness, injuries, impairments, or any other condition that tends to negatively affect one’s equal access to education. If at any point in the term, you find yourself not able to fully access the space, content, and experience of this course, you are welcome to contact me to discuss your specific needs. I also encourage you to contact the Office of Accessibility Services (301-696-3421). If you have a diagnosis or history of accommodations in high school or previous postsecondary institutions, Accessibility Services can help you document your needs and create an accommodation plan. By making a plan through Accessibility Services, you can ensure appropriate accommodations without disclosing your condition or diagnosis to course instructors.\n  Tentative Schedule You can find a full schedule with much more details, including the readings, appendices, and other further resources for each class meeting on the schedule page.\n  On average, even for students who complete all assignments, those that do not regularly attend class suffer by a full letter grade, (Levitt 1993).↩︎\n   ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1629566402,"objectID":"e4d5a4a79239f08c6ad0d7cbf1be756c","permalink":"https://metricsf21.classes.ryansafner.com/syllabus/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/syllabus/","section":"","summary":"Course Format (and Covid)  Learning During a Global Pandemic  Course objectives Required Course materials  Books Articles Software  Assignments and Grades Policies and Expectations  Attendence Late Assignments Grading Communication: Email, Slack, and Virtual Office Hours Netiquette Privacy Enrollment Honor Code Accessibility, Equity, and Accommodations  Tentative Schedule   Instructor  Dr. Ryan Safner  Office: Rosenstock 218-B  Office Hours: MW 1:30 PM—2:30 PM \u0026amp; by appt  safner@hood.","tags":null,"title":"Syllabus","type":"page"}]