---
title: "2.4 — OLS: Goodness of Fit and Bias — Class Content"
draft: false
linktitle: "2.4 — OLS Goodness of Fit & Bias"
date: "2020-06-08"
menu:
  content:
    parent: Course content
    weight: 10
type: docs
output:
  blogdown::html_page:
    toc: true
slides: "2.4-slides"
---

<script src="/rmarkdown-libs/header-attrs/header-attrs.js"></script>

<div id="TOC">
<ul>
<li><a href="#overview"><i class="fas fa-info-circle fa-lg"></i> Overview</a></li>
<li><a href="#readings"><i class="fas fa-book-reader fa-lg"></i> Readings</a></li>
<li><a href="#slides"><i class="fas fa-chalkboard-teacher"></i> Slides</a></li>
<li><a href="#assignments"><i class="fas fa-laptop-code"></i> Assignments</a>
<ul>
<li><a href="#problem-set-1">Problem Set 1</a></li>
</ul></li>
<li><a href="#math-appendix">Math Appendix</a>
<ul>
<li><a href="#deriving-the-ols-estimators">Deriving the OLS Estimators</a></li>
<li><a href="#finding-hatbeta_0">Finding <span class="math inline">\(\hat{\beta_0}\)</span></a></li>
<li><a href="#finding-hatbeta_1">Finding <span class="math inline">\(\hat{\beta_1}\)</span></a></li>
<li><a href="#algebraic-properties-of-ols-estimators">Algebraic Properties of OLS Estimators</a></li>
<li><a href="#bias-in-hatbeta_1">Bias in <span class="math inline">\(\hat{\beta_1}\)</span></a></li>
<li><a href="#proof-of-the-unbiasedness-of-hatbeta_1">Proof of the Unbiasedness of <span class="math inline">\(\hat{\beta_1}\)</span></a></li>
</ul></li>
</ul>
</div>

<p>{{% alert note %}}
<em>Thursday, September 16, 2021</em>
{{% /alert %}}</p>
<p>{{% alert warning %}}
<a href="/assignments/01-problem-set">Problem Set 1</a> answers are posted on that page. <a href="/assignments/02-problem-set">Problem Set 2</a> is due by class Tuesday September 21.
{{% /alert %}}</p>
<div id="overview" class="section level2">
<h2><i class="fas fa-info-circle fa-lg"></i> Overview</h2>
<p>Today we continue looking at basic OLS regression. We will cover how to measure if a regression line is a good fit (using <span class="math inline">\(R^2\)</span> and <span class="math inline">\(\sigma_u\)</span> or SER), and whether OLS estimators are <em>biased</em>. These will depend on four critical <em>assumptions about <span class="math inline">\(u\)</span>.</em></p>
<p>In doing so, we begin an ongoing exploration into inferential statistics, which will finally become clear in another week. The most confusing part is recognizing that there is a <em>sampling distribution of each OLS estimator</em>. We want to measure the center of that sampling distribution, to see if the estimator is <em>biased</em>. Next class we will measure the spread of that distribution.</p>
<p>We continue the extended example about class sizes and test scores, which comes from a (Stata) dataset from an old textbook that I used to use, <a href="https://www.pearson.com/us/higher-education/product/Stock-Introduction-to-Econometrics-Update-3rd-Edition/9780133486872.html?tab=resources">Stock and Watson, 2007</a>. Download and follow along with the data from today’s example:<a href="#fn1" class="footnote-ref" id="fnref1"><sup>1</sup></a></p>
<ul>
<li><a href="http://metricsf21.classes.ryansafner.com/data/caschool.dta"><i class="fas fa-table"></i> <code>caschool.dta</code></a></li>
</ul>
<p>I have also made a RStudio Cloud project documenting all of the things we have been doing with this data that may help you when you start working with regressions (next class):</p>
<ul>
<li><a href="https://rstudio.cloud/spaces/83147/project/1611251"><i class="fas fa-cloud"></i> Class Size Regression Analysis</a></li>
</ul>
</div>
<div id="readings" class="section level2">
<h2><i class="fas fa-book-reader fa-lg"></i> Readings</h2>
<ul>
<li><i class="fas fa-book"></i> Ch. 3.2-3.4, 3.7-3.8 in Bailey, <em>Real Econometrics</em></li>
</ul>
</div>
<div id="slides" class="section level2">
<h2><i class="fas fa-chalkboard-teacher"></i> Slides</h2>
<p>Below, you can find the slides in two formats. Clicking the image will bring you to the html version of the slides in a new tab. Note while in going through the slides, you can type <kbd>h</kbd> to see a special list of viewing options, and type <kbd>o</kbd> for an outline view of all the slides.</p>
<p>The lower button will allow you to download a PDF version of the slides. I suggest printing the slides beforehand and using them to take additional notes in class (<em>not everything</em> is in the slides)!</p>
<p>{{% slide-links %}}</p>
</div>
<div id="assignments" class="section level2">
<h2><i class="fas fa-laptop-code"></i> Assignments</h2>
<div id="problem-set-1" class="section level3">
<h3>Problem Set 1</h3>
<p><a href="/assignments/02-problem-set">Problem Set 2</a> is due by Tuesday September 21. Please see the <a href="/assignments/problem-sets">instructions</a> for more information on how to submit your assignment (there are multiple ways!).</p>
</div>
</div>
<div id="math-appendix" class="section level2">
<h2>Math Appendix</h2>
<div id="deriving-the-ols-estimators" class="section level3">
<h3>Deriving the OLS Estimators</h3>
<p>The population linear regression model is:</p>
<p><span class="math display">\[Y_i=\beta_0+\beta_1 X_i + u _i\]</span></p>
<p>The errors <span class="math inline">\((u_i)\)</span> are unobserved, but for candidate values of <span class="math inline">\(\hat{\beta_0}\)</span> and <span class="math inline">\(\hat{\beta_1}\)</span>, we can obtain an estimate of the residual. Algebraically, the error is:</p>
<p><span class="math display">\[\hat{u_i}=    Y_i-\hat{\beta_0}-\hat{\beta_1}X_i\]</span></p>
<p>Recall our goal is to find <span class="math inline">\(\hat{\beta_0}\)</span> and <span class="math inline">\(\hat{\beta_1}\)</span> that <em>minimizes</em> the sum of squared errors (SSE):</p>
<p><span class="math display">\[SSE= \sum^n_{i=1} \hat{u_i}^2\]</span></p>
<p>So our minimization problem is:</p>
<p><span class="math display">\[\min_{\hat{\beta_0}, \hat{\beta_1}} \sum^n_{i=1} (Y_i-\hat{\beta_0}-\hat{\beta_1}X_i)^2\]</span></p>
<p>Using calculus, we take the partial derivatives and set it equal to 0 to find a minimum. The first order conditions are:</p>
<p><span class="math display">\[\begin{align*}
\frac{\partial SSE}{\partial \hat{\beta_0}}&amp;=-2\displaystyle\sum^n_{i=1} (Y_i-\hat{\beta_0}-\hat{\beta_1} X_i)=0\\
\frac{\partial SSE}{\partial \hat{\beta_1}}&amp;=-2\displaystyle\sum^n_{i=1} (Y_i-\hat{\beta_0}-\hat{\beta_1} X_i)X_i=0\\
\end{align*}\]</span></p>
</div>
<div id="finding-hatbeta_0" class="section level3">
<h3>Finding <span class="math inline">\(\hat{\beta_0}\)</span></h3>
<p>Working with the first FOC, divide both sides by <span class="math inline">\(-2\)</span>:</p>
<p><span class="math display">\[\displaystyle\sum^n_{i=1} (Y_i-\hat{\beta_0}-\hat{\beta_1} X_i)=0\]</span></p>
<p>Then expand the summation across all terms and divide by <span class="math inline">\(n\)</span>:</p>
<p><span class="math display">\[\underbrace{\frac{1}{n}\sum^n_{i=1} Y_i}_{\bar{Y}}-\underbrace{\frac{1}{n}\sum^n_{i=1}\hat{\beta_0}}_{\hat{\beta_0}}-\underbrace{\frac{1}{n}\sum^n_{i=1} \hat{\beta_1} X_i}_{\hat{\beta_1}\bar{X}}=0\]</span></p>
<p>Note the first term is <span class="math inline">\(\bar{Y}\)</span>, the second is <span class="math inline">\(\hat{\beta_0}\)</span>, the third is <span class="math inline">\(\hat{\beta_1}\bar{X}\)</span>.<a href="#fn2" class="footnote-ref" id="fnref2"><sup>2</sup></a></p>
<p>So we can rewrite as:
<span class="math display">\[\bar{Y}-\hat{\beta_0}-\beta_1=0\]</span></p>
<p>Rearranging:</p>
<p><span class="math display">\[\hat{\beta_0}=\bar{Y}-\bar{X}\beta_1\]</span></p>
</div>
<div id="finding-hatbeta_1" class="section level3">
<h3>Finding <span class="math inline">\(\hat{\beta_1}\)</span></h3>
<p>To find <span class="math inline">\(\hat{\beta_1}\)</span>, take the second FOC and divide by <span class="math inline">\(-2\)</span>:</p>
<p><span class="math display">\[\displaystyle\sum^n_{i=1} (Y_i-\hat{\beta_0}-\hat{\beta_1} X_i)X_i=0\]</span></p>
<p>From the formula for <span class="math inline">\(\hat{\beta_0}\)</span>, substitute in for <span class="math inline">\(\hat{\beta_0}\)</span>:</p>
<p><span class="math display">\[\displaystyle\sum^n_{i=1} \bigg(Y_i-[\bar{Y}-\hat{\beta_1}\bar{X}]-\hat{\beta_1} X_i\bigg)X_i=0\]</span></p>
<p>Combining similar terms:</p>
<p><span class="math display">\[\displaystyle\sum^n_{i=1} \bigg([Y_i-\bar{Y}]-[X_i-\bar{X}]\hat{\beta_1}\bigg)X_i=0\]</span></p>
<p>Distribute <span class="math inline">\(X_i\)</span> and expand terms into the subtraction of two sums (and pull out <span class="math inline">\(\hat{\beta_1}\)</span> as a constant in the second sum:</p>
<p><span class="math display">\[\displaystyle\sum^n_{i=1} [Y_i-\bar{Y}]X_i-\hat{\beta_1}\displaystyle\sum^n_{i=1}[X_i-\bar{X}]X_i=0\]</span></p>
<p>Move the second term to the righthand side:</p>
<p><span class="math display">\[\displaystyle\sum^n_{i=1} [Y_i-\bar{Y}]X_i=\hat{\beta_1}\displaystyle\sum^n_{i=1}[X_i-\bar{X}]X_i\]</span></p>
<p>Divide to keep just <span class="math inline">\(\hat{\beta_1}\)</span> on the right:</p>
<p><span class="math display">\[\frac{\displaystyle\sum^n_{i=1} [Y_i-\bar{Y}]X_i}{\displaystyle\sum^n_{i=1}[X_i-\bar{X}]X_i}=\hat{\beta_1}\]</span></p>
<p>Note that from the <a href="https://metricsf19.classes.ryansafner.com/class/07-class/#the-summation-operatorproperties">rules about summation operators</a>:</p>
<p><span class="math display">\[\displaystyle\sum^n_{i=1} [Y_i-\bar{Y}]X_i=\displaystyle\sum^n_{i=1} (Y_i-\bar{Y})(X_i-\bar{X})\]</span></p>
<p>and:</p>
<p><span class="math display">\[\displaystyle\sum^n_{i=1} [X_i-\bar{X}]X_i=\displaystyle\sum^n_{i=1} (X_i-\bar{X})(X_i-\bar{X})=\displaystyle\sum^n_{i=1}(X_i-\bar{X})^2\]</span></p>
<p>Plug in these two facts:</p>
<p><span class="math display">\[\frac{\displaystyle\sum^n_{i=1} (Y_i-\bar{Y})(X_i-\bar{X})}{\displaystyle\sum^n_{i=1}(X_i-\bar{X})^2}=\hat{\beta_1}\]</span></p>
</div>
<div id="algebraic-properties-of-ols-estimators" class="section level3">
<h3>Algebraic Properties of OLS Estimators</h3>
<p>The OLS residuals <span class="math inline">\(\hat{u}\)</span> and predicted values <span class="math inline">\(\hat{Y}\)</span> are chosen by the minimization problem to satisfy:</p>
<ol style="list-style-type: decimal">
<li><p>The expected value (average) error is 0:
<span class="math display">\[E(u_i)=\frac{1}{n}\displaystyle \sum_{i=1}^n \hat{u_i}=0\]</span></p></li>
<li><p>The covariance between <span class="math inline">\(X\)</span> and the errors is 0:
<span class="math display">\[\hat{\sigma}_{X,u}=0\]</span></p></li>
</ol>
<p>Note the first two properties imply strict <strong>exogeneity</strong>. That is, this is only a valid model if <span class="math inline">\(X\)</span> and <span class="math inline">\(u\)</span> are not correlated.</p>
<ol start="3" style="list-style-type: decimal">
<li><p>The expected predicted value of <span class="math inline">\(Y\)</span> is equal to the expected value of <span class="math inline">\(Y\)</span>:
<span class="math display">\[\bar{\hat{Y}}=\frac{1}{n} \displaystyle\sum_{i=1}^n \hat{Y_i} = \bar{Y}\]</span></p></li>
<li><p>Total sum of squares is equal to the explained sum of squares plus sum of squared errors:
<span class="math display">\[\begin{align*}TSS&amp;=ESS+SSE\\
\sum_{i=1}^n (Y_i-\bar{Y})^2&amp;=\sum_{i=1}^n (\hat{Y_i}-\bar{Y})^2 + \sum_{i=1}^n {u}^2\\
\end{align*}\]</span></p></li>
</ol>
<p>Recall <span class="math inline">\(R^2\)</span> is <span class="math inline">\(\frac{ESS}{TSS}\)</span> or <span class="math inline">\(1-SSE\)</span></p>
<ol start="5" style="list-style-type: decimal">
<li>The regression line passes through the point <span class="math inline">\((\bar{X},\bar{Y})\)</span>, i.e. the mean of <span class="math inline">\(X\)</span> and the mean of <span class="math inline">\(Y\)</span>.</li>
</ol>
</div>
<div id="bias-in-hatbeta_1" class="section level3">
<h3>Bias in <span class="math inline">\(\hat{\beta_1}\)</span></h3>
<p>Begin with the formula we derived for <span class="math inline">\(\hat{\beta_1}\)</span>:</p>
<p><span class="math display">\[\hat{\beta_1}=\frac{\displaystyle \sum^n_{i=1} (Y_i-\bar{Y})(X_i-\bar{X})}{\displaystyle\sum^n_{i=1} (X_i-\bar{X})^2}\]</span></p>
<p>Recall from <strong>Rule 6</strong> of summations, we can rewrite the numerator as</p>
<p><span class="math display">\[\begin{align*}
    =&amp;\displaystyle \sum^n_{i=1} (Y_i-\bar{Y})(X_i-\bar{X})\\
    =&amp; \displaystyle \sum^n_{i=1} Y_i(X_i-\bar{X})\\
\end{align*}\]</span></p>
<p><span class="math display">\[\hat{\beta_1}=\frac{\displaystyle \sum^n_{i=1} Y_i(X_i-\bar{X})}{\displaystyle\sum^n_{i=1} (X_i-\bar{X})^2}\]</span></p>
<p>We know the true population relationship is expressed as:</p>
<p><span class="math display">\[Y_i=\beta_0+\beta_1 X_i+u_i\]</span></p>
<p>Substituting this in for <span class="math inline">\(Y_i\)</span> in equation 2:</p>
<p><span class="math display">\[\hat{\beta_1}=\frac{\displaystyle \sum^n_{i=1} (\beta_0+\beta_1X_i+u_i)(X_i-\bar{X})}{\displaystyle\sum^n_{i=1} (X_i-\bar{X})^2}\]</span>
Breaking apart the sums in the numerator:</p>
<p><span class="math display">\[\hat{\beta_1}=\frac{\displaystyle \sum^n_{i=1} \beta_0(X_i-\bar{X})+\displaystyle \sum^n_{i=1} \beta_1X_i(X_i-\bar{X})+\displaystyle \sum^n_{i=1} u_i(X_i-\bar{X})}{\displaystyle\sum^n_{i=1} (X_i-\bar{X})^2}\]</span></p>
<p>We can simplify equation 4 using <strong>Rules 4 and 5</strong> of summations</p>
<ol style="list-style-type: decimal">
<li>The first term in the numerator <span class="math inline">\(\left[\displaystyle \sum^n_{i=1} \beta_0(X_i-\bar{X})\right]\)</span> has the constant <span class="math inline">\(\beta_0\)</span>, which can be pulled out of the summation. This gives us the summation of deviations, which add up to 0 as per <strong>Rule 4</strong>:</li>
</ol>
<p><span class="math display">\[\begin{align*}
\displaystyle \sum^n_{i=1} \beta_0(X_i-\bar{X})&amp;= \beta_0 \displaystyle \sum^n_{i=1} (X_i-\bar{X})\\
&amp;=\beta_0 (0)\\
&amp;=0\\
\end{align*}\]</span></p>
<ol start="2" style="list-style-type: decimal">
<li>The second term in the numerator <span class="math inline">\(\left[\displaystyle \sum^n_{i=1} \beta_1X_i(X_i-\bar{X})\right]\)</span> has the constant <span class="math inline">\(\beta_1\)</span>, which can be pulled out of the summation. Additionally, <strong>Rule 5</strong> tells us <span class="math inline">\(\displaystyle \sum^n_{i=1} X_i(X_i-\bar{X})=\displaystyle \sum^n_{i=1}(X_i-\bar{X})^2\)</span>:</li>
</ol>
<p><span class="math display">\[\begin{align*}
\displaystyle \sum^n_{i=1} \beta_1X_1(X_i-\bar{X})&amp;= \beta_1 \displaystyle \sum^n_{i=1} X_i(X_i-\bar{X})\\
&amp;=\beta_1\displaystyle \sum^n_{i=1}(X_i-\bar{X})^2\\
\end{align*}\]</span></p>
<p>When placed back in the context of being the numerator of a fraction, we can see this term simplifies to just <span class="math inline">\(\beta_1\)</span>:</p>
<p><span class="math display">\[\begin{align*}
    \frac{\beta_1\displaystyle \sum^n_{i=1}(X_i-\bar{X})^2}{\displaystyle\sum^n_{i=1} (X_i-\bar{X})^2} &amp;=\frac{\beta_1}{1} \times \frac{\displaystyle \sum^n_{i=1}(X_i-\bar{X})^2}{\displaystyle\sum^n_{i=1} (X_i-\bar{X})^2}\\
    &amp;=\beta_1   \\
\end{align*}\]</span></p>
<p>Thus, we are left with:</p>
<p><span class="math display">\[\hat{\beta_1}=\beta_1+\frac{\displaystyle \sum^n_{i=1} u_i(X_i-\bar{X})}{\displaystyle\sum^n_{i=1} (X_i-\bar{X})^2}\]</span></p>
<p>Now, take the expectation of both sides:</p>
<p><span class="math display">\[E[\hat{\beta_1}]=E\left[\beta_1+\frac{\displaystyle \sum^n_{i=1} u_i(X_i-\bar{X})}{\displaystyle\sum^n_{i=1} (X_i-\bar{X})^2} \right]\]</span></p>
<p>We can break this up, using properties of expectations. First, recall <span class="math inline">\(E[a+b]=E[a]+E[b]\)</span>, so we can break apart the two terms.</p>
<p><span class="math display">\[E[\hat{\beta_1}]=E[\beta_1]+E\left[\frac{\displaystyle \sum^n_{i=1} u_i(X_i-\bar{X})}{\displaystyle\sum^n_{i=1} (X_i-\bar{X})^2} \right]\]</span></p>
<p>Second, the true population value of <span class="math inline">\(\beta_1\)</span> is a constant, so <span class="math inline">\(E[\beta_1]=\beta_1\)</span>.</p>
<p>Third, since we assume <span class="math inline">\(X\)</span> is also “fixed” and not random, the variance of <span class="math inline">\(X\)</span>, <span class="math inline">\(\displaystyle\sum_{i=1}^n (X_i-\bar{X})\)</span>, in the denominator, is just a constant, and can be brought outside the expectation.</p>
<p><span class="math display">\[E[\hat{\beta_1}]=\beta_1+\frac{E\left[\displaystyle \sum^n_{i=1} u_i(X_i-\bar{X})\right]  }{\displaystyle\sum^n_{i=1} (X_i-\bar{X})^2}\]</span></p>
<p>Thus, the properties of the equation are primarily driven by the expectation <span class="math inline">\(E\bigg[\displaystyle \sum^n_{i=1} u_i(X_i-\bar{X})\bigg]\)</span>. We now turn to this term.</p>
<p>Use the property of summation operators to expand the numerator term:</p>
<p><span class="math display">\[\begin{align*}
    \hat{\beta_1}&amp;=\beta_1+\frac{\displaystyle \sum^n_{i=1} u_i(X_i-\bar{X})}{\displaystyle\sum^n_{i=1} (X_i-\bar{X})^2} \\
        \hat{\beta_1}&amp;=\beta_1+\frac{\displaystyle \sum^n_{i=1} (u_i-\bar{u})(X_i-\bar{X})}{\displaystyle\sum^n_{i=1} (X_i-\bar{X})^2} \\
\end{align*}\]</span></p>
<p>Now divide the numerator and denominator of the second term by <span class="math inline">\(\frac{1}{n}\)</span>. Realize this gives us the covariance between <span class="math inline">\(X\)</span> and <span class="math inline">\(u\)</span> in the numerator and variance of <span class="math inline">\(X\)</span> in the denominator, based on their respective definitions.</p>
<p><span class="math display">\[\begin{align*}
    \hat{\beta_1}&amp;=\beta_1+\cfrac{\frac{1}{n}\displaystyle \sum^n_{i=1} (u_i-\bar{u})(X_i-\bar{X})}{\frac{1}{n}\displaystyle\sum^n_{i=1} (X_i-\bar{X})^2} \\
    \hat{\beta_1}&amp;=\beta_1+\cfrac{cov(X,u)}{var(X)} \\
    \hat{\beta_1}&amp;=\beta_1+\cfrac{s_{X,u}}{s_X^2} \\
\end{align*}\]</span></p>
<p>By the Zero Conditional Mean assumption of OLS, <span class="math inline">\(s_{X,u}=0\)</span>.</p>
<p>Alternatively, we can express the bias in terms of correlation instead of covariance:</p>
<p><span class="math display">\[E[\hat{\beta_1}]=\beta_1+\cfrac{cov(X,u)}{var(X)}\]</span></p>
<p>From the definition of correlation:</p>
<p><span class="math display">\[\begin{align*}
     cor(X,u)&amp;=\frac{cov(X,u)}{s_X s_u}\\
     cor(X,u)s_Xs_u &amp;=cov(X,u)\\
\end{align*}\]</span></p>
<p>Plugging this in:</p>
<p><span class="math display">\[\begin{align*}
E[\hat{\beta_1}]&amp;=\beta_1+\frac{cov(X,u)}{var(X)} \\
E[\hat{\beta_1}]&amp;=\beta_1+\frac{\big[cor(X,u)s_xs_u\big]}{s^2_X} \\
E[\hat{\beta_1}]&amp;=\beta_1+\frac{cor(X,u)s_u}{s_X} \\
E[\hat{\beta_1}]&amp;=\beta_1+cor(X,u)\frac{s_u}{s_X} \\
\end{align*}\]</span></p>
</div>
<div id="proof-of-the-unbiasedness-of-hatbeta_1" class="section level3">
<h3>Proof of the Unbiasedness of <span class="math inline">\(\hat{\beta_1}\)</span></h3>
<p>Begin with equation:<a href="#fn3" class="footnote-ref" id="fnref3"><sup>3</sup></a></p>
<p><span class="math display">\[\hat{\beta_1}=\frac{\sum Y_iX_i}{\sum X_i^2}\]</span></p>
<p>Substitute for <span class="math inline">\(Y_i\)</span>:</p>
<p><span class="math display">\[\hat{\beta_1}=\frac{\sum (\beta_1 X_i+u_i)X_i}{\sum X_i^2}\]</span></p>
<p>Distribute <span class="math inline">\(X_i\)</span> in the numerator:</p>
<p><span class="math display">\[\hat{\beta_1}=\frac{\sum \beta_1 X_i^2+u_iX_i}{\sum X_i^2}\]</span></p>
<p>Separate the sum into additive pieces:</p>
<p><span class="math display">\[\hat{\beta_1}=\frac{\sum \beta_1 X_i^2}{\sum X_i^2}+\frac{u_i X_i}{\sum X_i^2}\]</span></p>
<p><span class="math inline">\(\beta_1\)</span> is constant, so we can pull it out of the first sum:</p>
<p><span class="math display">\[\hat{\beta_1}=\beta_1 \frac{\sum X_i^2}{\sum X_i^2}+\frac{u_i X_i}{\sum X_i^2}\]</span></p>
<p>Simplifying the first term, we are left with:</p>
<p><span class="math display">\[\hat{\beta_1}=\beta_1 +\frac{u_i X_i}{\sum X_i^2}\]</span></p>
<p>Now if we take expectations of both sides:</p>
<p><span class="math display">\[E[\hat{\beta_1}]=E[\beta_1] +E\left[\frac{u_i X_i}{\sum X_i^2}\right]\]</span></p>
<p><span class="math inline">\(\beta_1\)</span> is a constant, so the expectation of <span class="math inline">\(\beta_1\)</span> is itself.</p>
<p><span class="math display">\[E[\hat{\beta_1}]=\beta_1 +E\bigg[\frac{u_i X_i}{\sum X_i^2}\bigg]\]</span></p>
<p>Using the properties of expectations, we can pull out <span class="math inline">\(\frac{1}{\sum X_i^2}\)</span> as a constant:</p>
<p><span class="math display">\[E[\hat{\beta_1}]=\beta_1 +\frac{1}{\sum X_i^2} E\bigg[\sum u_i X_i\bigg]\]</span></p>
<p>Again using the properties of expectations, we can put the expectation inside the summation operator (the expectation of a sum is the sum of expectations):</p>
<p><span class="math display">\[E[\hat{\beta_1}]=\beta_1 +\frac{1}{\sum X_i^2}\sum E[u_i X_i]\]</span></p>
<p>Under the exogeneity condition, the correlation between <span class="math inline">\(X_i\)</span> and <span class="math inline">\(u_i\)</span> is 0.</p>
<p><span class="math display">\[E[\hat{\beta_1}]=\beta_1\]</span></p>
</div>
</div>
<div class="footnotes">
<hr />
<ol>
<li id="fn1"><p>Note this is a <code>.dta</code> Stata file. You will need to (install and) load the package <code>haven</code> to <code>read_dta()</code> Stata files into a dataframe.<a href="#fnref1" class="footnote-back">↩︎</a></p></li>
<li id="fn2"><p>From the <a href="https://metricsf19.classes.ryansafner.com/class/07-class/#the-summation-operator">rules about summation operators</a>, we define the mean of a random variable <span class="math inline">\(X\)</span> as <span class="math inline">\(\bar{X}=\frac{1}{n}\displaystyle\sum_{i=1}^n X_i\)</span>. The mean of a constant, like <span class="math inline">\(\beta_0\)</span> or <span class="math inline">\(\beta_1\)</span> is itself.<a href="#fnref2" class="footnote-back">↩︎</a></p></li>
<li id="fn3"><p>Admittedly, this is a simplified version where <span class="math inline">\(\hat{\beta_0}=0\)</span>, but there is no loss of generality in the results.<a href="#fnref3" class="footnote-back">↩︎</a></p></li>
</ol>
</div>
