---
title: "4.2 — Dummy Dependent Variables"
subtitle: "ECON 480 • Econometrics • Fall 2021"
author: 'Ryan Safner<br> Assistant Professor of Economics <br> <a href="mailto:safner@hood.edu"><i class="fa fa-paper-plane fa-fw"></i>safner@hood.edu</a> <br> <a href="https://github.com/ryansafner/metricsF21"><i class="fa fa-github fa-fw"></i>ryansafner/metricsF21</a><br> <a href="https://metricsF21.classes.ryansafner.com"> <i class="fa fa-globe fa-fw"></i>metricsF21.classes.ryansafner.com</a><br>'
#date:
output:
  xaringan::moon_reader:
    mathjax: "https://cdn.bootcss.com/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_HTMLorMML" # rescales math with css changes https://github.com/yihui/xaringan/issues/143
    lib_dir: libs
    df_print: paged
    css: [custom.css, "hygge"] #, metropolis, metropolis-fonts
    nature:
      beforeInit: ["macros.js", "https://platform.twitter.com/widgets.js"] # first is for rescaling images , second is for embedding tweets, https://github.com/yihui/xaringan/issues/100
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
      ratio: "16:9"
      slideNumberFormat: |
        <div class="progress-bar-container">
          <div class="progress-bar" style="width: calc(%current% / %total% * 100%);">
          </div>
        </div>
    includes:
      in_header: header.html # for font awesome, used in title  
---

```{r setup, include=FALSE}
options(htmltools.dir.version = FALSE)
knitr::opts_chunk$set(echo=F,
                      message=F,
                      warning=F,
                      fig.retina=3)
library(tidyverse)
library(ggthemes)
library(parallel)

set.seed(256)
update_geom_defaults("label", list(family = "Fira Sans Condensed"))
update_geom_defaults("text", list(family = "Fira Sans Condensed"))
xaringanExtra::use_tile_view()
xaringanExtra::use_tachyons()
xaringanExtra::use_freezeframe()
xaringanExtra::use_extra_styles(hover_code_line = TRUE)
library(gganimate)
library(broom)
```

class: inverse

# Outline

### [Difference-in-Difference Models](#3)
### [Example I: HOPE in Georgia](#32)
### [Generalizing DND Models](#74)
### [Example II: "The" Card-Kreuger Minimum Wage Study](#88)

---

# Dummy Dependent Variable Models

- We've seen models where *independent* $(X)$ variables are dummy variables

- We now consider in more detail models where the *dependent* $(Y)$ variable is a dummy variable

$$Y_i=\beta_0+\beta_1X_i+u_i \quad Y \in (0,1)$$

---

# Example: Defaults

.content-box-green[
.hi-green[Example]: Whether or not a borrower defaults

- Does having a higher balance .hi-purple[increase the probability of default]?

]

- $\text{Default}_i = \begin{cases} 1 & \text{ if person }i \text{ defaulted}\\ 0 & \text{ if person }i \text{ did not default}\\ \end{cases}$

---

# Example: Defaults

```{r, echo=F}
# install ISLR package
# install.packages("ISLR")
library(tidyverse)
Default <- ISLR::Default
```

```{r, echo=T}
Default
```

---

# Example: Defaults

```{r}
Default %>%
  count(default) %>%
  mutate(percent = round(n/sum(n) * 100,2))
```

- Only about 3.33% default

---

# Example: Defaults

.pull-left[

- Points cluster at `Yes` and `No` for `default`

- Since `default` is a `factor`, can graph just like this
]

.pull-right[

```{r}
ggplot(data = Default)+
  aes(x = balance,
      y = default)+
  geom_point(color = "purple", alpha=0.4, size = 2)+
  scale_x_continuous(labels = scales::dollar)+
  labs(x = "Balance",
       y = "Default")+
  theme_minimal()
```

]

---

# Example: Defaults

- We could make `default` explicitly a dummy variable

```{r}
Default <- Default %>%
  mutate(defaulted = ifelse(test = default == "Yes",
                            yes = 1,
                            no = 0))

Default
```


---

# Example: Defaults

.pull-left[

- Now graph as dummy, again points at either 0 or 1 for `defaulted`
]

.pull-right[
```{r}
p <- ggplot(data = Default)+
  aes(x = balance,
      y = defaulted)+
  geom_point(color = "purple", alpha=0.4, size = 2)+
  scale_x_continuous(labels = scales::dollar)+
  labs(x = "Balance",
       y = "Default")+
  theme_minimal()

p
```

]

---

# Example: Defaults

.pull-left[

- Now graph as dummy, again points at either 0 or 1 for `defaulted`

- Can try to `jitter` data to see better
]

.pull-right[
```{r}
ggplot(data = Default)+
  aes(x = balance,
      y = defaulted)+
  geom_jitter(color = "purple", height=0.25, alpha=0.4, size = 2)+
  scale_x_continuous(labels = scales::dollar)+
  labs(x = "Balance",
       y = "Default")+
  theme_minimal()
```

]

---

# Linear Probability Model

$$Pr(Y_i = 1 | X_1, X_2)= \beta_0+\beta_1 X_{1i} + \beta_2 X_{2i} + u_i$$

- The expected value of $Y$ given all $X$'s, which is $Pr(Y_i=1 |X_1, X_2)$, is a linear function of the $X$'s

- Left as is, this is a .hi[linear probability model]

- Easy interpretation: $\hat{\beta_1}$ is the marginal effect of a 1-unit increase in $X_1$ (holding $X_2$ constant) in the probability of $Y_i=1$

- Problem: $\hat{Y}$ is not constrained to be between 0 and 1! 

---

# Example: Defaults

- Suppose we estimated our OLS regression as usual

$$\text{Default}_i = \beta_0+\beta_1 \text{Balance}_i$$
```{r, echo = T}
lm(defaulted ~ balance, data = Default) %>%
  tidy()
```

- $\hat{\beta_0}$: an individual with a $0 balance has a -0.075 probability (-7.5%) of defaulting (!)

- $\hat{\beta_1}$: for every additional $1 balance, an individual’s probability of defaulting increases by 0.0001 (0.01%)
  - Additional $100 in balance increases probability by 0.01 (1%)
  
---

# Example: Defaults

.pull-left[

- Line doesn’t fit the data very well

]

.pull-right[
```{r}
p +
  geom_smooth(method = "lm", color = "red")
```
]

---

# Example: Defaults

.pull-left[

- Line doesn’t fit the data very well

- Also $y$-values not bounded between 0 and 1!
]

.pull-right[
```{r}
p +
  geom_rect(xmin = -Inf, xmax = Inf, ymin = -Inf, ymax = 0, fill = "pink", alpha = 0.01)+
  geom_rect(xmin = -Inf, xmax = Inf, ymin = 1, ymax = Inf, fill = "pink", alpha = 0.01)+
  geom_smooth(method = "lm", color = "red")
```

]

---


class: inverse, center, middle

# Probability, Odds, and Logit

---

# Odds

.pull-left[

- Often hear of .hi[“odds”] in gambling & sports betting, closely linked with probability

- Given some probability $p$ of an event occurring, the .hi[odds] of an event occurring are
$$Odds = \frac{p}{1-p}$$

- In betting, the odds are often reported as an .hi-purple[odds ratio]:


- .hi-green[Example] The odds of rolling a 6 on a fair 6-sided die are .hi-purple["5 to 1 against"]
  - $p(\text{Rolling a }6)= \frac{1}{6}$
  - $1-p=\frac{5}{6}$
  - Odds $= \cfrac{\frac{1}{6}}{1-\frac{1}{6}}= \frac{1}{5}$
  - For every 6 trials, we expect on average 1 occurrence to 5 non-occurrences
]

.pull-right[
.center[
![](../images/gamble.jpg)
]
]

---

```{r}
df <- tibble(
  `odds ratio` = c("99:1 against", "9:1 against", "4:1 against", "3:1 against", "2:1 against", "1:1 even", "2:1 for", "3:1 for", "4:1 for", "9:1 for", "99:1 for"),
  p = c(0.01, 0.1, 0.2, 0.25, 0.5, 0.75, 0.90, 0.99),
  `1-p` = 1-p,
  odds = round(p/(`1-p`),2),
)

df
```


---

```{r}
df %>%
  mutate(logit = (log(odds)))
```



---

# "Logit"

- Another name for "logged odds" (Natural log with base $e$.) is .hi-purple[“logit”]

$$\text{logit}(p) = \ln \left(\frac{p}{1-p}\right)$$

- Our logistic regression measures the log odds, or logit, with our coefficients $(\beta$’s):

$$\text{logit}(p)= \ln \left(\frac{p}{1-p}\right) = \beta_0 + \beta_1 X_{1i} + \beta_2 X_{2i}$$


---

# Interpretation of Coefficients: Direct

- Direct interpretation of coefficients:

- $\hat{\beta_k}$: expected change in *log odds* (of $Y_i=1)$ for a 1-unit increase in $X_{ki}$

---

# Interpretation of Coefficients: Odds Ratio

- Alternatively, intepret via odds ratio:
$$\frac{p}{1-p}= e^{\beta_0+\beta_1 X_{1i}+ \beta_2 X_{2i}} = e^{\hat{\beta_0}}e^{\hat{\beta_1}X_{1i}}e^{\hat{\beta_2}X_{2i}}$$
- $e^{\hat{\beta_k}}$: expected change in *odds* of 1-unit increase in $X_{ki}$ 

---

# Interpretation of Coefficients: Probability

- You can also "back out" a probability estimate from the coefficients $(\hat{\beta}$'s) by exponentiating and reorganizing:

$$\begin{align*}
\ln \left( \frac{p}{1-p} \right ) &= \beta_0+\beta_1 X_{1i}+ \beta_2 X_{2i}\\
\frac{p}{1-p}&= e^{\beta_0+\beta_1 X_{1i}+ \beta_2 X_{2i}}\\
p &=(1-p)e^{\beta_0+\beta_1 X_{1i}+ \beta_2 X_{2i}}\\
p &=e^{\beta_0+\beta_1 X_{1i}+ \beta_2 X_{2i}}-pe^{\beta_0+\beta_1 X_{1i}+ \beta_2 X_{2i}}\\
p+pe^{\beta_0+\beta_1 X_{1i}+ \beta_2 X_{2i}} &=e^{\beta_0+\beta_1 X_{1i}+ \beta_2 X_{2i}}\\
p(1+e^{\beta_0+\beta_1 X_{1i}+ \beta_2 X_{2i}})&=e^{\beta_0+\beta_1 X_{1i}+ \beta_2 X_{2i}}\\
p&=\frac{e^{\beta_0+\beta_1 X_{1i}+ \beta_2 X_{2i}}}{1+e^{\beta_0+\beta_1 X_{1i}+ \beta_2 X_{2i}}}\\
\end{align*}$$

$$p(Y_i=1 \vert X_{1i}, X_{2i}) = \frac{e^{\beta_0+\beta_1 X_{1i}+ \beta_2 X_{2i}}}{1+ e^{\beta_0+\beta_1 X_{1i}+ \beta_2 X_{2i}}}$$

---

```{r}
p+
  geom_smooth(method = "lm", color = "red", size = 1, alpha=0.3)+
  geom_smooth(method = "glm",
              method.args = list(family = "binomial"),
              color = "orange",
              size = 1)
```

---

#

$$Pr(default=Yes \vert Balance)$$

abbreviate as $p(Balance)$


$$odds = \frac{p}{1-p} = e^{\beta_0+\beta_1X}$$

marginal effect on odds, multiplies it by $e^{\beta_1}$
---

# In `R`

- Run logistic regression using `glm()` function^["**g**eneralized" **l**inear **m**odel, implying some transformation to $\beta_0+\beta_1X$, like a logistical transformation here!]

- Arguments:
  - `formula`: e.g. `y ~ x1 + x2`, etc.^[Like `lm()`, you don't need to spell out `formula=`]
  - `family = "binomial"` selects the logistic regression
  - `data`: dataframe to be used
---

# In `R`

.pull-left[
```{r logit-reg, echo=T, eval=F}
logit_reg<-glm(i_default ~ balance,
               family = "binomial",
               data = Default)

logit_reg %>% summary()
```
]

.pull-right[
```{r, ref.label="logit-reg"}

```
]

---

# In `R`

.pull-left[
```{r logit-reg2, echo=T, eval=F}
logit_reg %>% broom::tidy()
```
]

.pull-right[
```{r, ref.label="logit-reg2"}

```
]

---

```{r, echo = T}
logit_reg_tidy<-broom::tidy(logit_reg)

(beta_0<-logit_reg_tidy %>%
  filter(term == "(Intercept)") %>%
  pull(estimate))

(beta_1<-logit_reg_tidy %>%
  filter(term == "balance") %>%
  pull(estimate))

```

---

# In `R`: Intepretation

$$\widehat{Default_i} = -10.651 + 0.005 \, \text{Balance}_i$$

- Remember, these coefficients are the *log odds*
  - A $1 change in Balance $\rightarrow$ to a 0.005 change in log odds

---

# In `R`: Intepretation

$$\widehat{Default_i} = -10.651 + 0.005 \, \text{Balance}_i$$

- By exponentiating $\hat{beta_1}$, we can get the change in odds

```{r, echo=T}
exp(beta_1)
```

- A $1 increase in balance increases the odds of default by `r exp(beta_1)`

---

$$\hat{p}(\text{Default}_i = 1 \vert \text{Balance}_i) = \frac{e^{-10.651+0.005 \text{Balance}_i}}{1+e^{-10.651+0.005 \text{Balance}_i}}$$

Predictions:

- If $\text{Balance} = 0$, we then estimate $\hat{p} \approx `r (exp(beta_0)/(1+exp(beta_0))) %>% round(6) %>% format(scientific = F)`$

$$\hat{p}(\text{Default}_i = 1 \vert \text{Balance}_i) = \frac{e^{-10.651+0.005 \mathbf{(1000)}}}{1+e^{-10.651+0.005 \mathbf{(1000)}}}$$

- If $\text{Balance} = 2,000$, we then estimate $\hat{p} \approx `r (exp(beta_0 + beta_1 * 2e3)/(1+exp(beta_0 + beta_1 * 2e3))) %>% round(3)`$

---

```{r, echo=T}
exp(beta_1)
```

Odds of default increase by `r exp(beta_1)` for a $1 increase in balance.

```{r, echo=T}
logit_reg_tidy<-broom::tidy(logit_reg)

(beta_0<-logit_reg_tidy %>%
  filter(term == "balance") %>%
  pull(estimate))


exp(beta_1)/(1+exp(beta_1))
```


When Balance = 2K

```{r, echo=T}
(beta_0<-logit_reg_tidy %>%
  filter(term == "(Intercept)") %>%
  pull(estimate))


exp(beta_0+2000*beta_1)/(1+exp(beta_0+2000*beta_1))
```

---

Note: Everything we've done so far extends to models with many predictors.

Old news: You can use `predict()` to get predictions out of `glm` objects.

New and important: `predict()` produces multiple types of predictions

1. `type = "response"` predicts on the scale of the response variable
    for logistic regression, this means predicted probabilities (0 to 1)

2. `type = "link"` predicts on the scale of the linear predictors
    for logistic regression, this means predicted log odds (-∞ to ∞)

Beware: The default is type = "link", which you may not want.
