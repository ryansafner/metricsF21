---
title: "2.7 — Inference for Regression — R Practice"
author: "Answer Key"
date: "Tuesday, October 5, 2021"
output:
  html_document:
    df_print: paged
    toc: true 
    toc_depth: 3
    toc_float: true
    code_folding: show
    highlight: tango
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE,
                      message = TRUE,
                      warning = TRUE,
                      fig.retina = 3,
                      max.print = 25)
set.seed(20) # using this number means all "random" generated objects will be identical for all of us!
```

## Question 1
Let’s use the `diamonds` data built into `ggplot`. Simply load `tidyverse` and then to be explicit, save this as a tibble (feel free to rename it) with `diamonds <- diamonds`.

Let's answer the following questions:

> What is the effect of carat size on a diamond's price?

---

<!--ANSWER BELOW HERE-->

```{r}
library(tidyverse)
diamonds <- diamonds
```

---

### Part A

Just to see what we're looking at, make a scatterplot using `ggplot()`, with `x` as `carat` and `y` as `price`, and add a regression line.

---

<!--ANSWER BELOW HERE-->

```{r}
ggplot(data = diamonds)+
  aes(x = carat,
      y = price)+
  geom_point()+
  geom_smooth(method = "lm")
```

---

## Question 2
Suppose we want to estimate the following relationship:

$$\widehat{\text{price}}_i = \beta_0 + \beta_1 \text{carat}_i + u_i$$

Run a regression of `price` on `carat` using `lm()` and get a `summary()` of it. Be sure to save your regression model as an object, we'll need it later.

---

<!--ANSWER BELOW HERE-->

```{r}
reg <- lm(price ~ carat, data = diamonds)

summary(reg)
```

---

### Part A

Write out the estimated regression equation.

<!--ANSWER BELOW HERE-->

$$\widehat{\text{price}}_i = -2256.36 + 7756.43 \text{carat}_i$$

---

### Part B

Make a regression table of the output (using the `huxtable` package).

---

<!--ANSWER BELOW HERE-->

```{r}
library(huxtable)
huxreg(reg)
```

---

### Part C

What is $\hat{\beta_1}$ for this model? Interpret it in the context of our question.

---

<!--ANSWER BELOW HERE-->

$\hat{\beta_1}$, the estimated slope, is 7756.43. For every 1 carat, we expect the price of the diamond to increase by $7,756.43.

---

### Part D

Use the `broom` package's `tidy()` command on your regression object, and calculate confidence intervals for your estimates by setting `conf.int = T` inside `tidy()`. 

What is the 95% confidence interval for $\hat{\beta_1}$, and what does it mean?

Save these endpoints as an object (either by taking your `tidy()`-ed regression and `filter()`-ing the `term == "carat"` and then `select()`-ing the columns with the confidence interval and then saving this; or simply assigning the two values as a vector, `c( , )`, to an object).

---

<!--ANSWER BELOW HERE-->

```{r}
library(broom)

reg_tidy <- reg %>% tidy(conf.int = T)

# look at tidy outcome
reg_tidy

# extract & save the confidence interval
CI_endpoints <- reg_tidy %>%
  filter(term == "carat") %>%
  select(conf.low, conf.high)

# look at it
CI_endpoints

# you could have alternatively just found what these are and then saved them manually
CI_alt <- c(772.855,7783.996)

CI_alt
```
We are 95% confident that in similarly constructed samples, the true effect of carat on price $(\beta_1)$ is between $7,728.86 and $7,784.00.

---

### Part E

Save your estimated $\hat{\beta_1}$ as an object, we'll need it later with `infer` (either by taking your `tidy()`-ed regression and `filter()`-ing the `term == carat` and then `select()`-ing the column with the `estimate` and then saving this; or simply assigning the values to an object).

---

<!--ANSWER BELOW HERE-->

```{r}
# extract and save sample slope as object called beta_1_hat

beta_1_hat <- reg_tidy %>%
  filter(term == "carat") %>%
  select(estimate)

# confirm
beta_1_hat

# you could have alternatively just found what these are and then saved them manually
beta_1_hat_alt <- 7756.43

beta_1_hat_alt

```

---

## Question 3
Now let’s use `infer`. Install it if you don’t have it, then load it.

---

<!--ANSWER BELOW HERE-->

```{r}
library(infer)
```

---

### Part A

Let’s generate a confidence interval for $\hat{\beta_1}$ by simulating the sampling distribution of $\hat{\beta_1}$. Run the following code, which will `specify()` the model relationship and `generate()` 1,000 repetitions using the `bootstrap` method of resampling data points randomly from our sample, with replacement.

```{r, eval = F}
# save our simulations as an object (I called it "boot")
boot <- diamonds %>% # or whatever you named your dataframe!
  specify(carat ~ price) %>% # our regression model
  generate(reps = 1000, # run 1000 simulations
           type = "bootstrap") %>% # using bootstrap method
  calculate(stat = "slope") # estimate slope in each simulation

# look at it
boot
```

Note this will take a few minutes, its doing a lot of calculations! What does it show you when you look at it?

---

<!--ANSWER BELOW HERE-->

```{r}
# save our simulations as an object (I called it "boot")
boot <- diamonds %>% # or whatever you named your dataframe!
  specify(carat ~ price) %>% # our regression model
  generate(reps = 1000, # run 1000 simulations
           type = "bootstrap") %>% # using bootstrap method
  calculate(stat = "slope") # estimate slope in each simulation

# look at it
boot
```

This produces 1,000 bootstrapped samples, and calculates the estimated regression slope $(\hat{\beta_1})$ for each sample.

---

### Part B

Continue by piping your object from Part A into `get_confidence_interval()`. Set `level = 0.95, type = "se"` and `point_estimate` equal to our estimated $\hat{\beta_1}$ (saved) from Question 2 Part E.

```{r, eval = F}
boot %>%
  get_confidence_interval(level = 0.95,
                          type = "se", 
                          point_estimate = beta_1_hat) # or whatever you saved it as
```

---

<!--ANSWER BELOW HERE-->

```{r}
boot %>%
  get_confidence_interval(level = 0.95,
                          type = "se", 
                          point_estimate = beta_1_hat) # or whatever you saved it as
```

This gives us a confidence interval of $7,706.95 and $7,805.91. This not exactly, but quite close to `R`'s automatic calculation using the theoretical $t$-distribution in `lm()`.

---

### Part C

Now instead of `get_confidence_interval()`, pipe your object from Part A into `visualize()` to see the sampling distribution of $\hat{\beta_1}$ we simulated. You can add `+ shade_ci(endpoints = ...)` setting the argument equal to whatever you called your object containing the confidence interval from Question 2 Part D (I have it named here as `CI_endpoints`).

```{r, eval = F}
boot %>%
  visualize()+
  shade_ci(endpoints = CI_endpoints) # or whatever you saved it as
```

Compare your *simulated* confidence interval with the *theoretically-constructed* confidence interval from the output of `summary`, and/or of `tidy()` from Question 2.

---

<!--ANSWER BELOW HERE-->

```{r}
boot %>%
  visualize()+
  shade_ci(endpoints = CI_endpoints) # or whatever you saved it as
```

---

## Question 4

Now let’s test the following hypothesis:

$$\begin{align*}
H_0: \beta_1 &= 0\\
H_1: \beta_1 &\neq 0\\
\end{align*}$$

### Part A

What does the output of `summary`, and/or of `tidy()` from Question 2 tell you?

---

<!--ANSWER BELOW HERE-->

Reading the second row of estimates from the `lm summary` or `tidy` output of question 2, we saw that the `estimate` on `carat` $(\hat{\beta_1})$ was 7756.43, with a standard error of 14.07. This yields a test statistic value of 551.4, and a $p$-value of basically 0.

---

### Part B

Let’s now do run this hypothesis test with `infer`, which will simulate the sampling distribution under the null hypothesis that $\beta_1 = 0$. Run the following code, which will `specify()` the model relationship and `hypothesize()` the null hypothesis that there is no relationship between $X$ and $Y$ (i.e. $\beta_1=0)$, and `generate()` 1,000 repetitions using the `permute` method, which will center the distribution at 0, and then  `calculate(stat = "slope")`.

```{r, eval = F}
# save our simulations as an object (I called it "test_sims")
test_sims <- diamonds %>% # or whatever you named your dataframe!
  specify(carat ~ price) %>% # our regression model
  hypothesize(null = "independence") %>% # H_0 is that slope is 0
  generate(reps = 1000, # run 1000 simulations
           type = "permute") %>% # using permute method, centering distr at 0
  calculate(stat = "slope") # estimate slope in each simulation

# look at it
test_sims
```

Note this may also take a few minutes. What does it show you?

---

<!--ANSWER BELOW HERE-->

```{r}
# save our simulations as an object (I called it "test_sims")
test_sims <- diamonds %>% # or whatever you named your dataframe!
  specify(carat ~ price) %>% # our regression model
  hypothesize(null = "independence") %>% # H_0 is that slope is 0
  generate(reps = 1000, # run 1000 simulations
           type = "permute") %>% # using permute method, centering distr at 0
  calculate(stat = "slope") # estimate slope in each simulation

# look at it
test_sims
```

This produces 1,000 permuted samples, under the assumption that the true slope is 0 (the null hypothesis), and calculates the estimated regression slope $(\hat{\beta_1})$ for each sample.

---

### Part C

Pipe your object from the previous part into the following code, which will `get_p_value()`. Inside this function, we are setting `obs_stat` equal to our $\hat{\beta_1}$ we found (from Question 2 part E), and set `direction = "both"` to run a *two*-sided test, since our alternative hypothesis is two-sided, $H_1: \beta_1 \neq 0$.

```{r, eval = F}
test_sims %>%
  get_p_value(obs_stat = beta_1_hat,
              direction = "both")
```

Note the warning message that comes up!

---

<!--ANSWER BELOW HERE-->

```{r}
test_sims %>%
  get_p_value(obs_stat = beta_1_hat,
              direction = "both")
# again, instead of beta_1_hat you could type in 7756.426
```

---

### Part D

Instead of `get_p_value()`, pipe your object from Part B into the following code, which will `visualize()` the null distribution, and (in the second command), place our finding on it and `shade_p_value()`.

```{r, eval =F}
test_sims %>%
  visualize()
```

```{r, eval = F}
test_sims %>%
  visualize()+
  shade_p_value(obs_stat = beta_1_hat, # or whatever you saved it as
                direction = "both") # for two-sided test
```

---

<!--ANSWER BELOW HERE-->

```{r}
test_sims %>%
  visualize()
```

```{r}
test_sims %>%
  visualize()+
  shade_p_value(obs_stat = beta_1_hat, # or whatever you saved it as
                direction = "both") # for two-sided test
```

---

### Part E

Compare your *simulated* p-value with the *theoretically-constructed* p-value from the output of `summary`, and/or of `tidy()` from Question 2.

Both `summary` and `tidy()` also report the $t$-statistic (`t value` or `statistic`) on this test for `carat` (by default, that $H_0: \beta_1=0)$. What is the estimated test statistic for this model, and what does this number mean? Try to calculate it yourself with the formula:

$$t = \frac{\text{estimate} - \text{null hypothesis value}}{\text{standard error of estimate}}$$

The p-value is the probability of a $t$ statistic at least as large as ours if the null hypothesis were true. `R` constructs a $t$-distribution with `n-k-1` degrees of freedom (`n` is number of observations, `k` is number of $X$-variables) and calculates the probability in the tails of the distribution beyond this $t$ value. You can calculate it yourself (for a two-sided test) with: 

```{r, eval = F}
2 * pt(your_t, # put your t-statistic here
       df = your_df, # put the df number here
       lower.tail = F) # we'll use the right tail
```

---

<!--ANSWER BELOW HERE-->

The test-statistic was calculated, as described in the slides, as:

$$\begin{align*}
t & = \frac{\text{estimate} - \text{null hypothesis}}{\text{SE(estimate)}}\\
t & = \frac{7756.43 - 0}{14.07}\\
t &\approx 551.3 \\
\end{align*}$$

If the null hypothesis were true $(\beta_1=0)$, the probability that we get a test-statistic at least as extreme as 551.3 (essentially, 551 standard deviations away!!) is virtually 0. You can see this when we visualize using infer, above.

To calculate the p-value ourselves, we need to find the probability in the tails of the $t_{53938}$-distribution beyond $\pm551.3$. The $t$ distribution has 53,938 degrees of freedom — $(n-k-1)$ where $n = 53940$ and $k=1$. 

```{r}
2 * pt(551.3, # our t-statistic
       df = 53938, # the df number
       lower.tail = F) # we'll use the right tail
```
